
* R1 index
:PROPERTIES:
:max-ref:  R1
:ID:       23d78832-af74-40b8-9062-0965a25053e6
:prev-line-count: 1
:END:


Extracted some stats
#+begin_src shell :exports both
microfts info -grams index >grams.txt
#+end_src
  
I used this to create index
#+BEGIN_SRC sh :results raw drawer :exports both
for f in $(find . -name "*.org"); do
    echo "#+INCLUDE: " $f
done
#+END_SRC

#+RESULTS:
:results:

:end:


#+INCLUDE:  ./dynabench/api/builder/README.md.org
#+INCLUDE:  ./dynabench/api/evaluation/instructions.md.org
#+INCLUDE:  ./dynabench/api/cron/README.md.org
#+INCLUDE:  ./dynabench/api/templates/decen_dataperf/outdir/README.md.org
#+INCLUDE:  ./dynabench/api/templates/decen_dataperf/selection_algorithm/README.md.org
#+INCLUDE:  ./dynabench/api/templates/decen_dataperf/README.md.org
#+INCLUDE:  ./dynabench/api/templates/decen_dataperf/results/README.md.org
#+INCLUDE:  ./dynabench/api/templates/decen_dataperf/submissions/README.md.org
#+INCLUDE:  ./dynabench/docs/email.md.org
#+INCLUDE:  ./dynabench/docs/frontend.md.org
#+INCLUDE:  ./dynabench/docs/start.md.org
#+INCLUDE:  ./dynabench/docs/database.md.org
#+INCLUDE:  ./dynabench/docs/owners.md.org
#+INCLUDE:  ./dynabench/docs/overview.md.org
#+INCLUDE:  ./dynabench/docs/adding_task.md.org
#+INCLUDE:  ./dynabench/docs/evaluation.md.org
#+INCLUDE:  ./dynabench/CONTRIBUTING.md.org
#+INCLUDE:  ./dynabench/README.md.org
#+INCLUDE:  ./dynabench/legacy/mlcube/vision_data_selection/README.md.org
#+INCLUDE:  ./dynabench/legacy/mlcube/speech_data_selection/README.md.org
#+INCLUDE:  ./dynabench/legacy/torchserve/README.md.org
#+INCLUDE:  ./dynabench/legacy/education/Readme.md.org
#+INCLUDE:  ./dynabench/legacy/modelservers/nli/README.md.org
#+INCLUDE:  ./dynabench/CODE_OF_CONDUCT.md.org
#+INCLUDE:  ./dynabench/backend/app/domain/services/builder_and_evaluation/readme.md.org
#+INCLUDE:  ./dynabench/backend/dynalab/readme.md.org
#+INCLUDE:  ./dynabench/backend/dynalab/example/README.md.org
#+INCLUDE:  ./dynabench/backend/dynalab/example/base-model-nli/README.md.org
#+INCLUDE:  ./peoples-speech/scripts/archive.org
#+INCLUDE:  ./power-dev/CONTRIBUTING.md.org
#+INCLUDE:  ./power-dev/LICENSE.md.org
#+INCLUDE:  ./power-dev/power_meter_sampling/README.md.org
#+INCLUDE:  ./power-dev/README.md.org
#+INCLUDE:  ./power-dev/compliance/README.md.org
#+INCLUDE:  ./power-dev/ptd_client_server/README.md.org
#+INCLUDE:  ./power-dev/log_parsers/README.md.org
#+INCLUDE:  ./power-dev/INSTALL.md.org
#+INCLUDE:  ./croissant/wizard/README.md.org
#+INCLUDE:  ./croissant/docs/howto/readme.md.org
#+INCLUDE:  ./croissant/docs/howto/specify-splits.md.org
#+INCLUDE:  ./croissant/docs/definitions/index.md.org
#+INCLUDE:  ./croissant/docs/definitions/split.md.org
#+INCLUDE:  ./croissant/docs/definitions/validation_split.md.org
#+INCLUDE:  ./croissant/docs/definitions/testing_split.md.org
#+INCLUDE:  ./croissant/docs/definitions/training_split.md.org
#+INCLUDE:  ./croissant/docs/definitions/sitemap.md.org
#+INCLUDE:  ./croissant/docs/croissant-spec.md.org
#+INCLUDE:  ./croissant/CONTRIBUTING.md.org
#+INCLUDE:  ./croissant/LICENSE.md.org
#+INCLUDE:  ./croissant/python/mlcroissant/README.md.org
#+INCLUDE:  ./croissant/python/openmlconverter/README.md.org
#+INCLUDE:  ./croissant/README.md.org
#+INCLUDE:  ./croissant/datasets/README.md.org
#+INCLUDE:  ./algorithmic-efficiency/CHANGELOG.md.org
#+INCLUDE:  ./algorithmic-efficiency/CONTRIBUTING.md.org
#+INCLUDE:  ./algorithmic-efficiency/README.md.org
#+INCLUDE:  ./algorithmic-efficiency/reference_algorithms/target_setting_algorithms/README.md.org
#+INCLUDE:  ./algorithmic-efficiency/datasets/README.md.org
#+INCLUDE:  ./algorithmic-efficiency/getting_started.md.org
#+INCLUDE:  ./algorithmic-efficiency/RULES.md.org
#+INCLUDE:  ./algorithmic-efficiency/baselines/README.md.org
#+INCLUDE:  ./training_policies/CONTRIBUTING.md.org
#+INCLUDE:  ./training_policies/LICENSE.md.org
#+INCLUDE:  ./training_policies/README.md.org
#+INCLUDE:  ./inference/calibration/BraTS/README.md.org
#+INCLUDE:  ./inference/calibration/KiTS/README.md.org
#+INCLUDE:  ./inference/calibration/LibriSpeech/README.md.org
#+INCLUDE:  ./inference/calibration/SQuAD-v1.1/README.md.org
#+INCLUDE:  ./inference/calibration/CriteoTerabyte/README.md.org
#+INCLUDE:  ./inference/calibration/ImageNet/README.md.org
#+INCLUDE:  ./inference/recommendation/dlrm_v2/pytorch/README.md.org
#+INCLUDE:  ./inference/recommendation/dlrm/pytorch/README.md.org
#+INCLUDE:  ./inference/recommendation/dlrm/tf/README.md.org
#+INCLUDE:  ./inference/CONTRIBUTING.md.org
#+INCLUDE:  ./inference/LICENSE.md.org
#+INCLUDE:  ./inference/README.md.org
#+INCLUDE:  ./inference/loadgen/tests/README.md.org
#+INCLUDE:  ./inference/loadgen/demos/lon/README.md.org
#+INCLUDE:  ./inference/loadgen/README_BUILD.md.org
#+INCLUDE:  ./inference/loadgen/docs/src/README.md.org
#+INCLUDE:  ./inference/loadgen/README_FAQ.md.org
#+INCLUDE:  ./inference/loadgen/README.md.org
#+INCLUDE:  ./inference/loadgen/benchmark/README.md.org
#+INCLUDE:  ./inference/speech_recognition/rnnt/README_cm.md.org
#+INCLUDE:  ./inference/speech_recognition/rnnt/optional_harness_ck/README.md.org
#+INCLUDE:  ./inference/speech_recognition/rnnt/README.md.org
#+INCLUDE:  ./inference/vision/medical_imaging/3d-unet-kits19/README_cm.md.org
#+INCLUDE:  ./inference/vision/medical_imaging/3d-unet-kits19/README.md.org
#+INCLUDE:  ./inference/vision/medical_imaging/3d-unet-brats19/README.md.org
#+INCLUDE:  ./inference/vision/classification_and_detection/README_cm_resnet50.md.org
#+INCLUDE:  ./inference/vision/classification_and_detection/README_cm_retinanet.md.org
#+INCLUDE:  ./inference/vision/classification_and_detection/README.md.org
#+INCLUDE:  ./inference/vision/classification_and_detection/tools/retinanet_pytorch_to_onnx.md.org
#+INCLUDE:  ./inference/compliance/nvidia/README.md.org
#+INCLUDE:  ./inference/compliance/nvidia/TEST04/README.md.org
#+INCLUDE:  ./inference/compliance/nvidia/TEST01/README.md.org
#+INCLUDE:  ./inference/compliance/nvidia/TEST05/README.md.org
#+INCLUDE:  ./inference/tools/githooks/README.md.org
#+INCLUDE:  ./inference/tools/submission/README.md.org
#+INCLUDE:  ./inference/tools/upscale_coco/README.md.org
#+INCLUDE:  ./inference/translation/gnmt/README.md.org
#+INCLUDE:  ./inference/translation/gnmt/tensorflow/README.md.org
#+INCLUDE:  ./inference/language/bert/README_cm.md.org
#+INCLUDE:  ./inference/language/bert/README.md.org
#+INCLUDE:  ./inference/language/gpt-j/README_cm.md.org
#+INCLUDE:  ./inference/language/gpt-j/README.md.org
#+INCLUDE:  ./inference/language/gpt-j/DATASETS_MODELS.md.org
#+INCLUDE:  ./inference/Submission_Guidelines.md.org
#+INCLUDE:  ./mobile_models/v1_0/tflite/README.md.org
#+INCLUDE:  ./mobile_models/v1_0/SNPE/INDEPENDENT_WORK.md.org
#+INCLUDE:  ./mobile_models/v1_0/assets/README.md.org
#+INCLUDE:  ./mobile_models/CONTRIBUTING.md.org
#+INCLUDE:  ./mobile_models/v1_1/tflite/README.md.org
#+INCLUDE:  ./mobile_models/LICENSE.md.org
#+INCLUDE:  ./mobile_models/README.md.org
#+INCLUDE:  ./mobile_models/v3_0/datasets/READMDE.md.org
#+INCLUDE:  ./mobile_models/v3_0/assets/README.md.org
#+INCLUDE:  ./mobile_models/v0_7/tflite/README.md.org
#+INCLUDE:  ./mobile_models/v0_7/SNPE/INDEPENDENT_WORK.md.org
#+INCLUDE:  ./mobile_models/v0_7/README.md.org
#+INCLUDE:  ./ck/docs/news-mlperf-v3.1.md.org
#+INCLUDE:  ./ck/docs/introduction-ck.md.org
#+INCLUDE:  ./ck/docs/list_of_scripts.md.org
#+INCLUDE:  ./ck/docs/specs/cm-repository.md.org
#+INCLUDE:  ./ck/docs/specs/README.md.org
#+INCLUDE:  ./ck/docs/specs/cm-tool-architecture.md.org
#+INCLUDE:  ./ck/docs/specs/cm-automation-script.md.org
#+INCLUDE:  ./ck/docs/specs/cm-python-interface.md.org
#+INCLUDE:  ./ck/docs/specs/cm-cli.md.org
#+INCLUDE:  ./ck/docs/installation.md.org
#+INCLUDE:  ./ck/docs/interface.md.org
#+INCLUDE:  ./ck/docs/introduction-cm.md.org
#+INCLUDE:  ./ck/docs/installation-cuda.md.org
#+INCLUDE:  ./ck/docs/taskforce.md.org
#+INCLUDE:  ./ck/docs/README.md.org
#+INCLUDE:  ./ck/docs/news.md.org
#+INCLUDE:  ./ck/docs/mlperf-cm-automation-demo.md.org
#+INCLUDE:  ./ck/docs/mlperf/setup/setup-nvidia-jetson-orin.md.org
#+INCLUDE:  ./ck/docs/mlperf/setup/setup-aws-instance.md.org
#+INCLUDE:  ./ck/docs/mlperf/setup/setup-gcp-instance.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/README_collabora.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/retinanet/README_reference.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/retinanet/README.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/retinanet/README_nvidia.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/README_a100.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/dlrm_v2/README_reference.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/README.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/bert/README_reference.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/bert/README.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/bert/README_deepsparse.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/bert/README_nvidia.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/bert/README_intel.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/bert/tutorial.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/rnnt/README_reference.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/rnnt/README.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/rnnt/README_nvidia.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/gpt-j/README_reference.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/gpt-j/README.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/Submission.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/resnet50/README_reference.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/resnet50/README.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/resnet50/README_nvidia.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/resnet50/README_tflite.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/3d-unet/README_reference.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/3d-unet/README.md.org
#+INCLUDE:  ./ck/docs/mlperf/inference/3d-unet/README_nvidia.md.org
#+INCLUDE:  ./ck/docs/mlperf/README.md.org
#+INCLUDE:  ./ck/docs/faq.md.org
#+INCLUDE:  ./ck/docs/debugging.md.org
#+INCLUDE:  ./ck/docs/tutorials/mlperf-inference-power-measurement.md.org
#+INCLUDE:  ./ck/docs/tutorials/concept.md.org
#+INCLUDE:  ./ck/docs/tutorials/automate-mlperf-tiny.md.org
#+INCLUDE:  ./ck/docs/tutorials/sc22-scc-mlperf.md.org
#+INCLUDE:  ./ck/docs/tutorials/modular-image-classification.md.org
#+INCLUDE:  ./ck/docs/tutorials/README.md.org
#+INCLUDE:  ./ck/docs/tutorials/test-spec-ptdaemon.md.org
#+INCLUDE:  ./ck/docs/tutorials/scc23-mlperf-inference-bert.md.org
#+INCLUDE:  ./ck/docs/tutorials/reproduce-mlperf-training.md.org
#+INCLUDE:  ./ck/docs/tutorials/sc22-scc-mlperf-part3.md.org
#+INCLUDE:  ./ck/docs/tutorials/sc22-scc-mlperf3.md.org
#+INCLUDE:  ./ck/docs/tutorials/mlperf-language-processing.md.org
#+INCLUDE:  ./ck/docs/tutorials/common-interface-to-reproduce-research-projects.md.org
#+INCLUDE:  ./ck/docs/tutorials/sc22-scc-mlperf-part2.md.org
#+INCLUDE:  ./ck/docs/tutorials/reproduce-mlperf-tiny.md.org
#+INCLUDE:  ./ck/docs/tutorials/scripts.md.org
#+INCLUDE:  ./ck/docs/tutorials/reproduce-research-paper-ipol.md.org
#+INCLUDE:  ./ck/docs/tutorials/mlperf-inference-submission.md.org
#+INCLUDE:  ./ck/docs/tutorials/sc22-scc-mlperf2.md.org
#+INCLUDE:  ./ck/docs/list_of_automations.md.org
#+INCLUDE:  ./ck/docs/misc/history.md.org
#+INCLUDE:  ./ck/docs/misc/ML.md.org
#+INCLUDE:  ./ck/docs/misc/MLOps.md.org
#+INCLUDE:  ./ck/docs/misc/overview.md.org
#+INCLUDE:  ./ck/docs/archive/taskforce-2022.md.org
#+INCLUDE:  ./ck/docs/artifact-evaluation/hotcrp-config/README.md.org
#+INCLUDE:  ./ck/docs/artifact-evaluation/reviewing.md.org
#+INCLUDE:  ./ck/docs/artifact-evaluation/submission.md.org
#+INCLUDE:  ./ck/docs/artifact-evaluation/checklist.md.org
#+INCLUDE:  ./ck/docs/artifact-evaluation/faq.md.org
#+INCLUDE:  ./ck/docs/mlperf-education-workgroup.md.org
#+INCLUDE:  ./ck/cm/docs/tutorial-concept.md.org
#+INCLUDE:  ./ck/cm/docs/installation.md.org
#+INCLUDE:  ./ck/cm/docs/example-modular-image-classification.md.org
#+INCLUDE:  ./ck/cm/docs/README.md.org
#+INCLUDE:  ./ck/cm/docs/conventions.md.org
#+INCLUDE:  ./ck/cm/docs/tutorial-scripts.md.org
#+INCLUDE:  ./ck/cm/docs/tutorial-modular-mlperf.md.org
#+INCLUDE:  ./ck/cm/docs/specification.md.org
#+INCLUDE:  ./ck/cm/docs/motivation.md.org
#+INCLUDE:  ./ck/cm/docs/KB/ML.md.org
#+INCLUDE:  ./ck/cm/docs/KB/MLOps.md.org
#+INCLUDE:  ./ck/cm/docs/architecture.md.org
#+INCLUDE:  ./ck/cm/docs/enhancements.md.org
#+INCLUDE:  ./ck/cm/CONTRIBUTING.md.org
#+INCLUDE:  ./ck/cm/LICENSE.md.org
#+INCLUDE:  ./ck/cm/LICENSE.CK.md.org
#+INCLUDE:  ./ck/cm/CHANGES.md.org
#+INCLUDE:  ./ck/cm/README.md.org
#+INCLUDE:  ./ck/cm/cmind/repo/automation/automation/README.md.org
#+INCLUDE:  ./ck/cm/cmind/repo/automation/automation/template_list_of_automations.md.org
#+INCLUDE:  ./ck/cm/cmind/repo/automation/ck/README.md.org
#+INCLUDE:  ./ck/cm/cmind/repo/automation/core/README.md.org
#+INCLUDE:  ./ck/cm/cmind/repo/automation/repo/README.md.org
#+INCLUDE:  ./ck/cm/cmind/repo/README.md.org
#+INCLUDE:  ./ck/CONTRIBUTING.md.org
#+INCLUDE:  ./ck/LICENSE.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks-custom/task-speech-recognition.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks-custom/task-recommendation.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks-custom/README.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks-custom/task-object-detection.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks-custom/task-image-classification.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks-custom/task-nlp.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks-custom/task-medical-imaging.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tbd/ck2.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tbd/automation.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tbd/standardization.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/models/notes-convert-resnet50-onnx-format-to-quantized-tensorrt-engine.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/models/notes.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/models/notes-convert-tf-resnet50-1.5-to-tflite.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/models/notes-using-tflite-models-for-datacenter-scenarios.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-ae88dc4516a7084e.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-object-detection-rpi4-coral-tflite.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-3e0ad4b09998375d.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-object-detection-x86-64-docker.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-a399f837b48b0d1b.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-b14c70816eca59c6.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-mlperf-inf-1.1-image-classification-resnet50-aws-c6gd.xlarge-arm64-edge-closed.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-object-detection-rpi4-tflite.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-1b165548d8adbe4d.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/README.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-4f1a470a8a034bc3.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-image-classification-x86-64-tflite2.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-image-classification-x86-64-openvino-2019.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-c3d81b4b869e8e07.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-image-classification-rpi4-tflite.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-mlperf-inf-1.1-image-classification-resnet50-rpi4-arm64-edge-closed.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-94cc7bdd1f23cce3.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-object-detection-x86-64.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/demo-webcam-object-detection-x86-64.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/image-classification-nvidia-jetson-xavier-mlperf.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-image-classification-x86-64-tflite.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-image-classification-x86-64-docker.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-image-classification-jetson-nano-tflite.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-6582273dd3646e28.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-9fb65e57d8c61db4.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-mlperf-inf-1.1-image-classification-dse-pareto-mobilenet-v1-0.25-224-quantized-tflite-aws-c6gd.xlarge-neoverse-n1-edge-open.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/reproduce/ck-3c77b273b4c7d878.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/setup/framework-pytorch.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/setup/framework-onnx.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/setup/common.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/setup/framework-tvm.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/setup/lib-transformers.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/setup/compiler-cuda.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/setup/framework-tf.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/setup/framework-tflite.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/inference/workflow.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/inference/containers.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/inference/notes.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/submit/power.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/submit/README.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/README.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/platform/rpi4-ubuntu.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/platform/nvidia-generic.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/platform/README.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/platform/nvidia-jetson-nano.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/platform/rpi4-debian.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/platform/x8664-yocto.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/platform/rpi4-coral-ubuntu.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/platform/x8664-ubuntu.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/results/ck-dashboard.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/results/README.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/datasets/imagenet2012.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/datasets/coco2017-preprocess.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/datasets/README.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/datasets/coco2017.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/datasets/imagenet2012-preprocess.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/datasets/librispeech.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/datasets/squad.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/datasets/brats2019.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/dse/README.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/task-object-speech-pytorch.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/task-speech-recognition.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/task-medical-imaging-pytorch.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/tvm/README.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/tvm/mlperf-object-detection.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/tvm/mlperf-image-classification.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/task-image-classification-tf.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/task-object-detection-tflite.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/task-object-detection-onnx.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/task-recommendation.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/task-object-detection-tensorrt.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/README.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/task-language-onnx.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/task-object-detection.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/task-image-classification.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/task-medical-imaging-onnx.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/task-image-classification-tvm.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/task-image-classification-openvino.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/task-nlp.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/task-image-classification-pytorch.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/task-object-detection-tvm.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/task-image-classification-onnx.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tasks/task-image-classification-tflite.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tools/README.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tools/ck.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tools/logging.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tools/mlcube.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tools/ck-venv.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tools/continuous-integration.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tools/ck-docker.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tutorials/mlperf-inference-v1.1-submission-demo.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tutorials/tvmcon-2021-automating-mlperf-with-tvm-and-ck.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tutorials/README.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/tutorials/tvmcon-2021-automating-mlperf-with-tvm-and-ck-demo.md.org
#+INCLUDE:  ./ck/ck/docs/mlperf-automation/components/README.md.org
#+INCLUDE:  ./ck/ck/docs/src/typical-usage.md.org
#+INCLUDE:  ./ck/ck/docs/src/commands.md.org
#+INCLUDE:  ./ck/ck/docs/src/misc.md.org
#+INCLUDE:  ./ck/ck/docs/src/installation.md.org
#+INCLUDE:  ./ck/ck/docs/src/introduction.md.org
#+INCLUDE:  ./ck/ck/docs/src/acknowledgments.md.org
#+INCLUDE:  ./ck/ck/docs/src/use-cases.md.org
#+INCLUDE:  ./ck/ck/docs/src/portable-workflows.md.org
#+INCLUDE:  ./ck/ck/docs/src/first-steps.md.org
#+INCLUDE:  ./ck/ck/docs/src/how-to-contribute.md.org
#+INCLUDE:  ./ck/ck/docs/src/specs.md.org
#+INCLUDE:  ./ck/ck/docs/src/feedback.md.org
#+INCLUDE:  ./ck/ck/CONTRIBUTING.md.org
#+INCLUDE:  ./ck/ck/LICENSE.md.org
#+INCLUDE:  ./ck/ck/ck/repo/module/result/README.md.org
#+INCLUDE:  ./ck/ck/ck/repo/module/dashboard/README.md.org
#+INCLUDE:  ./ck/ck/ck/repo/module/ck-platform/README.md.org
#+INCLUDE:  ./ck/ck/ck/repo/module/graph/third-party/d3/README.md.org
#+INCLUDE:  ./ck/ck/README.md.org
#+INCLUDE:  ./ck/ck/incubator/ck-lite/README.md.org
#+INCLUDE:  ./ck/ck/incubator/cdatabase/CONTRIBUTING.md.org
#+INCLUDE:  ./ck/ck/incubator/cdatabase/README.md.org
#+INCLUDE:  ./ck/ck/incubator/README.md.org
#+INCLUDE:  ./ck/ck/incubator/cbench/README.md.org
#+INCLUDE:  ./ck/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/repro-mlperf-inf-v3.0-orin/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1--tournament/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1-universal-cpp-implementation-2023/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/train-llm-for-cm-mlperf-2023/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/repro-mlperf-inference-retinanet-scc2022/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/reproduce-and-automate-neurips-2022-paper-arxiv-2204.09656/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1-modular-mojo-2023/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1-google-tpu-2023/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1-create-end-to-end-app/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/run-mlperf@home-v3.1-gpu/run-nvidia-gpu-bert-99-nvidia-docker-tensorrt.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/run-mlperf@home-v3.1-gpu/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/run-mlperf@home-v3.1-gpu/run-nvidia-gpu-gpt-j-6b-ref-pytorch.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1-android/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/reproduce-mlperf-training-v3.0-2023/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v2.1-2022/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/participate-hipeac-reproducibilty-challenge-2024/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1-hugging-face-models-2023/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/run-mlperf@home-v3.1-cpu/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/run-mlperf@home-v3.1-cpu/run-cpu-dse-mobilenets-efficientnets-tflite.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/run-mlperf@home-v3.1-cpu/run-cpu-bert-99-deepsparse.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/reproduce-and-automate-tinymlperf-v1.1-2023/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-scc2023/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/connect-mlperf-with-medperf/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/connect-mlperf-inference-v3.1-with-openbenchmarking/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/repro-micro2023/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/reproduce-and-automate-ipol-paper/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1-amazon-inferentia-2023/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.0-2023/docs/setup-nvidia-jetson-orin.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.0-2023/docs/crowd-benchmark-mlperf-bert-inference-cuda.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.0-2023/docs/generate-rnnt-submission.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.0-2023/docs/generate-retinanet-submission.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.0-2023/docs/setup-aws-instance.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.0-2023/docs/generate-resnet50-submission.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.0-2023/docs/generate-3d-unet-submission.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.0-2023/docs/setup-gcp-instance.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.0-2023/docs/generate-bert-submission.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.0-2023/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1-tvm-2023/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1-windows-2023/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/reproduce-automate-explain-past-mlperf-inference-results-2023/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1-intel-2023/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1-qualcomm-ai100-2023/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/add-derived-metrics-to-mlperf-inference-v3.1/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1-2023/docs/setup-nvidia-jetson-orin.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1-2023/docs/generate-rnnt-submission.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1-2023/docs/generate-retinanet-submission.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1-2023/docs/setup-aws-instance.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1-2023/docs/generate-resnet50-submission.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1-2023/docs/generate-3d-unet-submission.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1-2023/docs/setup-gcp-instance.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1-2023/docs/generate-bert-submission.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1-2023/README.md.org
#+INCLUDE:  ./ck/cm-mlops/challenge/optimize-mlperf-inference-v3.1-deepsparse/README.md.org
#+INCLUDE:  ./ck/cm-mlops/CONTRIBUTING.md.org
#+INCLUDE:  ./ck/cm-mlops/automation/challenge/README.md.org
#+INCLUDE:  ./ck/cm-mlops/automation/list_of_scripts.md.org
#+INCLUDE:  ./ck/cm-mlops/automation/docker/README.md.org
#+INCLUDE:  ./ck/cm-mlops/automation/cache/README.md.org
#+INCLUDE:  ./ck/cm-mlops/automation/cache/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/automation/utils/README.md.org
#+INCLUDE:  ./ck/cm-mlops/automation/contributor/README.md.org
#+INCLUDE:  ./ck/cm-mlops/automation/report/README.md.org
#+INCLUDE:  ./ck/cm-mlops/automation/experiment/README.md.org
#+INCLUDE:  ./ck/cm-mlops/automation/experiment/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/automation/script/template-pytorch/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/automation/script/template-ae-python/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/automation/script/README-specs.md.org
#+INCLUDE:  ./ck/cm-mlops/automation/script/README.md.org
#+INCLUDE:  ./ck/cm-mlops/automation/script/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/automation/script/template_list_of_scripts.md.org
#+INCLUDE:  ./ck/cm-mlops/automation/script/template/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/automation/script/template-python/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/automation/project/README.md.org
#+INCLUDE:  ./ck/cm-mlops/LICENSE.md.org
#+INCLUDE:  ./ck/cm-mlops/CHANGES.md.org
#+INCLUDE:  ./ck/cm-mlops/README.md.org
#+INCLUDE:  ./ck/cm-mlops/LICENSE.third-party.md.org
#+INCLUDE:  ./ck/cm-mlops/report/mlperf-inference-v3.1-analysis-ctuning/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/build-dockerfile/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/build-dockerfile/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-onnxruntime-prebuilt/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-cudnn/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/push-csv-to-spreadsheet/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-ml-model-retinanet/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-ml-model-retinanet/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/import-experiment-to-sqlite/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-sys-utils-min/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-ml-model-gptj/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/test-set-sys-user-cm/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/reproduce-micro-paper-2023-victima/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/reproduce-micro-paper-2023-victima/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/convert-ml-model-huggingface-to-onnx/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-training-src/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-training-src/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-ck/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-inference-sut-configs/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-inference-sut-configs/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-ck-repo-mlops/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-xilinx-sdk/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-inference-nvidia-scratch-space/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-inference-nvidia-scratch-space/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-dataset-openimages-annotations/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-inference-sut-description/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-dlrm/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-dlrm/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-ml-model-efficientnet-lite/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/detect-sudo/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-tvm-model/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-tvm-model/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-ml-model-bert-base-squad/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/install-cmake-prebuilt/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/install-bazel/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/reproduce-mlperf-training-nvidia/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/run-terraform/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/run-terraform/README-about.md.org
#+INCLUDE:  ./ck/cm-mlops/script/run-terraform/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/install-tensorflow-from-src/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/install-gflags/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-power-dev/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-power-dev/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-ml-model-retinanet-nvidia/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/install-openssl/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-bert-squad-vocab/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/remote-run-commands/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/remote-run-commands/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/install-terraform-from-src/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-dataset-squad-vocab/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-aocl/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-aocl/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/prepare-training-data-bert/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-ml-model-3d-unet-kits19/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-image-classification-tvm-onnx-py/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-image-classification-tvm-onnx-py/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/upgrade-python-pip/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/process-mlperf-accuracy/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-mlperf-inference-reference/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-mlperf-inference-reference/README-about.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-mlperf-inference-reference/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-cuda/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-cuda/README-about.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-cuda/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-loadgen-generic-python/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-loadgen-generic-python/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/download-and-extract/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-ml-model-bert-large-squad/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-go/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-go/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/import-mlperf-training-to-experiment/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/import-mlperf-training-to-experiment/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-java/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-java/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-lib-armnn/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-spec-ptd/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-spec-ptd/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/run-python/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-zephyr/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-zephyr/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-image-classification-torch-py/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-image-classification-torch-py/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-ml-model-dlrm-terabyte/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-generic-sys-util/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/detect-os/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-image-classification-tf-onnx-cpp/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-image-classification-tf-onnx-cpp/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/run-all-mlperf-models/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/publish-results-to-dashboard/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/install-aws-cli/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/preprocess-mlperf-inference-submission/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/install-tensorflow-for-c/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/wrapper-reproduce-octoml-tinyml-submission/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/wrapper-reproduce-octoml-tinyml-submission/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-dataset-librispeech/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-dataset-librispeech/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/create-fpgaconvnet-config-tinyml/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-blis/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-blis/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/build-docker-image/examples/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/build-docker-image/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/build-docker-image/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-tiny-src/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-inference-nvidia-common-code/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-inference-nvidia-common-code/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/detect-cpu/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/detect-cpu/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/install-tflite-from-src/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-tensorrt/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-tensorrt/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-compiler-flags/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-inference-loadgen/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-inference-loadgen/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/pull-git-repo/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-dataset-kits19/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/process-ae-users/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-cl/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-cl/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-dataset-openimages/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-github-cli/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/flash-tinyml-binary/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/flash-tinyml-binary/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/run-mlperf-inference-app/modular-cm-containers/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/run-mlperf-inference-app/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/run-mlperf-inference-app/README-about.md.org
#+INCLUDE:  ./ck/cm-mlops/script/run-mlperf-inference-app/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/compile-program/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/compile-program/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/add-custom-nvidia-system/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/add-custom-nvidia-system/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-ipol-src/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/set-echo-off-win/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-mlperf-inference-nvidia/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-mlperf-inference-nvidia/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-mlperf-inference-cpp/CONTRIBUTING.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-mlperf-inference-cpp/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-mlperf-inference-cpp/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-lib-dnnl/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-ml-model-resnet50/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-ml-model-resnet50/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/print-hello-world-java/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-llvm/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-llvm/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/activate-python-venv/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/activate-python-venv/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-cmsis_5/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-cmsis_5/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/reproduce-micro-paper-2023-xyz/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-preprocessed-dataset-openimages/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-preprocessed-dataset-openimages/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/install-python-src/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-cmake/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/test-download-and-extract-artifacts/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/test-download-and-extract-artifacts/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-image-corner-detection/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-image-corner-detection/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/test-mlperf-inference-retinanet-win/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/reproduce-ipol-paper-2022-439/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/reproduce-ipol-paper-2022-439/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-mlperf-inference-tflite-cpp/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-android-sdk/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-android-sdk/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-cuda-devices/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/install-python-venv/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/run-docker-container/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/run-docker-container/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/generate-mlperf-inference-submission/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/generate-mlperf-inference-submission/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/generate-mlperf-inference-submission/default_files/analyzer_table.md.org
#+INCLUDE:  ./ck/cm-mlops/script/generate-mlperf-inference-submission/default_files/power_settings.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-ml-model-rnnt/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/run-mlperf-inference-submission-checker/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/run-mlperf-inference-submission-checker/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/download-torrent/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-generic-python-lib/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-generic-python-lib/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/prepare-training-data-resnet/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-image-classification-onnx-py/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-image-classification-onnx-py/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-rclone/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/download-file/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-ml-model-mobilenet/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-ml-model-mobilenet/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/run-mlperf-inference-mobilenet-models/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/run-mlperf-inference-mobilenet-models/README-about.md.org
#+INCLUDE:  ./ck/cm-mlops/script/run-mlperf-training-submission-checker/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-ml-model-huggingface-zoo/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-qaic-software-kit/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/run-mlperf-power-client/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/run-mlperf-power-client/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-dataset-imagenet-helper/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/print-python-version/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/print-hello-world-py/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/create-fpgaconvnet-app-tinyml/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-tvm/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-tvm/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-compiler-rust/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/truncate-mlperf-inference-accuracy-log/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/truncate-mlperf-inference-accuracy-log/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-dataset-imagenet-train/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/generate-mlperf-inference-user-conf/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/generate-nvidia-engine/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/generate-nvidia-engine/README-about.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-zephyr-sdk/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-zephyr-sdk/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-zendnn/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/import-mlperf-inference-to-experiment/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/import-mlperf-inference-to-experiment/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/reproduce-mlperf-octoml-tinyml-results/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/reproduce-mlperf-octoml-tinyml-results/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-preprocessed-dataset-criteo/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-preprocessed-dataset-criteo/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/print-hello-world/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/print-hello-world-javac/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-kilt/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-kilt/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/set-sqlite-dir/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-training-nvidia-code/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-dataset-cifar10/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-brew/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-microtvm/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-microtvm/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-logging/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-logging/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-nvidia-docker/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-dataset-imagenet-val/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-dataset-imagenet-val/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-javac/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-javac/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/install-cuda-package-manager/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-dataset-squad/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-dataset-squad/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-qaic-compute-sdk/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/run-mlperf-power-server/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/run-mlperf-power-server/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/push-mlperf-inference-results-to-github/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-inference-src/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-inference-src/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-openssl/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-openssl/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-git-repo/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-git-repo/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/generate-mlperf-tiny-report/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/generate-mlperf-tiny-report/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/extract-file/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-preprocessed-dataset-kits19/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/reproduce-mlperf-inference-nvidia/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/reproduce-mlperf-inference-nvidia/README-about.md.org
#+INCLUDE:  ./ck/cm-mlops/script/convert-csv-to-md/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/benchmark-program-mlperf/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-ml-model-tiny-resnet/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/install-github-cli/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-python3/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-python3/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-mlperf-training-nvidia/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-dataset-cnndm/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/import-mlperf-tiny-to-experiment/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/import-mlperf-tiny-to-experiment/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/build-mlperf-inference-server-nvidia/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/build-mlperf-inference-server-nvidia/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-preprocessed-dataset-imagenet/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-preprocessed-dataset-imagenet/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-aws-cli/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-aws-cli/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/install-cuda-prebuilt/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/install-cuda-prebuilt/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-dataset-imagenet-aux/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/tar-my-folder/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/tar-my-folder/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/install-llvm-prebuilt/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/install-llvm-prebuilt/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/prune-bert-models/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/prune-bert-models/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/install-gcc-src/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-ml-model-using-imagenet-from-model-zoo/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/generate-mlperf-tiny-submission/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/generate-mlperf-tiny-submission/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/benchmark-program/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-mlperf-inference/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-mlperf-inference/dockerfiles/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-mlperf-inference/README-about.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-mlperf-inference/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-preprocessed-dataset-librispeech/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-preprocessed-dataset-generic/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-tiny-eembc-energy-runner-src/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/set-performance-mode/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-sys-utils-cm/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-docker/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-inference-results/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-mlperf-inference-results/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-bazel/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-bazel/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-ml-model-neuralmagic-zoo/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-gcc/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-gcc/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-terraform/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-terraform/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/destroy-terraform/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/destroy-terraform/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/script/install-llvm-src/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/gui/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/gui/README-about.md.org
#+INCLUDE:  ./ck/cm-mlops/script/gui/playground_beta_README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/app-mlperf-training-reference/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-google-test/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-dataset-criteo/README.md.org
#+INCLUDE:  ./ck/cm-mlops/script/get-dataset-criteo/README-extra.md.org
#+INCLUDE:  ./ck/cm-mlops/project/mlperf-inference-v3.0-submissions/docs/crowd-benchmark-mlperf-bert-inference-cuda.md.org
#+INCLUDE:  ./ck/cm-mlops/project/mlperf-inference-v3.0-submissions/docs/run-nvidia-implementation.md.org
#+INCLUDE:  ./ck/cm-mlops/project/mlperf-inference-v3.0-submissions/docs/generate-resnet50-submission.md.org
#+INCLUDE:  ./ck/cm-mlops/project/mlperf-inference-v3.0-submissions/docs/generate-bert-submission.md.org
#+INCLUDE:  ./ck/cm-mlops/project/mlperf-inference-v3.0-submissions/docs/setup-aws-graviton.md.org
#+INCLUDE:  ./ck/cm-mlops/project/mlperf-inference-v3.0-submissions/README.md.org
#+INCLUDE:  ./ck/platform/register.md.org
#+INCLUDE:  ./ck/platform/README.md.org
#+INCLUDE:  ./ck/platform/register2.md.org
#+INCLUDE:  ./ck/platform/get-started.md.org




* some of the prep scripts

#+begin_src sh :exports both

  find -name \*.org | xargs grep -h . | sort -u > ulines.txt
  find -name \*.org | xargs grep -H '^\s*\s\*+'

  
  find -name \*.org | xargs grep -h . | sort |uniq -c | sort -n > clines.txt
  tr -s '[[:punct:][:space:]]' '\n'  < ulines.txt  > words.txt
  sort uwords.txt |uniq -c | sort -n >wordcounts.txt
  find  > files.txt
  cat files.txt  | grep -o "\.[A-Za-z][A-Za-z][A-Za-z]$"  | sort |uniq -c | sort -n 
  cat files.txt  | grep -o "\.[^\/\.]*$"  | sort |uniq -c | sort -n > all.txt
  cat files.txt  | grep "\.md$"  >markdown.txt

#+end_src

* Approach

1. vectorize all unique lines, load into postgres. line and vector.
2. count line occurance, add to table. line and count.
3. for similar counts find co ccurances.  for count >N, find ones in a range of +/- %
   
So take begin_src end_src babel blocks

using emacs org babel mode, write an inline elisp script that traverses the current document and
finds the previous block of data and parses it.

** programming languages
use python to sum the numbers in the following table.
#+begin_src sh
grep -i BEGIN_SRC clines.txt 
#+end_src
#+NAME: source-table
#+RESULTS:
|      1 | #+BEGIN_SRC | latex    |          |     |        |
|      1 | #+begin_src | shell    |          |     |        |
|      1 | #+BEGIN_SRC | sh       | :results | raw | drawer |
|      2 | #+BEGIN_SRC | css      |          |     |        |
|      2 | #+BEGIN_SRC | dot      |          |     |        |
|      2 | #+BEGIN_SRC | python   |          |     |        |
|      2 | #+BEGIN_SRC | sql      |          |     |        |
|      3 | #+BEGIN_SRC | C++      |          |     |        |
|      4 | #+BEGIN_SRC | sh       |          |     |        |
|      8 | #+BEGIN_SRC | sh       |          |     |        |
|      8 | #+BEGIN_SRC | sh       |          |     |        |
|     11 | #+BEGIN_SRC | ruby     |          |     |        |
|     12 | #+BEGIN_SRC | C        |          |     |        |
|     17 | #+BEGIN_SRC | python   |          |     |        |
|     36 | #+BEGIN_SRC | plantuml |          |     |        |
|     48 | #+BEGIN_SRC | java     |          |     |        |
|     77 | #+BEGIN_SRC | sh       |          |     |        |
|    111 | #+BEGIN_SRC | C++      |          |     |        |
|    266 | #+BEGIN_SRC | python   |          |     |        |
|    299 | #+BEGIN_SRC | sh       |          |     |        |
|   1059 | #+BEGIN_SRC | js       |          |     |        |
|   3167 | #+BEGIN_SRC | python   |          |     |        |
| 199711 | #+BEGIN_SRC | sh       |          |     |        |

** Programming language sum
#+NAME: table-sum
#+BEGIN_SRC python :var table=source-table :exports both
    from collections import  Counter
    total = Counter()
    total_sum = 0
    for x in table:
        count=x[0]
        total_sum += count
        name=x[2]
        total[name] += count
    res = dict(total.most_common())
    res["total"]=total_sum
    return list(res.items())
#+END_SRC
#+NAME: programming-language-table
#+RESULTS: table-sum
| sh       | 200108 |
| python   |   3452 |
| js       |   1059 |
| C++      |    114 |
| java     |     48 |
| plantuml |     36 |
| C        |     12 |
| ruby     |     11 |
| css      |      2 |
| dot      |      2 |
| sql      |      2 |
| latex    |      1 |
| shell    |      1 |
| total    | 204848 |


#+begin_src sh
grep -i END_SRC clines.txt 
#+end_src
#+NAME: end-source-table
#+RESULTS:
|      1 | #+end_src |
|      4 | #+END_SRC |
|      8 | #+END_SRC |
|     10 | #+END_SRC |
|    319 | #+END_SRC |
|    345 | #+END_SRC |
| 204161 | #+END_SRC |

** end count 
#+NAME: end-src-sum
#+BEGIN_SRC python :var table=end-source-table :exports both
  total = 0
  for x in table:
      count=x[0]
      total += count
  return total
#+END_SRC

#+RESULTS: end-src-sum
: 204848

We can now see that the results are the same for the begin and end tags.

*** construct graph of lines as grammar, treat lines as tokens, tokenize and compare.

Now we have BEGIN and END src blocks what do we look at now?

This is interesting : lets look at * in the org mode files as headlines
  40872 ***** Neon
  40872 ***** OpenCL
    #grep -e '^\\*\\*' clines.txt  |sort -n |tail -10
    #grep -e '^\s*[0-9]*\s*\* ' clines.txt   |sort |uniq -c |sort -n |tail -10
#+begin_src sh :results  verbatim
  grep -e '^\s*[0-9]*\s*\* ' clines.txt   |sort -n |tail -10
#+end_src

#+RESULTS:
#+begin_example
    116 * Management Firmware Settings
    131 * Boot/BIOS Firmware Settings
    144 * Setup with docker
    152 * Info
    152 * Qualcomm Cloud AI - MLPerf Inference - Datacenter and Edge servers
    191 * 2. Directions
    199 * 1. Problem
    577 * MLPerf Inference v1.0 - Image Classification - TFLite
   1623 * MLPerf Inference - Image Classification - TFLite
   2633 * MLPerf Inference - Image Classification - ArmNN-TFLite
#+end_example

#+NAME two stars
#+begin_src sh :results  verbatim
  grep -e '^\s*[0-9]*\s*\*\* ' clines.txt   |sort -n |tail -10
#+end_src

#+RESULTS:
#+begin_example
    228 ** Requirements
    286 ** Benchmark via the "neoclassical" CK interface
    290 ** Optimizations
    338 ** Instructions for Auditors
    412 ** Prerequisites
    459 ** Model
    491 ** Dataset
   2023 ** SingleStream
   2532 ** Single Stream
  13499 ** Dependent CM scripts
#+end_example

#+NAME 3 stars
#+begin_src sh :results  verbatim
  grep -e '^\s*[0-9]*\s*\*\*\* ' clines.txt   |sort -n |tail -10
#+end_src

#+RESULTS:
#+begin_example
    304 *** Downloading / obtaining the dataset
    307 *** Downloading / obtaining the model
    325 *** Run with New Weights
    334 *** Script workflow, dependencies and native scripts
    406 *** Customization
    406 *** Script output
    409 *** Description
    481 *** Run Inference through LoadGen
   1429 *** ResNet50
   4555 *** Workloads
#+end_example

#+NAME 4 stars
#+begin_src sh :results  verbatim
  grep -e '^\s*[0-9]*\s*\*\*\*\* ' clines.txt   |sort -n |tail -10
#+end_src

#+RESULTS:
#+begin_example
    406 **** CM GUI
    406 **** CM installation
    406 **** CM modular Docker container
    406 **** CM Python API
    406 **** Information
    406 **** New environment keys (filter)
   4579 **** Compliance
  22883 **** Accuracy
  23500 **** Performance
  23742 **** "All-in-one"
#+end_example

#+NAME 5 stars
#+begin_src sh :results  verbatim
  grep -e '^\s*[0-9]*\s*\*\*\*\*\* ' clines.txt   |sort -n |tail -10
#+end_src

#+RESULTS:
#+begin_example
     52 ***** NUMA nodes per socket: NPS1
     53 ***** Determinism Control: Manual
     54 ***** DF Cstates: Auto
    152 ***** Edge - Q1 Pro
    406 ***** CM pull repository
    406 ***** CM script automation help
   8024 ***** Use a uniform target latency
  10571 ***** [[https://github.com/krai/ck-mlperf/tree/master/program/generate-target-latency][Estimate
  40872 
  40872 ***** OpenCL
#+end_example

#+NAME 6 stars
#+begin_src sh :results  verbatim
  grep -e '^\s*[0-9]*\s*\*\*\*\*\*\* ' clines.txt   |sort -n |tail -10
#+end_src

#+RESULTS:
#+begin_example
      1 ****** Loss function
      1 ****** Model initialization
      1 ****** Optimizer state initializer
      1 ****** Variable update function
      4 ****** SingleStream
      8 ****** Offline
     96 ****** precision fp16
     96 ****** precision mixed
  17829 ****** [[https://github.com/krai/ck-mlperf/tree/master/program/generate-target-latency][Estimate
  20376 ****** Use a uniform target latency
#+end_example

** Finally no 7 stars
#+NAME 7 stars
#+begin_src sh :results  verbatim
  grep -e '^\s*[0-9]*\s*\*\*\*\*\*\*\* ' clines.txt   |sort -n |tail -10
#+end_src

** Conclusion
we can now see that we have some very common structures, now we want to know which
one occurs inside which one. we can use the pandoc json file for that to get containment.
we can use jq to look for them.

We can also use the FTS to find chunks that co-occur

But we can also just grep stars out of the org files and get a structure
#+begin_src sh :results  verbatim
  find -type f -name \*.md\*.org -print0 | xargs -0 grep -H -e '^\*'  > outline.txt
  wc outline.txt

#+end_src

#+RESULTS:
:   322767  1054156 49978562 outline.txt

#+begin_src sh :results  verbatim
  cut -d: -f2 outline.txt > structure.txt
  wc structure.txt
#+end_src

#+RESULTS:
:  322767  964883 6412265 structure.txt
  
Now we can do a sliding window for first stars
#+begin_src sh :results  verbatim
  grep -e '^\* ' -C1 structure.txt > structure1.txt
  wc structure1.txt
#+end_src

#+RESULTS:
:   45413  163305 1018957 structure1.txt

#+begin_src sh :results  verbatim
  grep -e '^\*\* ' -C1 structure.txt > structure2.txt
  wc structure2.txt
#+end_src

#+RESULTS:
:   73082  279174 1753962 structure2.txt


#+begin_src sh :results  verbatim
  grep -e '^\*\*\* ' -C1 structure.txt > structure3.txt
  wc structure3.txt
#+end_src

#+RESULTS:
:   67557  215306 1407207 structure3.txt

#+begin_src sh :results  verbatim
  grep -e '^\*\*\*\* ' -C1 structure.txt > structure4.txt
  wc structure4.txt
#+end_src

#+RESULTS:
:  216164  473728 3233056 structure4.txt

#+begin_src sh :results  verbatim
  grep -e '^\*\*\*\*\* ' -C1 structure.txt > structure5.txt
  wc structure5.txt
#+end_src

#+RESULTS:
:  207139  529465 3406612 structure5.txt

#+begin_src sh :results  verbatim
  grep -e '^\*\*\*\*\*\* ' -C1 structure.txt > structure6.txt
  wc structure6.txt
#+end_src

#+RESULTS:
:   81876  248135 1567007 structure6.txt


Now we have enough basic information to look at the lines before and after a structure element to see how it occurs

#+begin_src sh :results  verbatim
python ./parse_structure.py |sort |uniq -c |sort -n
#+end_src

#+RESULTS:
#+begin_example
      1 10|1|Benchmarks for Data-Centric AI Development*,|Contributing to the DataPerf Benchmark Suite
      1 10|1|NOTE: Only ONE power supply is allowed to supply the DUT|Software Setup
      1 10|2|0.0.16 /(Last updated 28 April 2023)/|Introduction
      1 10|2|1. =cycle_min_mom=: minimum momentum in cycle phase 2.|Required Model Configuration Changes
      1 10|2|Configuring a Custom Seldon Core Secret*|How do you use it?
      1 10|2|- [[https://arxiv.org/abs/1506.04579][ParseNet Paper]]\\|License
      1 10|2|- [[https://arxiv.org/abs/1512.02325][SSD: Single Shot|License
      1 10|2|- [[https://arxiv.org/abs/1612.08242][YOLOv2 paper]]|License
      1 10|2|-|License
      1 10|2|Make sure that no files and directories exist in the project|Encrypting your project for submission
      1 10|2|of the repo:* 1. *Hardware Requirements* - Container images,|Hardware requirements
      1 10|2|(optional) >= 8.4 GA|Installation
      1 10|2|or Container (if container which image + tag)*:|Relevant Files
      1 10|2|set caching to =False= on steps that depend on *external|Configuring caching behavior of your pipelines
      1 10|2|Speed and Scalability:/*|3. BERT Pre-training with 1-bit Adam
      1 10|2|Symbols*|Server Issues
      1 10|2|This change disrupts existing pipeline schedules. After|Performance enhancements (#3207)
      1 10|3|and caching*|Pass any kind of data to your steps
      1 10|3|[boolean]|Sparse Attention
      1 10|3|it should use, the *default user details,* and more. The|Secret store environment variables
      1 10|3|/This example requires TensorRT 8.4 or later./|Using The =--layer-precisions= Option
      1 10|4|for the GPU hardware to be properly utilized. If you don't|1. *Specify a CUDA-enabled parent image in your =DockerSettings=*
      1 10|4|Registries*|Other configuration
      1 10|4|tailored to specific use cases/tools. With ZenML installed,|Stack Switching
      1 10|4|to score your submission on a workload, from the|Pytorch DDP
      1 10|5|dependent packages*|for Mac users*
      1 10|5|Inference with Tensor-Slicing:* For massive models such as|Inference with ZeroQuant:* For massive models with tens or
      1 10|6|Layer* - [[https://arxiv.org/abs/1807.03247][Arxiv paper by|- [[https://github.com/onnx/onnx][GitHub: ONNX]] -
      1 10|6|prompt highlighting the harms in the image:* " A female|represented in the images:*
      1 10|6|prompt highlighting the harms in the image:* "Baby lying in a|represented in the images:* Violent or Graphic Imagery,
      1 10|7|* Announce in slack, for instance, =onnx-general= channel. *|conda-forge package with the new ONNX version* * Conda builds of
      1 10|7|License.* You hereby grant, and agree to grant, to SAP a|License.* You hereby grant, and agree to grant, to SAP a
      1 10|8||JSON Text that matches the fields of the =RecordSet=.
      1 10|9|-|per-tensor dynamic range:* -
      1 11|10|and caching*|and caching*
      1 11|10|for Transformers:* For transformer-based models such as|Inference with Tensor-Slicing:* For massive models such as
      1 11|10|The results are summarized in the table below. The total|Speed and Scalability:/*
      1 11|1|Unlike Performance Mode, you cannot talk directly to the DUT|Custom Configuration
      1 11|2|engine generation* - If you generate a saved serialized|Building =trtexec=
      1 11|2|- [[https://arxiv.org/abs/1506.01497][Faster R-CNN]]|License
      1 11|2|- [[https://arxiv.org/abs/1512.02325][SSD: Single Shot|License
      1 11|2|- [[https://arxiv.org/abs/1706.03762][Transformer]]|License
      1 11|2|- [[https://arxiv.org/abs/1810.04805][BERT]]|License
      1 11|2|- [[https://arxiv.org/pdf/2010.04159.pdf][Deformable DETR]]|License
      1 11|2|to inform their work or research. The way a given task is|Codebase
      1 11|3|[list of integer]|Monitoring Module (TensorBoard, WandB, CSV)
      1 11|3|notes:/*|Building the binaries
      1 11|3|notes:/*|Further reading
      1 11|3|The first time a training is run nnU-Net will extract the|Automatically determine the best configuration
      1 11|6|Transformation*: This flag primarily defines various offsets|Malformed Boxes by +1*: Some legacy implementations of ROI
      1 11|9|- [[https://arxiv.org/pdf/1512.03385.pdf][Deep Residual|-
      1 11|9|Trackers*|Stores*
      1 12|1|by [[https://cKnowledge.io/@gfursin][Grigori Fursin]] on|MLPerf Inference v1.0 - Object Detection - TFLite (with Coral EdgeTPU
      1 12|1|The nnU-Net default is to perform 'CT' normalization for CT|How to implement custom normalization strategies?
      1 12|2|all processes must call this method and not just the|DeepSpeed Configuration
      1 12|2|If you upgraded to ZenML v0.45.2 and are experiencing|What's Changed
      1 12|2|If you upgraded to ZenML v0.45.2 or v0.45.3 and are|What's Changed
      1 12|2|it is not recommended to continue using MLflow older than|Breaking Changes
      1 12|2||License
      1 12|2|of Tensors as Input:/*|Auto-Generated Model Configuration
      1 12|2|over *pipeline-level* defined hooks. {% endhint %}|Accessing step information inside a hook
      1 12|2|The current MLPerf Inference Results Chair is Guenther|Common issues
      1 12|3|[dictionary]|Scheduler Parameters
      1 12|3|*Polygraphy supports only Python 3.6 and later.* *Before|Installing Prebuilt Wheels
      1 12|3|The Comparator is designed for scenarios where you need to|Data Loaders
      1 12|3|The following commands rely on the content of this folder,|2. JPQD-BERT-large-99
      1 12|4|=pipeline.with_config(...)=*|to migrate*: Replaced with the new =pipeline.run(config_path=...)=.
      1 12|4|Runners may reuse their output buffers. Thus, if you need|Writing A Custom Runner
      1 12|4|=step.with_return_materializer(...)=*|to migrate*: Simply remove the =with_return_materializer= method
      1 12|4|the =enable_xxx= decorators*|to migrate*: Simply remove the decorator and pass something like
      1 12|4|the =requirements= and =required_integrations= parameters*|to migrate*: Simply remove the parameters and use the
      1 12|9|>= 2.8\\|>= 1.10.1\\
      1 12|9|[boolean]|[string]
      1 12|9||Speed*: (0.37s step-time, 15.3K wps) on /K40m/ & (0.17s
      1 12|9||Speed*: (2.1s step-time, 3.4K wps) on /Nvidia K40m/ & (0.7s
      1 13|11|[list of integer]|[list of integer]
      1 13|11|network* - If you have a model saved as a UFF file, ONNX|engine generation* - If you generate a saved serialized
      1 13|2|A pivotal function responsible for forwarding queries to|Device Management
      1 13|2|DeepSpeed version =0.3.15= introduced automatic external|Extracting weights
      1 13|2|gains/*, and */memory footprint reduction/* from using|Training GPT-2 with the Original Megatron-LM
      1 13|2|Run the network script but allow TensorRT to ignore|See Also
      1 13|2|with various tools for each category. Once code is|Available integrations
      1 13|3|(based on Vladimir Agafonkin's excellent|d3-array
      1 13|3|[boolean]|Autotuning
      1 13|3|*cpu_offload* is deprecated and will be removed in future,|Parameter offloading
      1 13|3||DataSource
      1 13|3||extract
      1 13|3||Field
      1 13|3||fileProperty
      1 13|3||FileSet
      1 13|3||RecordSet
      1 13|3|To the fullest extent permitted under applicable law, your|Obligation.* You acknowledge that SAP is under no obligation to use
      1 13|6|Layers*|Layers* Other layers are inherited from =BaseQuantizeWrapper=
      1 13|7|A clear and concise description of what the bug is.|Information* What version of Triton are you using?
      1 13|7|If applicable, add screenshots to help explain your|Version* Version information of the GaNDLF package in the
      1 13|8|If applicable, add screenshots to help explain your|(please complete the following information):* - OS:
      1 13|9|Aware Training*|
      1 14|11|[integer]|[boolean]
      1 14|11|[string]|[string]
      1 14|13|[integer]|[list of integer]
      1 14|2|A permanent ban from any sort of public interaction|Attribution
      1 14|2||How to Use MedPerf
      1 14|2|register missing secrets for your stack*|Set scope for secrets
      1 14|2|This method readies input data for inference, involving|DeviceGptJ: Specialized Device
      1 14|3|A private, written warning from community leaders,|2. Warning
      1 14|3|A temporary ban from any sort of interaction or public|4. Permanent Ban
      1 14|3|A warning with consequences for continued behavior. No|3. Temporary Ban
      1 14|3|[boolean]|Asynchronous I/O
      1 14|3|[dictionary]|Communication options
      1 14|3|[dictionary]|Elastic Training Config (V0.1 and V0.2)
      1 14|3|for NVIDIA GPU set up*: You may have to install the|Building Docker Image
      1 14|3|[string]|Activation Checkpointing
      1 14|4|vs ZeRO-Offload:* DeepSpeed first included offloading|Allocating Massive Megatron-LM Models
      1 14|5|[boolean]|params/*: [various]
      1 14|8|[dictionary]|[dictionary]
      1 15|10||(optional) >= 8.4 GA
      1 15|12|This virtual method necessitates subclass overrides for|Allocation management for input or output data, inclusive
      1 15|14|[integer]|[integer]
      1 15|2||Known Issues
      1 15|2|must share the same geometry with their corresponding|Supported file formats
      1 15|3|[boolean]|Automatic mixed precision (AMP) training options
      1 15|3|[dictionary]|Compression
      1 15|3|[integer]|Optimizer offloading
      1 15|3|of XTC methods:* To accommodate users who do not have a|3.1 One-bit or Two-bit BERT-base (12-layer) with 8-bit activation
      1 15|3|rematerialization.* When fusing kernels of the different|(b) Invertible operators to save memory and run large batches
      1 15|4|broadcasting* (tensor C should be unidirectional|Version
      1 15|4|The closed-power would need additional setup for power|Closed
      1 15|6|- complete refactoring of all the files to make the bot|Bugs:* - CLA check not updated to success when all the
      1 15|9|for Resource Constrained Systems:* Models such as Bloom|Optimizations:* When applicable, MII automatically applies
      1 16|13|[boolean]|*cpu_offload* is deprecated and will be removed in future,
      1 16|15|[integer]|[integer]
      1 16|1|-|License
      1 16|2|- [[https://arxiv.org/abs/1504.08083][Original ROI|License
      1 16|2|- [[https://arxiv.org/abs/1504.08083][ROI Pooling|License
      1 16|2|-|License
      1 17|14|[integer]|[boolean]
      1 17|15|This function primarily focuses on processing|This method spearheads batch-based inference, utilizing
      1 17|16|[integer]|[integer]
      1 17|16|[integer]|[string]
      1 17|2|document. - [ ] If my change requires a change to|Types of changes
      1 17|3|[boolean]|Data Type options
      1 17|4|(i.e., Numpy-style) broadcasting*; for more details|Version
      1 18|13|You represent that, other than the Third Party|To the fullest extent permitted under applicable law, your
      1 18|14|[integer]|[integer]
      1 18|17|[string]|[integer]
      1 18|2||Dependency
      1 18|2|or *eval_data//train/*. Note that in MLPerf Tiny we|Detailed Usage
      1 18|2|or strictly-negative*; the domain must not include or|scaleLog(/domain/, /range/)
      1 18|4|[dictionary]|Weight Quantization
      1 19|3|[boolean]|FP16 training options
      1 19|3|[dictionary]|Checkpoint options
      1 19|3|[dictionary]|Curriculum Learning
      1 19|3|[integer]|Flops Profiler
      1 19|4|[dictionary]|Activation Quantization
      1 19|4|[dictionary]|Channel Pruning
      1 19|4|[dictionary]|Head Pruning
      1 19|4|[dictionary]|Row Pruning
      1 19|4|[dictionary]|Sparse Pruning
      1 19|7|[integer]|[boolean]
      1 19|8|This method augments a sample library to a device,|Configuring the server is facilitated by this method, which
      1 20|15|Training (MobileNetV1-SSD)**|Quantization (ResNet50)**
      1 20|3|[float]|ZeRO Optimizations for FP16 Training
      1 20|8|[dictionary]|[integer]
      1 21|10||
      1 21|19|[integer]|[integer]
      1 21|3|and allow *large batch sizes*. Alternative|How to use ZeRO-Inference
      1 21|3|key to the DeepSpeed JSON configuration. A full|Training a 1.5B Parameter GPT-2 model
      1 2|1|👭 4. Start the Dashboard|🗺 Roadmap
      1 2|1|About|
      1 2|1|Adding Experimental Operators [Deprecated - as of v1.5 experimental|Submit an Issue with a proposal explaining the motivation and plan. It
      1 2|1|Adversarial Nibbler|Participating in the DataPerf Challenges
      1 2|1|Bug Fixes + Refactor|0.3.4
      1 2|1|Bug Fixes + Refactor|0.3.5
      1 2|1|Bug Fixes + Refactor|0.3.6
      1 2|1|Bug Fixes + Refactor|0.3.7
      1 2|1|Bug Fixes|Release 0.2.0
      1 2|1|Bug Fixes|Release 0.2.1
      1 2|1|Build and deploy HabanaLabs MLPERF training 2.1 container in the|Resnet50
      1 2|1|Build and deploy HabanaLabs MLPERF training 3.0 container in the|Resnet50
      1 2|1|Building TensorRT-OSS|References
      1 2|1|Changed|TensorRT 8.5 GA Release - 2022-11-2
      1 2|1|Checkpoint|Prepare enviroment
      1 2|1|Checkpoint|Running the model
      1 2|1|Closed Division: Offline evaluation of a submission|Baselines
      1 2|1|Common Errors|Testing
      1 2|1|🙌 Community Contributions|0.7.3
      1 2|1|Cost Sensitive Scenarios|Deployment Options
      1 2|1|Customized Inference Kernels for Boosted Compute Efficiency of|Kernel-Fusion
      1 2|1|Dataset|Prepare environment
      1 2|1|DeepSpeed Configuration|Launching DeepSpeed Training
      1 2|1|Dell Submission Systems|GPU Implementations
      1 2|1|Evaluation Metric|/M/ is user defined, but Dynabench will host two leaderboards per
      1 2|1|Evaluation|Recipe
      1 2|1|Execute complete pipeline|Guidelines v0.5
      1 2|1|Full Dimensions + Dynamic Shapes|
      1 2|1|🧰 How the example is implemented|🖥 Run it locally
      1 2|1|How to update an existing dataset|Example dataset conversion scripts
      1 2|1|Installation|Guidelines (alpha version)
      1 2|1|Installation|MLCube execution
      1 2|1|Install dependencies|CLI
      1 2|1|Install loadgen|Dataset
      1 2|1|Install or detect ImageNet dataset|Install TF
      1 2|1|Latest News|Extreme Speed and Scale for DL Training and Inference
      1 2|1|License|Credits
      1 2|1|Local settings|Examples
      1 2|1|Major Features and Improvements|Release 0.1.0
      1 2|1|Major Features and Improvements|Release 0.2.2
      1 2|1|Major Features and Improvements|Release 1.0.0
      1 2|1|Megatron-DeepSpeed on AzureML|Workspace Setup
      1 2|1|MII-Azure Deployment|Concluding Remarks
      1 2|1|Mounting Local Repository|Submitting PRs
      1 2|1|New Contributors|0.6.2
      1 2|1|NEW: The KiTS23 Challenge is Underway!|KiTS19
      1 2|1|[NOTICE] POSSIBLY OUTDATED|Web Interface
      1 2|1|onScroll event|Methods
      1 2|1|Operator Changelog|ai.onnx.ml
      1 2|1|Options|displayInput : default=true | false=hide input. * displayPrevious :
      1 2|1|➕ Other updates, additions and fixes|0.5.7
      1 2|1|Overview|0.5.0
      1 2|1|Performance Mode vs. Energy Mode|Hardware Setup
      1 2|1|prepare env|Setup with docker
      1 2|1|Pretrained backbone|Running the model
      1 2|1|Programming Languages|C#: https://silentorbit.com/protobuf/ * C#/.NET/WCF/VB:
      1 2|1|Proposal|Symbol generation and propagation
      1 2|1|Push the datatsets to the device|Now you can launch the app, select submission mode and press GO
      1 2|1|ResNet50 Offline - ONNXRuntime|BERT-Large
      1 2|1|Resolved issues|Notes
      1 2|1|Scaling out the training to 64 Gaudi2|GPT3-175B PT
      1 2|1|Scenarios|Alibaba Submission
      1 2|1|Scenarios|NEUCHIPS Submissions
      1 2|1|Serialization performance|The cpp performance can be improved by using
      1 2|1|Setup Conda Environment and Build Dependencies|Run Benchmark
      1 2|1|Shared data pipelines between JAX and PyTorch|FAQS
      1 2|1|Software Packages|Repository Contents
      1 2|1|Supported Operators|Installation
      1 2|1|Types of changes|[1.3.4] - 2023-02-02
      1 2|1|Unit Tests|Compiling
      1 2|1|Verify that environment parameters are set|Windows
      1 2|1|Video Usage Examples|Options
      1 2|1|What happened to the old nnU-Net?|Acknowledgements
      1 2|1|What's Changed|0.44.0
      1 2|1|What's changed|0.6.0
      1 2|1|What's Changed|0.6.1
      1 2|1|What's Changed|0.7.0
      1 2|1|What to expect in the next weeks and the new ZenML|0.3.7.1
      1 2|1|What to expect in the next weeks and the new ZenML|0.5.0rc2
      1 2|1|Windows|Features
      1 2|1|zfnet512|Overall Test Coverage
      1 22|3|[integer]|BFLOAT16 training options
      1 22|4|is now renamed to =DockerSettings=*|to migrate*: Rename =DockerConfiguration= to =DockerSettings= and
      1 23|10|[int]|[int]
      1 23|19|[integer]|[integer]
      1 23|3|you can rename plugin. ### A. JavaScript way|B. jQuery way
      1 24|18|[integer]|[integer]
      1 24|20|[boolean]|[boolean]
      1 24|4|is now renamed to =ResourceSettings=*|to migrate*: Rename =ResourceConfiguration= to =ResourceSettings=
      1 25|17|[boolean]|[boolean]
      1 25|18|[integer]|[integer]
      1 26|15|[integer]|[boolean]
      1 26|18|[boolean]|[dictionary]
      1 26|21|[boolean]|[boolean]
      1 27|25|[integer]|[integer]
      1 28|19|[float]|[boolean]
      1 28|2|(no default)|Objective C Generator =protoc= Options
      1 29|3|[boolean]|Logging
      1 30|3|[integer]|Optimizer Parameters
      1 31|13|[integer]|[string]
      1 31|30|[integer]|[integer]
      1 3|1|A demo to query inception model|Additional Resources
      1 3|1|Authentication for Google Cloud Container Registry|Installation
      1 3|1|Autotuning Metric|"throughput": training samples per second (calculated as
      1 3|1|B. Conversion to tfrecord|Workflow
      1 3|1|B. jQuery way|No conflict
      1 3|1|B. jQuery way|Real Usage Examples
      1 3|1|Breaking Changes|Changes in D3 5.0
      1 3|1|Can submission be structured using multiple files?|Citing AlgoPerf Benchmark
      1 3|1|Custom Monitoring|Note - Some Monitor backends don't support mixed sample values. Be
      1 3|1|Enabling proto2 features|Generated code
      1 3|1|Extensions|APIs
      1 3|1|Fractured Messages from the IO Manager|Bill of Materials
      1 3|1|Get the Results|Complinace Test
      1 3|1|*ImageDecoder*|Portable image format (PBM, PGM, PPM, PXM, PNM) Decoded images follow
      1 3|1|Install required packages|Running demoDiffusion
      1 3|1|Max Train Batch Size|Model Parallelism Size
      1 3|1|Optimization|Run Resnet50
      1 3|1|Options For Video|Events
      1 3|1|Run benchmark with SLURM for NVIDIA DGXA100|3. Model
      1 3|1|Run GPT3 on HLS-Gaudi2-N48-PT system|UNet3D
      1 3|1|Sample =--help= options|Models other than ResNet-50 with custom configuration
      1 3|1|Setting the correct number of Workers for data augmentation|Installation instructions
      1 3|1|SSD-MobileNet-v1|Inference
      1 3|1|Task =infer=|An output folder is created (=predictions=) * For each video, a csv
      1 3|1|Task =sanity_check=|a csv file doesn't have a corresponding folder in the =frames= folder,
      1 3|1|Using Singularity/Apptainer instead of Docker|Getting Started
      1 3|1|V2|Example
      1 3|1|View MLPerf inference v3.1 result|
      1 3|1|WIP: DEFINE PROD AWS ACCOUNT OWNER ID = {prod_aws_account_owner_id}|Task Owner Setup
      1 3|2|1.2 Challenges in applying error-compensation to Adam|2. Compressing communication with 1-bit Adam
      1 3|2|1.3 fixed_discrete schedule|2. Curriculum learning for Megatron-LM GPT-2 pre-training
      1 32|19|* *gradient_accumulation* * number of|[integer]
      1 3|2|1Cycle Learning Rate Schedule|Simplified Data Loader
      1 3|2|2.2 Addressing system challenges for 1-bit Adam|3. Benefits of 1-bit Adam on communication-constrained systems
      1 3|2|2. SW requirements|Run DLRM
      1 3|2|3. Application Samples|Known Limitations
      1 3|2|Accuracy|Known issues
      1 3|2|Accuracy or performance changes|Released Versions
      1 3|2|accuracy target|Evaluate accuracy on Android devices
      1 3|2|ACR container registry|Authentication Methods
      1 3|2|Added|v0.0.3 (2020-07-15)
      1 3|2|Added|v0.10.0 (2019-10-28)
      1 3|2|Added|v0.11.1 (2020-02-11)
      1 3|2|Added|v0.1.1 (2020-02-11)
      1 3|2|Added|v0.13.2 (2020-03-20)
      1 3|2|Added|v0.20.11 (2020-09-25)
      1 3|2|Added|v0.2.0 (2020-04-15)
      1 3|2|Added|v0.20.5 (2020-09-16)
      1 3|2|Added|v0.21.0 (2020-11-30)
      1 3|2|Added|v0.28.4 (2021-04-23)
      1 3|2|Added|v0.29.1 (2021-04-28)
      1 3|2|Added|v0.3.11 (2021-07-14)
      1 3|2|Added|v0.3.12 (2021-08-24)
      1 3|2|Added|v0.3.16 (2022-02-23)
      1 3|2|Added|v0.3.18 (2022-03-31)
      1 3|2|Added|v0.3.20 (2022-07-12)
      1 3|2|Added|v0.34.0 (2021-11-22)
      1 3|2|Added|v0.34.1 (2021-11-24)
      1 3|2|Added|v0.37.0 (2022-04-18)
      1 3|2|Added|v0.38.1 (2022-06-22)
      1 3|2|Added|v0.3.9 (2021-04-20)
      1 3|2|Added|v0.45.0 (2023-01-12)
      1 3|2|Added|v0.9.2 (2019-10-1)
      1 3|2|Adding Tests|Design Principles
      1 3|2|Additional Documentation|Contributing
      1 3|2|=additional_files.tar.gz= (Optional)|Preparing an MLCube for hosting
      1 3|2|Additional styles|Call the plugin
      1 3|2|Advanced Options|Telemetry
      1 3|2|Advanced server configuration options|Run the ZenML server with Docker
      1 3|2|A function with attributes|Parsing
      1 3|2|ai.onnx.ml|ai.onnx.ml
      1 3|2|ai.onnx.preview.training|ai.onnx (default)
      1 3|2|Alternative approach: Host some model weights in GPU memory|Model Scaling on 1 GPU
      1 3|2|Amazon Linux 2|Set up user-space dependencies
      1 3|2|A Possible Solution|Running The Example
      1 3|2|Attribute Types|Training Related Information
      1 3|2|Authentication Methods|How do you use it?
      1 3|2|Automate build reuse by connecting a code repository|Customize the Docker building
      1 3|2|Automatic loss scaling with mixed precision|Training Optimizers
      1 3|2|AutoSinian|Benchmarks
      1 3|2|AWS Federation Token|Auto-configuration
      1 3|2|AWS Model Repository|Deploy Prometheus and Grafana
      1 3|2|AWS Model Repository|Deploy the Triton Inference Server
      1 3|2|Azure Access Token|Auto-configuration
      1 3|2|Benchmark TensorRT Engine|Inference
      1 3|2|(b) Invertible operators to save memory and run large batches|Overlapping I/O with Computation through Asynchronous Prefetching
      1 3|2|BoundingBox|Properties
      1 3|2|Build from Source - Without Maven|Compatibility Notice
      1 3|2|Building Python API Documentation Locally|Deprecation Policy
      1 3|2|Building the API Docs locally|Contributors
      1 3|2|Build the container and push to a docker registry.|Running training
      1 3|2|Build your own custom orchestrator|Implementation guide
      1 3|2|Caching of Cloud Storage|Model Versions
      1 3|2|Calibration Datasets|Quantization
      1 3|2|Cannot build for Android|Resolved issues
      1 3|2|C|Benchmark datasets
      1 3|2|Changed|[[https://github.com/NVIDIA/TensorRT/releases/tag/21.04][21.04]] -
      1 3|2|Changed|[[https://github.com/NVIDIA/TensorRT/releases/tag/22.02][22.02]] -
      1 3|2|Changed|Removed
      1 3|2|Changed|v0.0.2 (2020-07-05)
      1 3|2|Changed|v0.0.4 (2020-07-18)
      1 3|2|Changed|v0.0.5 (2020-07-21)
      1 3|2|Changed|v0.0.7 (2022-08-10)
      1 3|2|Changed|v0.10.1 (2019-10-31)
      1 3|2|Changed|v0.1.0 (2020-02-11)
      1 3|2|Changed|v0.10.3 (2019-11-18)
      1 3|2|Changed|v0.11.2 (2020-02-11)
      1 3|2|Changed|v0.12.0 (2020-03-06)
      1 3|2|Changed|v0.1.3 (2020-02-26)
      1 3|2|Changed|v0.13.3 (2020-03-20)
      1 3|2|Changed|v0.14.0 (2020-04-09)
      1 3|2|Changed|v0.14.1 (2020-04-17)
      1 3|2|Changed|v0.20.6 (2020-09-18)
      1 3|2|Changed|v0.23.2 (2021-02-11)
      1 3|2|Changed|v0.23.3 (2021-02-13)
      1 3|2|Changed|v0.2.5 (2020-09-21)
      1 3|2|Changed|v0.2.6 (2020-09-25)
      1 3|2|Changed|v0.2.8 (2020-10-08)
      1 3|2|Changed|v0.28.3 (2021-04-22)
      1 3|2|Changed|v0.29.0 (2021-04-28)
      1 3|2|Changed|v0.31.0 (2021-07-02)
      1 3|2|Changed|v0.3.1 (2021-02-12)
      1 3|2|Changed|v0.3.21 (2022-08-19)
      1 3|2|Changed|v0.33.0 (2021-09-16)
      1 3|2|Changed|v0.35.0 (2022-01-06)
      1 3|2|Changed|v0.36.2 (2022-03-31)
      1 3|2|Changed|v0.36.4 (2022-04-12)
      1 3|2|Changed|v0.37.1 (2022-04-18)
      1 3|2|Changed|v0.37.2 (2022-04-20)
      1 3|2|Changed|v0.3.8 (2021-04-15)
      1 3|2|Changed|v0.40.0 (2022-07-29)
      1 3|2|Changed|v0.40.2 (2022-08-12)
      1 3|2|Changed|v0.45.1 (2023-01-19)
      1 3|2|Changed|v0.45.2 (2023-01-25)
      1 3|2|Changed|v0.46.0 (2023-02-10)
      1 3|2|Changed|v0.47.0 (2023-03-28)
      1 3|2|Changed|v0.6.0 (2019-07-17)
      1 3|2|Changed|v0.8.0 (2019-09-18)
      1 3|2|Changed|v0.8.1 (2019-09-26)
      1 3|2|Changed|v0.9.0 (2019-09-30)
      1 3|2|Changed|v0.9.1 (2019-10-1)
      1 3|2|Changed|v0.9.3 (2019-10-1)
      1 3|2|Changed|v0.9.5 (2019-10-9)
      1 3|2|Changed|v0.9.6 (2019-10-15)
      1 3|2|Changed|v0.9.8 (2019-10-24)
      1 3|2|Check/clean CM cache|Add CM interface for new projects and papers
      1 3|2|Checking a Large ONNX Model >2GB|Running Shape Inference on an ONNX Model
      1 3|2|Checking output format|Requirements
      1 3|2|Checklist|Removing operator or function
      1 3|2|C. jQuery way|Video Usage Examples
      1 3|2|=CK_IMAGE_COUNT=|Detection result file format
      1 3|2|=CK_SKIP_IMAGES=|Models
      1 3|2|Clean up|Librispeech
      1 3|2|CMake variables|Common Errors
      1 3|2|Coding Style|Contributing Process
      1 3|2|Coding your submission|Run your submission
      1 3|2|Combining ZeRO-Offload and DeepSpeed MoE for very large models|Random Token Selection
      1 3|2|Communication Logging|Sparse Attention
      1 3|2|Communication Overlapping|Training Features
      1 3|2|Comparing Different Models|Further Reading
      1 3|2|Comparing Per-Layer Outputs Between ONNX-Runtime And TensorRT|Further Reading
      1 3|2|Configuring ZeRO configurations|Autotuning Output
      1 3|2|Connect to the deployed ZenML server|ZenML Helm Deployment Scenarios
      1 3|2|Considerations for change-compatibility|How to enable /explicit presence/ in proto3
      1 3|2|Contiguous Memory Optimization (CMO)|ZeRO-Offload
      1 3|2|CPU-only container composition|Build it yourself
      1 3|2|Create SQS queue|Starting the app
      1 3|2|Create TFRecord for ADE20K|Train
      1 3|2|C|Run instructions
      1 3|2|*Current version:* alpha|Requirements
      1 3|2|*Current version:* beta|Requirements
      1 3|2|*Custom Quantization with 'Custom Q/DQ Insertion Case' (optimal)*|*Library provided custom Q/DQ insertion cases*
      1 3|2|[[./d3-array/transform.md][Transform]]|[[./d3-axis.md][d3-axis]]
      1 3|2|[[./d3-geo/math.md][Spherical math]]|[[./d3-hierarchy.md][d3-hierarchy]]
      1 3|2|[[./d3-interpolate/zoom.md][Zoom interpolation]]|[[./d3-path.md][d3-path]]
      1 3|2|[[./d3-scale-chromatic/sequential.md][Sequential]]|[[./d3-selection.md][d3-selection]]
      1 3|2|[[./d3-scale/point.md][Point scales]]|[[./d3-scale-chromatic.md][d3-scale-chromatic]]
      1 3|2|[[./d3-selection/namespaces.md][Namespaces]]|[[./d3-shape.md][d3-shape]]
      1 3|2|[[./d3-shape/stack.md][Stacks]]|[[./d3-time.md][d3-time]]
      1 3|2|Data Serialization|Initializer, default value
      1 3|2|Datasets|Running
      1 3|2|Data Validator Flavors|How to use it
      1 3|2|DCO|CI Pipelines
      1 3|2|Deadlock|Client Issues
      1 3|2|Default Max Batch Size and Dynamic Batcher|Datatypes
      1 3|2|/delaunay/.inedges|Delaunay.from(/points/, /fx/, /fy/, /that/)
      1 3|2|Dependency and Integration Version Updates|What's Changed
      1 3|2|Deploy as a Metrics Generator|Federating your model using OpenFL
      1 3|2|Deploying a ZenML Server|Using the ZenML CLI to connect to a deployed ZenML Server
      1 3|2|Deprecated|TensorRT 8.4 GA Release - 2022-6-6
      1 3|2|Different system configurations that use the same GPU configuration|NUMA configuration
      1 3|2|Direct download links of files on GitHub|Synapse hosting
      1 3|2|Disable Implicit Auth Methods for Service Connectors by Default|What's Changed
      1 3|2|Distributed training|Expected Output(s)
      1 3|2|*(DOCKER)* Run the benchmark:|Limitations and Best Practices for Running MLPerf
      1 3|2|*(DOCKER)* Run the benchmark:|Run the 3D U-Net benchmark
      1 3|2|*(DOCKER)* Run the benchmark:|Run the BERT benchmark
      1 3|2|*(DOCKER)* Run the benchmark:|Run the DLRM benchmark
      1 3|2|*(DOCKER)* Run the benchmark:|SSD-Resnet34
      1 3|2|docs-site Action Development documentation|python-publish action
      1 3|2|Download dataset|Reproduce results
      1 3|2|Download the dataset and the model|Running your first benchmark
      1 3|2|Download URLs for a particular category|Download images for a given category
      1 3|2|easeBack.overshoot(/s/)|easeBounce
      1 3|2|easeCircleInOut|easeElastic
      1 3|2|easeCubicInOut|easeSin
      1 3|2|easeElastic.period(/p/)|easeBack
      1 3|2|easeExpInOut|easeCircle
      1 3|2|easePoly.exponent(/e/)|easeQuad
      1 3|2|easeQuadInOut|easeCubic
      1 3|2|easeSinInOut|easeExp
      1 3|2|ECR container registry|Authentication Methods
      1 3|2|EfficientNet-B3|[[https://github.com/NVIDIA/TensorRT/tree/main/tools/tensorflow-quantization/examples/inception][Inception]]
      1 3|2|Eigenvalue Parameters|How to Use MoQ for GLUE Training Tasks
      1 3|2|Eligibility for voting|Candidacy process
      1 3|2|Enable or disable logs storing|Settings in ZenML
      1 3|2|Enabling GPUs|MLCubes
      1 3|2|Ensemble Model Instance Groups|CUDA Compute Capability
      1 3|2|Ensemble Scheduler|Optimization Policy
      1 3|2|Evaluate a custom node|Implementation details
      1 3|2|Example 6: Tune throughput with multi-streaming|Tool command line arguments
      1 3|2|Example build invocation|Options
      1 3|2|Example|Extract
      1 3|2|Example|Model versioning
      1 3|2|Examples|Raw Binary Request
      1 3|2|Example Usage|GRPC
      1 3|2|Existing hosted SQL database|Configuration file templates
      1 3|2|Expected output|Demonstration case 2: Dynamic batching
      1 3|2|Exporters|Advanced
      1 3|2|Extract Build Artifacts|Building on Unsupported Platforms
      1 3|2|Features|Configure Contributor License Agreement within two minutes
      1 3|2|Features|Dependencies
      1 3|2|FeTS Challenge|Pilot Studies
      1 3|2|[[file:models_and_code/checkpoints/mobilenet_edgetpu_224_1.0][mobilenet_edgetpu_224_1.0]]|Accuracy
      1 3|2|Filling in the rest of the config|Filling in =eval_config=
      1 3|2|Fine-tuning Results|Enabling DeepSpeed's Transformer Kernel for better Throughput
      1 3|2|Fixed|0.42.2 (2022-09-22)
      1 3|2|Fixed|Removed
      1 3|2|Fixed|v0.0.1 (2022-06-23)
      1 3|2|Fixed|v0.0.6 (2020-07-21)
      1 3|2|Fixed|v0.10.2 (2019-11-11)
      1 3|2|Fixed|v0.10.4 (2019-12-4)
      1 3|2|Fixed|v0.10.5 (2019-12-9)
      1 3|2|Fixed|v0.10.6 (2019-12-11)
      1 3|2|Fixed|v0.11.0 (2020-01-28)
      1 3|2|Fixed|v0.11.3 (2020-02-25)
      1 3|2|Fixed|v0.1.2 (2020-02-19)
      1 3|2|Fixed|v0.13.0 (2020-03-17)
      1 3|2|Fixed|v0.13.1 (2020-03-17)
      1 3|2|Fixed|v0.15.0 (2020-05-05)
      1 3|2|Fixed|v0.16.0 (2020-06-11)
      1 3|2|Fixed|v0.17.0 (2020-07-20)
      1 3|2|Fixed|v0.20.0 (2020-09-08)
      1 3|2|Fixed|v0.20.10 (2020-09-23)
      1 3|2|Fixed|v0.20.1 (2020-09-09)
      1 3|2|Fixed|v0.20.12 (2020-10-01)
      1 3|2|Fixed|v0.20.13 (2020-10-08)
      1 3|2|Fixed|v0.20.2 (2020-09-11)
      1 3|2|Fixed|v0.20.3 (2020-09-11)
      1 3|2|Fixed|v0.20.4 (2020-09-14)
      1 3|2|Fixed|v0.20.7 (2020-09-22)
      1 3|2|Fixed|v0.20.8 (2020-09-22)
      1 3|2|Fixed|v0.20.9 (2020-09-22)
      1 3|2|Fixed|v0.21.1 (2021-01-12)
      1 3|2|Fixed|v0.2.1 (2020-06-10)
      1 3|2|Fixed|v0.22.0 (2021-01-20)
      1 3|2|Fixed|v0.23.0 (2021-02-02)
      1 3|2|Fixed|v0.2.3 (2020-06-17)
      1 3|2|Fixed|v0.23.4 (2021-02-15)
      1 3|2|Fixed|v0.24.0 (2021-02-19)
      1 3|2|Fixed|v0.24.1 (2021-02-22)
      1 3|2|Fixed|v0.2.4 (2020-09-14)
      1 3|2|Fixed|v0.24.2 (2021-02-25)
      1 3|2|Fixed|v0.25.0 (2021-03-01)
      1 3|2|Fixed|v0.26.0 (2021-03-30)
      1 3|2|Fixed|v0.26.1 (2021-04-01)
      1 3|2|Fixed|v0.2.7 (2020-09-29)
      1 3|2|Fixed|v0.28.0 (2021-04-20)
      1 3|2|Fixed|v0.28.1 (2021-04-22)
      1 3|2|Fixed|v0.28.2 (2021-04-22)
      1 3|2|Fixed|v0.28.5 (2021-04-23)
      1 3|2|Fixed|v0.28.6 (2021-04-23)
      1 3|2|Fixed|v0.28.7 (2021-04-26)
      1 3|2|Fixed|v0.2.9 (2021-02-01)
      1 3|2|Fixed|v0.29.2 (2021-04-30)
      1 3|2|Fixed|v0.30.0 (2021-05-26)
      1 3|2|Fixed|v0.30.1 (2021-06-07)
      1 3|2|Fixed|v0.30.2 (2021-06-15)
      1 3|2|Fixed|v0.30.3 (2021-06-25)
      1 3|2|Fixed|v0.3.10 (2021-05-20)
      1 3|2|Fixed|v0.3.13 (2021-09-21)
      1 3|2|Fixed|v0.3.14 (2021-10-14)
      1 3|2|Fixed|v0.3.15 (2022-01-18)
      1 3|2|Fixed|v0.3.17 (2022-03-18)
      1 3|2|Fixed|v0.3.19 (2022-04-13)
      1 3|2|Fixed|v0.32.0 (2021-08-10)
      1 3|2|Fixed|v0.3.2 (2021-02-13)
      1 3|2|Fixed|v0.3.22 (2022-08-22)
      1 3|2|Fixed|v0.3.23 (2022-08-24)
      1 3|2|Fixed|v0.3.24 (2022-08-31)
      1 3|2|Fixed|v0.3.25 (2022-10-14)
      1 3|2|Fixed|v0.33.1 (2021-10-08)
      1 3|2|Fixed|v0.3.3 (2021-03-04)
      1 3|2|Fixed|v0.3.4 (2021-03-10)
      1 3|2|Fixed|v0.35.1 (2022-01-14)
      1 3|2|Fixed|v0.3.5 (2021-03-24)
      1 3|2|Fixed|v0.35.2 (2022-02-03)
      1 3|2|Fixed|v0.36.0 (2022-02-24)
      1 3|2|Fixed|v0.36.1 (2022-03-25)
      1 3|2|Fixed|v0.3.6 (2021-03-27)
      1 3|2|Fixed|v0.36.3 (2022-04-07)
      1 3|2|Fixed|v0.3.7 (2021-03-31)
      1 3|2|Fixed|v0.38.0 (2022-05-24)
      1 3|2|Fixed|v0.40.1 (2022-08-08)
      1 3|2|Fixed|v0.40.3 (2022-08-17)
      1 3|2|Fixed|v0.41.0 (2022-08-24)
      1 3|2|Fixed|v0.42.0 (2022-09-01)
      1 3|2|Fixed|v0.42.1 (2022-09-07)
      1 3|2|Fixed|v0.43.0 (2022-10-06)
      1 3|2|Fixed|v0.43.1 (2022-10-12)
      1 3|2|Fixed|v0.44.0 (2022-11-30)
      1 3|2|Fixed|v0.44.1 (2022-12-06)
      1 3|2|Fixed|v0.45.3 (2023-01-25)
      1 3|2|Fixed|v0.46.1 (2023-02-27)
      1 3|2|Fixed|v0.46.2 (2023-02-28)
      1 3|2|Fixed|v0.7.0 (2019-07-30)
      1 3|2|Fixed|v0.9.4 (2019-10-7)
      1 3|2|Fixed|v0.9.7 (2019-10-18)
      1 3|2|Fixes|21.03 Container Release - 2021-03-09
      1 3|2|Fixes|TensorRT 7.2.1 Release - 2020-10-20
      1 3|2|Fixes|TensorRT 8.2 EA Release - 2021-10-04
      1 3|2|Fixes|TensorRT 8.2 GA Release - 2021-11-23
      1 3|2|Foldes contents|Resources folder
      1 3|2|GCP OAuth 2.0 token|Auto-configuration
      1 3|2|GCR container registry|Authentication Methods
      1 3|2|GCS Permissions|Deploy Prometheus and Grafana
      1 3|2|generate results for accuracy and performance separately add|FP8 flow
      1 3|2|Generating Golden Values|Tips And Tricks
      1 3|2|Generating non-TFLite specific files (export_inference_graph.py)|Accuracy
      1 3|2|=getPluginCreator() could not find Plugin <operator name> version 1=|Custom Layer Support
      1 3|2|GitLab|Developing a custom code repository
      1 3|2|gRPC Endpoint|Handling in Triton Core
      1 3|2|Helm v3|Model Repository
      1 3|2|How do I run this on my SLURM cluster?|Submissions
      1 3|2|👣 How to migrate your Profiles|Decoupling Stack Component configuration from implementation
      1 3|2|How to use ZeRO-Inference|Conclusion
      1 3|2|Imagenet for resnet50|Build benchmark images
      1 3|2|ImageNet validation dataset (required for calibration)|Example:
      1 3|2|Impact of generation output length|Using ZeRO-Inference
      1 3|2|Import in all the right places|Step 4: Create a PR and celebrate :tada:
      1 3|2|Inference|Plot the final results
      1 3|2|Infrastructure Deployment|How to find the registry URI
      1 3|2|Infrastructure Deployment|How to use it
      1 3|2|=inputOrder=|Additional resources
      1 3|2|Input Parameters*|=custom_tensorRT.py=
      1 3|2|Inputs Description :|License
      1 3|2|Installation|Part 1. Concurrent inference and dynamic batching
      1 3|2|Install common CK packages|Setup for EdgeTPU
      1 3|2|Install from Sources|Docker Installation
      1 3|2|Installing requirements|Prepare checkpoint
      1 3|2|Install the Chart|Configuration
      1 3|2|InstanceNormalizaiton Performance|Executable Usage
      1 3|2|Integration with Megatron-LM|The Zero Redundancy Optimizer
      1 3|2|Interactive stack deployment|Displaying Terraform outputs for stacks deployed with mlstacks
      1 3|2|Issue Reporting Disclaimer|Contribute Code
      1 3|2|LabelEncoder|💔No Cover Common Operators
      1 3|2|Labeling Errors|Challenge Results and References
      1 3|2|Large models >2GB|TensorProto: data_location and external_data fields
      1 3|2|Layer Class|NVIDIA(R) vs TensorFlow Toolkit
      1 3|2|Loading a =distribution= via HTTP with Basic Auth|Programmatically build JSON-LD files
      1 3|2|Local and remote availability|Register Service Connectors
      1 3|2|Local|Documentation
      1 3|2|Loop|Extensibility
      1 3|2|Mac|Verify Installation
      1 3|2|Markdown based tutorials|Questions?
      1 3|2|Member Companies|Organizational Structure
      1 3|2|Memory-Efficient Training with ZeRO Optimizer|Training Agnostic Checkpointing
      1 3|2|MLPerf training benchmark results|How to update this repository with new results
      1 3|2|Mobilenet EdgeTPU latency|Pretrained models
      1 3|2|MobileNet-v2|[[https://github.com/NVIDIA/TensorRT/tree/main/tools/tensorflow-quantization/examples/efficientnet][EfficientNet]]
      1 3|2|Mobilenet V2 Imagenet Checkpoints|Training
      1 3|2|Model Instances|Framework-Specific Optimization
      1 3|2|Model Pipeline|*Resources*
      1 3|2|Modifying Input Shapes In An ONNX Model|Advanced Topics
      1 3|2|MPI and AzureML Compatibility|Resource Configuration (single-node)
      1 3|2|MultiStream|Fix INVALID results
      1 3|2|MultiStream|Other performance tips
      1 3|2|MultiStream|Tune parameters for better performance
      1 3|2|NEUCHIPS Submission|System Preprocessing
      1 3|2|Non-goals|Terminology
      1 3|2|Notes|[[https://github.com/NVIDIA/TensorRT/releases/tag/21.06][21.06]] -
      1 3|2|ONNX Compose|Tools
      1 3|2|onnx_resnet50|Prerequisites
      1 3|2|Operator as function|Tricks learned from experience
      1 3|2|Optional|Building the MLPerf app with the QTI backend
      1 3|2|Optional cluster services|ZenML Helm Installation
      1 3|2|(Optional) Trying a different configuration|Advanced
      1 3|2|Orin NX NVME ASPM|Download Data, Model and Preprocess the data
      1 3|2|Other Areas of Interest|End-to-end Example
      1 3|2|Other Dependencies|Programs
      1 3|2|Other types|What is an opset version?
      1 3|2|Outputs|Calibration
      1 3|2|Package managers|Set up your HTML
      1 3|2|/pack/(/root/) {#_pack}|/pack/.radius(/radius/)
      1 3|2|Parallelism based technologies*: 3D Parallelism refers to a|Deciding which technology to use
      1 3|2|Parameters|Additional Resources
      1 3|2|Parameters|License
      1 3|2|Parameters|Limitations
      1 3|2|Partial data computation and propagation|Special Cases
      1 3|2|Part III : Compare performance of TEST04-A with TEST04-B|Help
      1 3|2|Performance Profiling|Submitting an Issue
      1 3|2|Perform inference|MLflow Deployments
      1 3|2|Prepare checkpoints|Prune models and test the accuracy on GLUE/SQuAD benchmarks
      1 3|2|Preprocessing the datasets for inference|Running NEUCHIPS DLRM benchmark
      1 3|2|Presence in proto3 APIs|Semantic differences
      1 3|2|/project/.invert(/x/, /y/)|geoProjection(/project/)
      1 3|2|Prometheus ServiceMonitor Support|Using Triton Inference Server
      1 3|2|Prompt Hacking Journey|Examples of unsafe images generated by safe-seeming prompts
      1 3|2|Protoc|Usage
      1 3|2|Pull Request Checklist|How to become a contributor and submit your own code
      1 3|2|Python API|Examples
      1 3|2|Python model using Python Backend|Deploying Decoupled Models
      1 3|2|Python Modules|ONNX-TensorRT Python Backend Usage
      1 3|2|PyTorch|Example: Tuning for Large Batch Sizes
      1 3|2|PyTorch Execution Graphs|Execution Trace Generator (et_generator)
      1 3|2|*PyTorch model*|Batch Scaling Example
      1 3|2|Reducing Failing ONNX Models|Examples
      1 3|2|Reduction Modes|Further Reading
      1 3|2|Removed|20.12 Container Release - 2020-12-17
      1 3|2|Removed|21.02 Container Release - 2021-01-18
      1 3|2|Removed|[[https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-2-1][8.2.1
      1 3|2|Removed|[[https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-4-1][8.4.1
      1 3|2|Removed|[[https://docs.nvidia.com/deeplearning/tensorrt/release-notes/#rel-8-5-1][8.5.1
      1 3|2|Removed|[[https://docs.nvidia.com/deeplearning/tensorrt/release-notes/tensorrt-7.html#rel_7-2-1][7.2.1]] -
      1 3|2|Removed|[[https://github.com/NVIDIA/TensorRT/releases/tag/20.10][20.10]] -
      1 3|2|Removed|[[https://github.com/NVIDIA/TensorRT/releases/tag/20.11][20.11]] -
      1 3|2|Removed|[[https://github.com/NVIDIA/TensorRT/releases/tag/20.12][20.12]] -
      1 3|2|Removed|[[https://github.com/NVIDIA/TensorRT/releases/tag/21.02][21.02]] -
      1 3|2|Removed|[[https://github.com/NVIDIA/TensorRT/releases/tag/21.03][21.03]] -
      1 3|2|Removed|[[https://github.com/NVIDIA/TensorRT/releases/tag/21.05][21.05]] -
      1 3|2|Removed|[[https://github.com/NVIDIA/TensorRT/releases/tag/21.07][21.07]] -
      1 3|2|Removed|[[https://github.com/NVIDIA/TensorRT/releases/tag/21.08][21.08]] -
      1 3|2|Removed|[[https://github.com/NVIDIA/TensorRT/releases/tag/21.09][21.09]] -
      1 3|2|Removed|[[https://github.com/NVIDIA/TensorRT/releases/tag/21.10][21.10]] -
      1 3|2|Removed|[[https://github.com/NVIDIA/TensorRT/releases/tag/22.03][22.03]] -
      1 3|2|Removed|[[https://github.com/NVIDIA/TensorRT/releases/tag/22.04][22.04]] -
      1 3|2|Removed|[[https://github.com/NVIDIA/TensorRT/releases/tag/22.05][22.05]] -
      1 3|2|Removed|[[https://github.com/NVIDIA/TensorRT/releases/tag/22.07][22.07]] -
      1 3|2|Removed|v0.13.4 (2020-03-25)
      1 3|2|Removed|v0.2.2 (2020-06-17)
      1 3|2|Removed|v0.23.1 (2021-02-05)
      1 3|2|Removed|v0.25.1 (2021-03-15)
      1 3|2|Removed|v0.27.0 (2021-04-06)
      1 3|2|Removed|v0.3.0 (2021-02-12)
      1 3|2|Removed|v0.31.1 (2021-07-16)
      1 3|2|Removed|v0.33.2 (2021-10-21)
      1 3|2|Removed|v0.34.2 (2021-11-29)
      1 3|2|Removed|v0.37.3 (2022-05-04)
      1 3|2|Removed|v0.39.0 (2022-07-01)
      1 3|2|Removed|v0.40.4 (2022-08-17)
      1 3|2|Removed|v0.41.1 (2022-08-25)
      1 3|2|Removed|v0.7.1 (2019-08-29)
      1 3|2|🚨 Reporting a Vulnerability|Coding Conventions
      1 3|2|Requirements|Quick Start Guide
      1 3|2|ResNet101-v2|[[https://github.com/NVIDIA/TensorRT/tree/main/tools/tensorflow-quantization/examples/mobilenet][MobileNet]]
      1 3|2|Reusability using MLCommons CM automation language|Install Python virtual environment via CM
      1 3|2|Run Calibration|Instructions for Auditors
      1 3|2|Run DCGAN Model with DeepSpeed Enabled|Performance Comparison
      1 3|2|Run experiments|Setup for RPi4 CPU
      1 3|2|Run Fast api swagger|Test your endpoints
      1 3|2|Run Hello World MLCube example|Setup Docker
      1 3|2|Running BingBertSquad|DeepSpeed Integration
      1 3|2|Running multiple experiments (optional)|Running GaNDLF (Training/Inference)
      1 3|2|Running preprocessing before training/inference (optional)|Constructing the Data CSV
      1 3|2|Running the API server|Frontend
      1 3|2|Running the Bing BERT model|Enabling DeepSpeed
      1 3|2|Running Unmodified Megatron-LM GPT2 model|Enabling DeepSpeed
      1 3|2|Run post-training quantization|Reference
      1 3|2|Run trainings|How to make predictions with pretrained weights
      1 3|2|Run your MLCube|Using the Example with GPUs
      1 3|2|Sample =--help= options|Additional resources
      1 3|2|Sanity-checking for FP16 limitations|Debugging accuracy failures
      1 3|2|Saving Experiments to GCP|Getting Information from a Container
      1 3|2|Scaling the 1T and 2T models|How to run training experiments on Azure?
      1 3|2|Scan|Functions
      1 3|2|SequenceLength (call for test cases)|💚Covered Experimental Operators
      1 3|2|Serializing SemVer version numbers in protobuf|IR versioning
      1 3|2|Service Connector Verification|Configure local clients
      1 3|2|Setup Conda Environment and Build Dependencies|Run Benchmark
      1 3|2|Setup the algorithm selectors|Preparing sample data
      1 3|2|Shape inference a Large ONNX Model >2GB|Running Type Inference on an ONNX Function
      1 3|2|Single-GPU, Multi-GPU, and Multi-Node Training|Pipeline Parallelism
      1 3|2|Sinian|Benchmarks
      1 3|2|Software Versions|Quick Start Guide
      1 3|2|Special notes for Inference for Histology images|Generate Metrics
      1 3|2|SSD-MobileNet-v1|Datasets
      1 3|2|SSD ResNet34 int8|ONNXRuntime PTQ
      1 3|2|stackOrderReverse(/series/)|Stack offsets
      1 3|2|Start the Sax GPU model server|Use Sax
      1 3|2|Start Training|Reproducing Fastest BERT Training Results with DeepSpeed
      1 3|2|Statistics Response JSON Error Object|GRPC
      1 3|2|Step 4: ONNX release|Updating an existing operator
      1 3|2|Step 4: TensorRT Deployment|Results
      1 3|2|Step Information|Artifacts
      1 3|2|Step output names|Configure steps/pipelines
      1 3|2||Steps to calibrate GPT-J
      1 3|2||Steps to do calibration for RNNT
      1 3|2||Steps to run GPT-J
      1 3|2|Step* There are three different steps in the =facets= integration|Disabling Visualizations
      1 3|2|Structure|Additional resources
      1 3|2|Submission|Files
      1 3|2|Submit reflection of your results as a paper|Evaluation
      1 3|2|Submit the MLCube|3. Request Participation
      1 3|2|Supported flavors|Requirements
      1 3|2|Supported =.json= File Structure:|Configuration
      1 3|2|Supported orchestrators|Stopping/pausing a scheduled run
      1 3|2|/symbolType/.draw(/context/, /size/)|pointRadial(/angle/, /radius/)
      1 3|2|Table of Contents|[[https://github.com/d3/d3-array/blob/master/README.md][Arrays
      1 3|2|Tags|Use
      1 3|2|TensorFlow Automatic FP16 Optimization|NUMA Optimization
      1 3|2|TensorRT API layers and ops|Prerequisites
      1 3|2|TensorRT API layers and ops|Running the sample
      1 3|2|TensorRT inference process|Accuracy
      1 3|2|Terminology|Pretraining on the source dataset
      1 3|2|Tests|Pre-trained Models
      1 3|2|The jobfile should look like:|Parse results and create all the plots
      1 3|2|this your first time setting up a model repository?* Check out|Repository Layout
      1 3|2|this your first time writing a config file?* Check out|Minimal Model Configuration
      1 3|2|TIP: Generating Script Templates Automatically|Running The Example
      1 3|2|Training setup using Azure VMSS|Performance Evaluation on Various Model Configurations
      1 3|2|Train scripts|DeepSpeed Evaluation using GPT-2
      1 3|2|Triton Harness|NVIDIA Submissions
      1 3|2|Triton-reported Response Cache Metrics|Custom Metrics
      1 3|2|Troubleshooting|Apply Perf Optmization for AGX/NX
      1 3|2|Tutorial Videos & Slides|Performance Mode vs. Energy Mode
      1 3|2|Unload|GRPC
      1 3|2|Update all packages and repositories|3. Install CUDNN on Ubuntu 22.04
      1 3|2|Updated|21.05 Container Release - 2021-05-17
      1 3|2|Updated|TensorRT 8.0 Release - 2021-07-02
      1 3|2|Updating Model"s Inputs Outputs Dimension Sizes with Variable Length|ONNX Parser
      1 3|2|USB-C Power Adapters|Apply Perf/W Optmization for AGX
      1 3|2|USB-C Power Adapters|Apply Perf/W Optmization for NX
      1 3|2|Use Java Protocol Buffers with Bazel|Build from Source
      1 3|2|Using CM script|Copyright
      1 3|2|Using .wav Files for Selection|Optional MLCube Workflow
      1 3|2|Verify Triton Is Running Correctly|Send an Inference Request
      1 3|2|Version Info|Setup
      1 3|2|/voronoi/.xmin/voronoi/.ymin/voronoi/.xmax/voronoi/.ymax|/voronoi/.contains(/i/, /x/, /y/)
      1 3|2|Voting platform|Election officers and Steering Committee emeritus members
      1 3|2|Warning|Stack Component: =KubernetesSparkStepOperator=
      1 3|2|Weekly Meeting|Disclaimer
      1 3|2|Weights|Additional Details
      1 3|2|Weights package|Build
      1 3|2|Weights package|Compile
      1 3|2|WG - Working Groups|Repository Guidelines
      1 3|2|What if the limit is zero?|7. Verify download
      1 3|2|What is already built in?|How do I use it?
      1 3|2|Windows build|To push artifacts to Maven Central
      1 3|2|WMT|FastMRI
      1 3|2|Workflow to Test Patches with Zephyr SDK|Release Process
      1 3|2|Workspace|MNIST MLCube directory structure summary
      1 3|2|Xor|💔No Cover Common Operators
      1 3|2|your system is not listed above or in|Running your first benchmark
      1 3|2|ZenML server with =docker-compose=|Troubleshooting
      1 3|2|ZenML Test Environments|The ZenML Test CLI
      1 3|2|ZipMap (call for test cases)|💚Covered Experimental Operators
      1 33|21|[boolean]|[integer]
      1 33|30|[integer]|[integer]
      1 34|10|[boolean]|[boolean]
      1 4|1|2.2. TensorRT Inference|Additional resources
      1 4|1|Additional Resources|
      1 4|1|BERT-99.9 and ResNet50 (6,900 RPM)|Maximum Frequency
      1 4|1|CK components can be found at|Author
      1 4|1|CK components can be found at|Status
      1 4|1|CK components for AI and ML are now collected in|Author
      1 4|1|Core Performance Boost: Disable|Management Firmware Settings
      1 4|1|ResNet50 (6,900 RPM)|Maximum Frequency
      1 4|1|ResNet50 (6,900 RPM)|Power Consumption Settings
      1 4|1|ResNet50 Server: Disabled|Maximum Frequency
      1 4|1|Thermal Configuration: Optimal Cooling|Maximum Frequency
      1 4|2|Additional cache details|Architecture
      1 4|2|Apply postprocessing|How to run inference with pretrained models
      1 4|2|Artifact Visualizations|Code Example
      1 4|2|Awards and prize money|Model Track
      1 4|2|Building Manually|Examples
      1 4|2|Choose the Container Runner|What's Next?
      1 4|2|CK components can be found at|Minimal CK installation
      1 4|2|Cloud-specific settings|Connecting to deployed ZenML
      1 4|2|COCO 2017 validation dataset|Prerequisites and Installation
      1 4|2|Common Pitfalls|Appendix
      1 4|2|Component-Specific Metadata|Steps
      1 4|2|Concrete example that illustrates the different components:|Table of Contents
      1 4|2|Configure your MLCube|Build your MLCube
      1 4|2|Configuring materializers at runtime|Basic example
      1 4|2|Configuring RBAC|How to use it
      1 4|2|Custom Cache|Deprecation Notes
      1 4|2|Docker or Singularity|Install MedPerf
      1 4|2|Docker Tips|Score your submission
      1 4|2|Documentation|License
      1 4|2|do I install programs like valgrind in the container?*|get =nvcc fatal   : Unsupported gpu architecture 'compute_80'= error
      1 4|2|Download the Necessary files|1. Train a GaNDLF Model
      1 4|2|Example|In-Process Triton Server API
      1 4|2|Example|Interaction with custom artifact stores
      1 4|2|Examples|ai.onnx.preview.training
      1 4|2|External Tensor Data|Standard data types
      1 4|2|Hosting your own ONNX Model Hub|Raise issue if any
      1 4|2|How to proceed after requesting association|4. Execute the Benchmark
      1 4|2|If I share my email, will you spam me?|Version mismatch (downgrading)
      1 4|2|Inference APIs|Java bindings for In-Process Triton Server API
      1 4|2|Installing Manually|Command-line Toolkit
      1 4|2|Listing and inspecting Models:|Local Caching
      1 4|2|List Pipelines via CLI|Runs
      1 4|2|Longterm there are no explicit restrictions on what devices can be|References
      1 4|2|Metrics MLCube|5. Host the Demo Dataset
      1 4|2|Model MLCube*, and the *Metrics MLCube*. Each type has a specific|Data Preparator MLCube
      1 4|2|MultiStream corresponds to the official MLPerf scenario for|Q: Why is the code littered with so many lambdas? My eyes hurt.
      1 4|2|NF5**8 (single node)|Alternative launch with nvidia-docker
      1 4|2|NGC Container|Resources
      1 4|2|NVIDIA DGX H100 (single node)|Alternative launch with nvidia-docker
      1 4|2|On Windows|Installation
      1 4|2|Open EC2 ports to receive requests|Setting up AWS environment
      1 4|2|Option 2 - Execute the whole ML pipeline from a Python script:|:dart: Dashboard
      1 4|2|(Optional) Which Metadata to Extract for the Artifact|Usage
      1 4|2|(or *D3.js*) is a free, open-source JavaScript library for|D3 is a low-level toolbox
      1 4|2|(or *D3.js*) is a free, open-source JavaScript library for|Resources
      1 4|2|Parameters & Settings|2. Execution
      1 4|2|Run TFLite evaluation|Quantization-aware training
      1 4|2|Speeding up Docker builds for containerized components|Registering a code repository
      1 4|2|Stack Switching|3. Management
      1 4|2|Start tmux session (Recommended)|Datasets
      1 4|2|Stats (On the development set)|v1.0
      1 4|2|Summaries|GPU Metrics
      1 4|2|Table of Contents|Description
      1 4|2|TensorRT|Building for JetPack 4.x
      1 4|2|Terminology|NVIDIA Copyright
      1 4|2|This file uses the [Trace Event Format]|Q: What is the difference between the MultiStream and MultiStreamFree
      1 4|2|To iterate over all oneofs|Updating Reflection
      1 4|2|to migrate*: Replace all post-execution workflows from the paradigm|📡Future Changes
      1 4|2|to migrate*: Simply remove the parameters and use the|new pipeline intermediate representation*
      1 4|2|to Triton Inference Server and want do just deploy your model|Create A Model Repository
      1 4|2|to Triton Inference Server?* Make use of|*Installation*
      1 4|2|to Triton Inference Server?* Make use of|Serve a Model in 3 Easy Steps
      1 4|2|to use channel pruning*|2. Tutorial for ZeroQuant: efficient and affordable post-training
      1 4|2|to use DeepSpeed Compression:* The first section General Tutorial|1. General Tutorial
      1 4|2|to use ZeroQuant*|3. Tutorial for XTC: simple yet effective compression pipeline for
      1 4|2|Trace Setting Request JSON Object|GRPC
      1 4|2|Tracking code version for pipeline runs|Available implementations
      1 4|2|Trigger build and push of images on other branch|GCP Data and Experiment Integration
      1 4|2|Type Constraints|Version 2 of the 'ai.onnx.ml' operator set
      1 4|2|Type Constraints|Version 3 of the 'ai.onnx.ml' operator set
      1 4|2|Type Constraints|Version 4 of the 'ai.onnx.ml' operator set
      1 4|2|Unregister|GRPC
      1 4|2|Writing A Custom Runner|Comparator
      1 4|2|Yapf|Test
      1 4|3|1.2.2 MPI-based implementation|1.3 0/1 Adam Algorithm
      1 4|3|1.2.2 MPI-based implementation|1.3 1-bit Algorithm
      1 4|3|1.2.2 MPI-based implementation|1.3 1-bit LAMB Algorithm
      1 4|3|1 sample per batch (for the SingleStream scenario)|Prerequisites
      1 4|3|2.3 Copyright license back to You|3. Patents
      1 4|3|2. EfficientNet V2|Create ONNX Graph
      1 4|3|3.2 Revocation of patent license|4. License obligations by Us
      1 4|3|3. TFHub Models|Create ONNX Graph
      1 4|3|5,000 images|SSD-MobileNet-v1
      1 4|3|5. Users and bots in allowlist|Environmental Variables :
      1 4|3|8.3 In the event of a termination of this Agreement Sections 5.,|9. Miscellaneous
      1 4|3|Accuracy|EfficientNet
      1 4|3|Accuracy|MobileNet-v2
      1 4|3|Accuracy|MobileNet-v3
      1 4|3|Active User and Active Stack|Resource Models
      1 4|3|Added|Changed
      1 4|3|Additional logs*|Client and server logs
      1 4|3|Add model weights|Modify =mlcube.py=
      1 4|3|Add PyConfig.h|Build Python bindings
      1 4|3|Alternative launch with docker|Steps to launch training on multiple nodes
      1 4|3|AMD (Asynchronous Module Definition)|Node
      1 4|3|Anchors Input (Optional)|Dynamic Shape Support
      1 4|3|Authentication Methods|Caveats
      1 4|3|Building Manually|Installing Dependencies
      1 4|3|Call Evidently directly|Visualizing Evidently Reports
      1 4|3|Call whylogs directly|Visualizing whylogs Profiles
      1 4|3|=CK_TEST_INTERVAL=|Results
      1 4|3|Client Support and Examples|Extend Triton
      1 4|3|Compression|GRPC Options
      1 4|3|Configuration use-case: GCP Service Connector with different|Configuring the stack
      1 4|3|CPP generated code:|Go
      1 4|3|CPP generated code:|Go:
      1 4|3|CPU-Only Build|Building Without Docker
      1 4|3|Custom Batching|Sequence Batcher
      1 4|3|[[./d3-geo/cylindrical.md][Cylindrical projections]]|[[./d3-geo/stream.md][Streams]]
      1 4|3|Data Validator Flavors|How to use it
      1 4|3|Decision making|WG - Working Groups
      1 4|3|Decoupled|Maximum Batch Size
      1 4|3|Development Build of Backend or Repository Agent|Building with Debug Symbols
      1 4|3|DISCORD_TOKEN|How to Use the Discord Alerter
      1 4|3|Edge TPU checkpoints:|Mobilenet V2 Imagenet Checkpoints
      1 4|3|effect on the =zenml.io.fileio=*|Build your own custom artifact store
      1 4|3|Enabling CUDA for GPU-backed hardware|Important Note for Multi-Tenancy Deployments
      1 4|3|Evaluate pb|Evaluate TFLite
      1 4|3|Example: Facets Data Skew Visualization|Custom Class* The
      1 4|3|Example Request|Generate Response JSON Object
      1 4|3|Example Response|Generate Response JSON Error Object
      1 4|3|Examples|*Acosh*
      1 4|3|Examples|*Add*
      1 4|3|Examples|*AffineGrid*
      1 4|3|Examples|*ai.onnx.ml.Binarizer*
      1 4|3|Examples|*ai.onnx.ml.CastMap*
      1 4|3|Examples|*ai.onnx.ml.LinearClassifier*
      1 4|3|Examples|*ai.onnx.preview.training.Adam*
      1 4|3|Examples|*ai.onnx.preview.training.Gradient*
      1 4|3|Examples|*ai.onnx.preview.training.Momentum*
      1 4|3|Examples|*And*
      1 4|3|Examples|*ArgMax*
      1 4|3|Examples|*ArgMin*
      1 4|3|Examples|*Asin*
      1 4|3|Examples|*Asinh*
      1 4|3|Examples|*Atan*
      1 4|3|Examples|*Atanh*
      1 4|3|Examples|*AveragePool*
      1 4|3|Examples|*BatchNormalization*
      1 4|3|Examples|*Bernoulli*
      1 4|3|Examples|*BitShift*
      1 4|3|Examples|*BitwiseAnd*
      1 4|3|Examples|*BitwiseNot*
      1 4|3|Examples|*BitwiseOr*
      1 4|3|Examples|*BitwiseXor*
      1 4|3|Examples|*BlackmanWindow*
      1 4|3|Examples|*Cast*
      1 4|3|Examples|*CastLike*
      1 4|3|Examples|*Ceil*
      1 4|3|Examples|*Celu*
      1 4|3|Examples|*CenterCropPad*
      1 4|3|Examples|*Clip*
      1 4|3|Examples|*Col2Im*
      1 4|3|Examples|*Compress*
      1 4|3|Examples|*Concat*
      1 4|3|Examples|*ConcatFromSequence*
      1 4|3|Examples|*ConstantOfShape*
      1 4|3|Examples|*Conv*
      1 4|3|Examples|*ConvInteger*
      1 4|3|Examples|*ConvTranspose*
      1 4|3|Examples|*Cos*
      1 4|3|Examples|*Cosh*
      1 4|3|Examples|*CumSum*
      1 4|3|Examples|*DeformConv*
      1 4|3|Examples|*DepthToSpace*
      1 4|3|Examples|*DequantizeLinear*
      1 4|3|Examples|*Det*
      1 4|3|Examples|*DFT*
      1 4|3|Examples|*Div*
      1 4|3|Examples|*Dropout*
      1 4|3|Examples|*DynamicQuantizeLinear*
      1 4|3|Examples|*Einsum*
      1 4|3|Examples|*Elu*
      1 4|3|Examples|*Equal*
      1 4|3|Examples|*Erf*
      1 4|3|Examples|*Exp*
      1 4|3|Examples|*Expand*
      1 4|3|Examples|*EyeLike*
      1 4|3|Examples|*Flatten*
      1 4|3|Examples|*Floor*
      1 4|3|Examples|*Gather*
      1 4|3|Examples|*GatherElements*
      1 4|3|Examples|*GatherND*
      1 4|3|Examples|*Gelu*
      1 4|3|Examples|*Gemm*
      1 4|3|Examples|*GlobalAveragePool*
      1 4|3|Examples|*GlobalLpPool*
      1 4|3|Examples|*Greater*
      1 4|3|Examples|*GreaterOrEqual*
      1 4|3|Examples|*GroupNormalization*
      1 4|3|Examples|*GRU*
      1 4|3|Examples|*HammingWindow*
      1 4|3|Examples|*HannWindow*
      1 4|3|Examples|*Hardmax*
      1 4|3|Examples|*HardSigmoid*
      1 4|3|Examples|*HardSwish*
      1 4|3|Examples|*Identity*
      1 4|3|Examples|*If*
      1 4|3|Examples|*ImageDecoder*
      1 4|3|Examples|*InstanceNormalization*
      1 4|3|Examples|*IsInf*
      1 4|3|Examples|*IsNaN*
      1 4|3|Examples|*LayerNormalization*
      1 4|3|Examples|*LeakyRelu*
      1 4|3|Examples|*Less*
      1 4|3|Examples|*LessOrEqual*
      1 4|3|Examples|*LogSoftmax*
      1 4|3|Examples|*Loop*
      1 4|3|Examples|*LpNormalization*
      1 4|3|Examples|*LRN*
      1 4|3|Examples|*LSTM*
      1 4|3|Examples|*MatMul*
      1 4|3|Examples|*MatMulInteger*
      1 4|3|Examples|*Max*
      1 4|3|Examples|*MaxPool*
      1 4|3|Examples|*MaxRoiPool*
      1 4|3|Examples|*Mean*
      1 4|3|Examples|*MeanVarianceNormalization*
      1 4|3|Examples|*MelWeightMatrix*
      1 4|3|Examples|*Min*
      1 4|3|Examples|*Mish*
      1 4|3|Examples|*Mod*
      1 4|3|Examples|*Mul*
      1 4|3|Examples|*Multinomial*
      1 4|3|Examples|*NegativeLogLikelihoodLoss*
      1 4|3|Examples|*NonMaxSuppression*
      1 4|3|Examples|*NonZero*
      1 4|3|Examples|*Not*
      1 4|3|Examples|*OneHot*
      1 4|3|Examples|*Optional*
      1 4|3|Examples|*Or*
      1 4|3|Examples|*Pad*
      1 4|3|Examples|*Pow*
      1 4|3|Examples|*PRelu*
      1 4|3|Examples|*QLinearConv*
      1 4|3|Examples|*QLinearMatMul*
      1 4|3|Examples|*QuantizeLinear*
      1 4|3|Examples|*RandomNormal*
      1 4|3|Examples|*Reciprocal*
      1 4|3|Examples|*ReduceL1*
      1 4|3|Examples|*ReduceL2*
      1 4|3|Examples|*ReduceLogSum*
      1 4|3|Examples|*ReduceLogSumExp*
      1 4|3|Examples|*ReduceMax*
      1 4|3|Examples|*ReduceMean*
      1 4|3|Examples|*ReduceMin*
      1 4|3|Examples|*ReduceProd*
      1 4|3|Examples|*ReduceSum*
      1 4|3|Examples|*ReduceSumSquare*
      1 4|3|Examples|*RegexFullMatch*
      1 4|3|Examples|*Relu*
      1 4|3|Examples|*Reshape*
      1 4|3|Examples|*Resize*
      1 4|3|Examples|*ReverseSequence*
      1 4|3|Examples|*RNN*
      1 4|3|Examples|*RoiAlign*
      1 4|3|Examples|*Round*
      1 4|3|Examples|*Scan*
      1 4|3|Examples|*Scatter* (deprecated)
      1 4|3|Examples|*ScatterElements*
      1 4|3|Examples|*ScatterND*
      1 4|3|Examples|*Selu*
      1 4|3|Examples|*SequenceAt*
      1 4|3|Examples|*SequenceLength*
      1 4|3|Examples|*Shape*
      1 4|3|Examples|*Shrink*
      1 4|3|Examples|*Sigmoid*
      1 4|3|Examples|*Sign*
      1 4|3|Examples|*Sin*
      1 4|3|Examples|*Sinh*
      1 4|3|Examples|*Size*
      1 4|3|Examples|*Slice*
      1 4|3|Examples|*Softmax*
      1 4|3|Examples|*SoftmaxCrossEntropyLoss*
      1 4|3|Examples|*Softplus*
      1 4|3|Examples|*Softsign*
      1 4|3|Examples|*SpaceToDepth*
      1 4|3|Examples|*Split*
      1 4|3|Examples|*SplitToSequence*
      1 4|3|Examples|*Sqrt*
      1 4|3|Examples|*Squeeze*
      1 4|3|Examples|*STFT*
      1 4|3|Examples|*StringConcat*
      1 4|3|Examples|*StringNormalizer*
      1 4|3|Examples|*StringSplit*
      1 4|3|Examples|*Sub*
      1 4|3|Examples|*Sum*
      1 4|3|Examples|*Tan*
      1 4|3|Examples|*Tanh*
      1 4|3|Examples|*TfIdfVectorizer*
      1 4|3|Examples|*ThresholdedRelu*
      1 4|3|Examples|*Tile*
      1 4|3|Examples|*TopK*
      1 4|3|Examples|*Transpose*
      1 4|3|Examples|*Trilu*
      1 4|3|Examples|*Unique*
      1 4|3|Examples|Unload
      1 4|3|Examples|*Unsqueeze*
      1 4|3|Examples|*Upsample* (deprecated)
      1 4|3|Examples|*Where*
      1 4|3|Examples|*Xor*
      1 4|3|Example to Follow|Step 3: PR Review by Operators SIG
      1 4|3|Example:|Visualization via Materializers
      1 4|3|From a local machine|Open the dashboard
      1 4|3|Generate data for post-training quantization|Run post-training quantization
      1 4|3|Graph|Exporters
      1 4|3|Having an existing NGINX Ingress Controller|Existing hosted SQL database
      1 4|3|Inference Request/Response Cache|Model Pipeline
      1 4|3|Installation from composer|Protoc
      1 4|3|Installation from PECL|PHP Package
      1 4|3|INT8 Precision|Benchmark TensorRT Engine
      1 4|3|Learning rate scaling when the effective batch size changes|Configuring ZeRO configurations
      1 4|3|List of available parameters|Register models via the CLI
      1 4|3|Mask R-CNN|Evaluate mAP Metric
      1 4|3|MobileNet, int8, 250 samples per batch|Accuracy mode
      1 4|3|MobileNet, int8|Prepare a CK repository with the experimental results
      1 4|3|Node|example ReLU node from ResNet50:*
      1 4|3|Optional Files|Using .wav Files for Selection
      1 4|3|Other Info|ToDo
      1 4|3|Pending Request Count (Queue Size) Per-Model|Latencies
      1 4|3|Power Regulator Settings: OS Control Mode|Fans
      1 4|3|Prepare your Dockerfile|The =mlcube= folder
      1 4|3|Preprocess the calibration dataset|Calibrate the model
      1 4|3|Priority|Ensemble Model Instance Groups
      1 4|3|Pytorch DDP|Run your submission in a Docker container
      1 4|3|QAT|Only accuracy
      1 4|3|Qualification set|Scoring
      1 4|3|Representation|Tensor Element Types
      1 4|3|Run Triton Harness|Multi-MIG Harness
      1 4|3|Sample Images|Evaluate mAP Metric
      1 4|3|Sample Implementation|*Acos*
      1 4|3|Self-tuning ruleset|Workloads
      1 4|3|Sign-off|Step 4: ONNX release
      1 4|3|Special Case for Training|Enabling GPUs
      1 4|3|Stacks, Infrastructure, Authentication|Client Methods
      1 4|3|Static tensor shapes|Attribute Types
      1 4|3|Storing and retrieving the artifact|(Optional) How to Visualize the Artifact
      1 4|3|Structure|SIG - Special Interest Groups
      1 4|3|Tensor|example constant tensor from ResNet50:*
      1 4|3|TensorRT inference benchmark|Results
      1 4|3|to delete a scheduled pipeline*|Additional configuration
      1 4|3|to migrate*: If you have written a|=PipelineSpec= now uniquely defines pipelines
      1 4|3|to migrate*: No code changes, but rather keep in mind the behavior|New post-execution workflow
      1 4|3|to migrate*: Rename all references to =BaseStepConfig= in your code|Configuration Rework
      1 4|3|to migrate*: Rename all references to =Repository= in your code to|The =BaseStepConfig= class is now called =BaseParameters=
      1 4|3|to use activation quantization*|1.4 Pruning
      1 4|3|to use layer reduction*|1.2 Weight Quantization
      1 4|3|to use weight quantization*|1.3 Activation Quantization
      1 4|3|Training|Checkpoints Saving & Loading
      1 4|3|Triton flavor|Deploy models tracked in MLflow to Triton
      1 4|3|Type Constraints|*ai.onnx.ml.Binarizer-1*
      1 4|3|Type Constraints|*ai.onnx.ml.CastMap-1*
      1 4|3|Type Constraints|*ai.onnx.ml.CategoryMapper*
      1 4|3|Type Constraints|*ai.onnx.ml.CategoryMapper-1*
      1 4|3|Type Constraints|*ai.onnx.ml.DictVectorizer*
      1 4|3|Type Constraints|*ai.onnx.ml.DictVectorizer-1*
      1 4|3|Type Constraints|*ai.onnx.ml.FeatureVectorizer*
      1 4|3|Type Constraints|*ai.onnx.ml.FeatureVectorizer-1*
      1 4|3|Type Constraints|*ai.onnx.ml.Imputer*
      1 4|3|Type Constraints|*ai.onnx.ml.Imputer-1*
      1 4|3|Type Constraints|*ai.onnx.ml.LabelEncoder*
      1 4|3|Type Constraints|*ai.onnx.ml.LabelEncoder-1*
      1 4|3|Type Constraints|*ai.onnx.ml.LinearClassifier-1*
      1 4|3|Type Constraints|*ai.onnx.ml.LinearRegressor*
      1 4|3|Type Constraints|*ai.onnx.ml.LinearRegressor-1*
      1 4|3|Type Constraints|*ai.onnx.ml.Normalizer*
      1 4|3|Type Constraints|*ai.onnx.ml.Normalizer-1*
      1 4|3|Type Constraints|*ai.onnx.ml.OneHotEncoder*
      1 4|3|Type Constraints|*ai.onnx.ml.OneHotEncoder-1*
      1 4|3|Type Constraints|*ai.onnx.ml.Scaler*
      1 4|3|Type Constraints|*ai.onnx.ml.Scaler-1*
      1 4|3|Type Constraints|*ai.onnx.ml.SVMClassifier*
      1 4|3|Type Constraints|*ai.onnx.ml.SVMClassifier-1*
      1 4|3|Type Constraints|*ai.onnx.ml.SVMRegressor*
      1 4|3|Type Constraints|*ai.onnx.ml.SVMRegressor-1*
      1 4|3|Type Constraints|*ai.onnx.ml.TreeEnsembleClassifier*
      1 4|3|Type Constraints|*ai.onnx.ml.TreeEnsembleClassifier-1*
      1 4|3|Type Constraints|*ai.onnx.ml.TreeEnsembleRegressor*
      1 4|3|Type Constraints|*ai.onnx.ml.TreeEnsembleRegressor-1*
      1 4|3|Type Constraints|*ai.onnx.ml.TreeEnsembleRegressor-3*
      1 4|3|Type Constraints|*ai.onnx.ml.ZipMap*
      1 4|3|Type Constraints|*ai.onnx.ml.ZipMap-1*
      1 4|3|Type Constraints|*Constant*
      1 4|3|Type Constraints|*GlobalMaxPool*
      1 4|3|Type Constraints|*GridSample*
      1 4|3|Type Constraints|*Log*
      1 4|3|Type Constraints|*LpPool*
      1 4|3|Type Constraints|*MaxUnpool*
      1 4|3|Type Constraints|*Neg*
      1 4|3|Type Constraints|*OptionalGetElement*
      1 4|3|Type Constraints|*OptionalHasElement*
      1 4|3|Type Constraints|*RandomNormalLike*
      1 4|3|Type Constraints|*RandomUniform*
      1 4|3|Type Constraints|*RandomUniformLike*
      1 4|3|Type Constraints|*Range*
      1 4|3|Type Constraints|*SequenceConstruct*
      1 4|3|Type Constraints|*SequenceEmpty*
      1 4|3|Type Constraints|*SequenceErase*
      1 4|3|Type Constraints|*SequenceInsert*
      1 4|3|Type Constraints|*SequenceMap*
      1 4|3|Use a DNS service to map a different hostname to the Ingress|Secret Backends
      1 4|3|Use precalibrated profile|Compile the Server/Offline model for the PCIe server cards
      1 4|3|Using the Fused Box Decoder|Additional Resources
      1 4|3|=--volume ${CK_EXPERIMENTS_DIR}:/home/dvdt/CK_REPOS/local/experiment=|Accuracy mode
      1 4|3|Weight updates|Evaluation
      1 46|16|[boolean]|[boolean]
      1 5|1|Changelog*:|0.10.0
      1 5|1|Changelog*:|0.11.0
      1 5|1|Changelog*:|0.12.0
      1 5|1|Changelog*:|0.13.0
      1 5|1|Changelog*:|0.13.1
      1 5|1|Changelog*:|0.13.2
      1 5|1|Changelog*:|0.20.0 / 0.20.1
      1 5|1|Changelog*:|0.20.2
      1 5|1|Changelog*:|0.20.3
      1 5|1|Changelog*:|0.20.4
      1 5|1|Changelog*:|0.20.5
      1 5|1|Changelog*:|0.21.0
      1 5|1|Changelog*:|0.21.1
      1 5|1|Changelog*:|0.22.0
      1 5|1|Changelog*:|0.23.0
      1 5|1|Changelog*:|0.30.0
      1 5|1|Changelog*:|0.31.0
      1 5|1|Changelog*:|0.31.1
      1 5|1|Changelog*:|0.32.0
      1 5|1|Changelog*:|0.32.1
      1 5|1|Changelog*:|0.33.0
      1 5|1|Changelog*:|0.34.0
      1 5|1|Changelog*:|0.35.0 (YANKED)
      1 5|1|Changelog*:|0.35.1
      1 5|1|Changelog*:|0.36.0
      1 5|1|Changelog*:|0.36.1
      1 5|1|Changelog*:|0.37.0
      1 5|1|Changelog*:|0.38.0
      1 5|1|Changelog*:|0.39.0
      1 5|1|Changelog*:|0.39.1
      1 5|1|Changelog*:|0.40.0
      1 5|1|Changelog*:|0.40.1
      1 5|1|Changelog*:|0.40.2
      1 5|1|Changelog*:|0.40.3
      1 5|1|Changelog*:|0.41.0
      1 5|1|Changelog*:|0.42.0
      1 5|1|Changelog*:|0.42.1
      1 5|1|Changelog*:|0.43.0
      1 5|1|Changelog*:|0.44.1
      1 5|1|Changelog*:|0.44.3
      1 5|1|Changelog*:|0.45.2
      1 5|1|Changelog*:|0.45.3
      1 5|1|Changelog*:|0.45.4
      1 5|1|Changelog*:|0.5.1
      1 5|1|Changelog*:|0.5.2
      1 5|1|Changelog*:|0.5.3
      1 5|1|Changelog*:|0.5.4
      1 5|1|Changelog*:|0.5.5
      1 5|1|Changelog*:|0.6.3
      1 5|1|Changelog*:|0.7.1
      1 5|1|Changelog*:|0.7.2
      1 5|1|Changelog*:|0.8.0
      1 5|1|Changelog*:|0.8.1
      1 5|1|Changelog*:|0.9.0
      1 5|1|Changelog*: https://github.com/zenml-io/zenml/compare/0.44.2...tes|0.44.2
      1 5|2|2019* - This is the first release of the =README.md= file and|Known issues
      1 5|2|2:* Install mlsphere utility and download the image.|Run
      1 5|2|2: The Runner will look in the subfolder defined in the firmware.|Selecting Energy Mode
      1 5|2|based technologies*: For most training scenarios, ZeRO offer|Understanding performance tradeoff between ZeRO and 3D Parallelism
      1 5|2|Benchmarks*. Implementation of DLRM benchmarks in ./bench|How to run dlrm code?
      1 5|2|Changelog*:|New Features
      1 5|2|for AIX users*|C++ Installation - Windows
      1 5|2|graph*.|Input, Output, Node, Initializer, Attributes
      1 5|2|Hint install prebuilt TensorFlow via pip to check all suitable|Prevent running out of memory
      1 5|2|If you have previously installed the =coco= dataset, you should|Running
      1 5|2|sure you run the below commands after =results/= is populated with|Truncate Accuracy Logs
      1 5|2|that not all U-Net configurations are created for all datasets. In|How to get started?
      1 5|2|The dependencies are needed for:|Install via =ck=
      1 5|2|This is equivalent to:|Known issues
      1 5|2|To use machine-specific build options (very important on Raspberry|Unresolved issues
      1 5|2|Updated*: October 17, 2023|Suggestions for Resolving Dependency Conflicts
      1 5|2|your deployed server details*|The ZenML Dashboard is now available
      1 5|3|3. Build with Debug Flags*|Specific Issues
      1 5|3|Add the =--no-cache= flag to rebuild the image from scratch.|Accuracy mode
      1 5|3|Alternative scores|Benchmark Procedure
      1 5|3|based technologies*: In simple terms, ZeRO is a memory efficient|Parallelism based technologies*: 3D Parallelism refers to a
      1 5|3|Both 1.5.2. and 1.5.3 can be installed but fail to convert ResNet|Convert TF to ONNX
      1 5|3|Currently, this downloads the COCO 2017 validation dataset, etc.,|Locate the dashboard plugin
      1 5|3|Hint|Detect a pregenerated profile
      1 5|3|Loss Scaling|Checkpoint Saving & Loading
      1 5|3|NUMA nodes per socket: NPS1|CPU Common Options
      1 5|3|Oldest|Ensemble Models
      1 5|3|OpenCL|MobileNet-v1
      1 5|3|params/*: [various]|Gradient Clipping
      1 5|3|Quantization mode|Misc
      1 5|3|R-CNN*, default is 0.5.|TF vs TRT Comparison
      1 5|3|Refresh all CK repositories after any updates (e.g. bug fixes):|Latest
      1 5|3|Running the close-power benchmark|How do I know the run is finished?
      1 5|3|Since the preprocessed COCO dataset takes up 21G, you may wish to|Download the SSD ResNet34 model
      1 5|3|Software dependencies|Tuning
      1 5|3|sure, that =TRITON_AZURE_MOUNT_DIRECTORY= exists on your local|Cloud Storage with Credential file (Beta)
      1 5|3|that not all U-Net configurations are created for all datasets. In|2D U-Net
      1 5|3|that once the scratch space is setup and all the data, models, and|Download the dataset and the model
      1 5|3|that the 3D full resolution U-Net of the cascade requires the five|Using multiple GPUs for training
      1 5|3|This is equivalent to:|Build-thread variations
      1 5|3|This only updates CK repositories on the host system. To update|Set up environment variables
      1 5|3|Use (CUDA 8 and GCC 5) or (CUDA 9 and GCC 6).|Detect Python, Keras
      1 5|3|Use Java 1.8, Bazel 0.8, Python 3, (CUDA 8 and GCC 5) or (CUDA 9|YAD2K
      1 5|3|Use Python 3, (CUDA 8 and GCC 5), cuDNN 6.|TensorFlow [build from sources]
      1 5|3|Use Python 3.|TensorFlow [=x86_64=]
      1 5|3|Use =--target_os=android23-arm64= to build for Android API 23|Weights package
      1 5|3|Using OpenCV gives better accuracy than using Pillow.|SSD-ResNet34
      1 5|3|Windows|DCO
      1 5|3|You may need to run commands below with =sudo=, unless you|Prepare repository
      1 5|4|=g292_z43_q16=|Power (full)
      1 5|4|Hint|Select the calibration dataset
      1 5|4|is DeepSpeed Compression:* DeepSpeed Compression is a library|use DeepSpeed Compression:* DeepSpeed Compression offers novel
      1 5|4|is pruning*|1.4.1 Sparse Pruning
      1 5|4|L2 Up/Down Prefetcher: Auto|Core Performance Boost: Disable
      1 5|4|Megatron Large with Sparsity|Inference performance: NVIDIA A30
      1 5|4|Methods*|Methods*
      1 5|4|model weights|Configure your MLCube
      1 5|4|Requesting Additional Baselines|Licensing
      1 5|4|SMT Control: Disable|Prefetcher Settings
      1 5|4|sure, that =TRITON_AWS_MOUNT_DIRECTORY= exists on your local|Azure Storage
      1 5|4|sure, that =TRITON_GCS_MOUNT_DIRECTORY= exists on your local|S3
      1 5|4|This is equivalent to the default run command:|ResNet, int8, 15 samples per batch
      1 5|4|to use activation quantization*|to use activation quantization*
      1 5|4|to use channel pruning*|to use channel pruning*
      1 5|4|to use head pruning*|to use head pruning*
      1 5|4|to use layer reduction*|to use layer reduction*
      1 5|4|to use row pruning*|to use row pruning*
      1 5|4|to use sparse pruning*|to use sparse pruning*
      1 5|4|to use weight quantization*|to use weight quantization*
      1 5|4|to use XTC*|to use XTC*
      1 5|4|to use ZeroQuant*|to use ZeroQuant*
      1 5|4|Version and Later*|External Tensor Data
      1 5|4|you tried [[https://developer.nvidia.com/tensorrt][the latest|this model run on other frameworks?* For example run ONNX model
      1 6|1|Please note that we use the same optimizers and hyperparameters|Configuration details
      1 6|1|that this repository is outdated: we are now using the next|Collective Knowledge workflows for MLPerf
      1 6|1|that this repository is outdated: we are now using the next|Fighting the software and hardware chaos
      1 6|2|be folded - =((x + c0) + c1) + c2= will *not* be folded, even|Prerequisites
      1 6|2|Bugs:* - Skip CLA comment if already commented|[[https://github.com/cla-assistant/github-action/tree/v2.0.0-alpha][v2.0.0-alpha]]
      1 6|2|evaluation:* All submissions will be evaluated in a validation|How to participate
      1 6|2|Layers* Other layers are inherited from =BaseQuantizeWrapper=|*How to add a new wrapper?*
      1 6|2|link*:|Steps To Reproduce
      1 6|2|Malformed Boxes by +1*: Some legacy implementations of ROI|Additional Resources
      1 6|2|Of Contents* - [[#changelog][Changelog]] -|Changelog
      1 6|2|Of Contents* - [[#coordconvacplugin][coordConvACPlugin]] -|Description
      1 6|2|Of Contents* - [[#models][Models]] -|Models
      1 6|2|Of Contents* - [[#tensorrt-command-line-wrapper-trtexec][TensorRT|Description
      1 6|2|on language in this and all related documents*:|Components
      1 6|2|out!* 0/1 Adam relies on an compression error compensation|2. BERT Pre-training with 0/1 Adam
      1 6|2|out!* 1-bit Adam relies on an compression error compensation|2. BingBertSQuAD Fine-tuning with 1-bit Adam
      1 6|2|out!* 1-bit LAMB relies on an compression error compensation|2. BERT Pre-training with 1-bit LAMB
      1 6|2|out!* The pipeline engine /pulls/ data from an iterator instead|Advanced Topics
      1 6|2|that CUDA profiling incurs non-negligible overhead.|Profile memory consumption
      1 6|2|that VS Code =preLaunchTask= hook will not run this command when|Android
      1 6|2|The repository agent API is beta quality and is subject to|Using a Repository Agent
      1 6|2|Version*:|Relevant Files
      1 6|3|As of the publication of this document, no ONNX implementation is|Operators
      1 6|3|example is currently not compatible with the latest MedPerf|data_prep
      1 6|3|example is currently not compatible with the latest MedPerf|prep: Data Preparation MLCube
      1 6|3|example is currently not compatible with the latest MedPerf|surg_prep
      1 6|3|Exploring an ONNX file*|Model Semantics
      1 6|3|files get included*|Installing additional pip dependencies or apt packages
      1 6|3|guide: * Check this|Your Account & Log-in
      1 6|3||ImageNet validation dataset (required for calibration)
      1 6|3|objects or dicts*|Utilizing the settings
      1 6|3|out!* After the update on 10/29/2021, now there are two|2.1 Training data truncation
      1 6|3||POC 2 - Pancreas Segmentation
      1 6|3||POC 3 - Surgical Workflow Phase Recognition
      1 6|3||POC 4 - Cloud Experiments
      1 6|3|you add your entry into config.json*, you will have to add your|Different system configurations that use the same GPU configuration
      1 6|4|Data selection|Evaluation during training
      1 6|4||Download the COCO training dataset
      1 6|4|Initializing State from File|Scheduling Strategies
      1 6|4|the TensorRT engine*:|a question*:
      1 6|5|Loss function|Submission functions
      1 7|1|Distribution* * Make sure all the git submodules are updated *|onnx/onnx-operator.pb.h * onnx/onnx.pb.cc * onnx/onnx.pb.h * If they
      1 7|1|Packages* * [[https://developer.nvidia.com/cuda-toolkit][CUDA]]|Recommended versions: * cuda-12.2.0 + cuDNN-8.8 * cuda-11.8.0 +
      1 7|1|refer to /closed/Intel for detailed instructions for Intel CPU|Dell Technologies Open Submission Systems
      1 7|1|refer to /closed/NVIDIA for detailed instructions for NVIDIA GPU|Fujitsu Submission Systems
      1 7|1|refer to /closed/NVIDIA for detailed instructions for NVIDIA GPU|H3C Submission Systems
      1 7|1|refer to /closed/NVIDIA/README.md for detailed instructions for|Dell Technologies Submission Systems
      1 7|1|we have moved the main AE pages|Description
      1 7|2|/0.44.0 was removed from pypi due to an issue with the alembic|What's Changed
      1 7|2|Currently, DeepSpeed Sparse Attention can be used only on NVIDIA|Sparse attention modules
      1 7|2|Currently DeepSpeed Transformer Kernels do not support Sparse|How to use sparse attention with DeepSpeed launcher
      1 7|2|Currently, Triton core does not detect cancellation status of a|Handling in Backend
      1 7|2|DeepSpeed now supports PreLayerNorm as the default way for|Fine-tuning with DeepSpeed on GLUE Tasks
      1 7|2|Dev Containers is an open spec which is supported by|Sample Data
      1 7|2|Distribution* * Follow the same process in TestPyPI to produce|After PyPI Release
      1 7|2|distribution verification* * Test the source distribution by|Upload to official PyPI
      1 7|2|Flutter may not work correctly if your temp directory is located|Tested environment
      1 7|2|For CLI run, please go to the cloned repository's root directory|2. Data preparation
      1 7|2|[[https://pytorch.org/][PyTorch]] must be installed /before/|Install DeepSpeed from source
      1 7|2|If any linter doesn't pass, your pull request is not going to be|Migrations
      1 7|2|Including Python modules into respective package listed above we|Validation
      1 7|2|input* The scores input are of shape|Parameters
      1 7|2|is an extensible, open-source MLOps framework for creating|1. Development
      1 7|2|is a Python package that can be installed directly via =pip=:|Install with the dashboard
      1 7|2|note that due to end-of-life, Python <= 3.6 is no longer|File Structure
      1 7|2|On 08/15/2022 we have added another BERT|Pre-training Bing BERT without DeepSpeed
      1 7|2|Please consider the|Classification
      1 7|2|Please consider the|Regression
      1 7|2|Polygraphy provides a default =DataLoader= class that uses numpy|Logger
      1 7|2|refer to Intel's readme under /closed/Intel for detailed|Dell Submission Systems
      1 7|2|SOON:* We also recommend you check out our|🧰 How the example is implemented
      1 7|2|Specifying the =--safe= parameter turns the safety mode switch|Additional resources
      1 7|2|stack* as its environment. {% endhint %} {% endtab %} {% endtabs|Components of a stack
      1 7|2|The above settings are slightly different to the original|Additional resources
      1 7|2|the following flags are *deprecated*:|Supported Trace Level Option
      1 7|2|The tool does not update a remote repo, so after execution you|Run
      1 7|2|This document is autogenerated from internal documentation. If|HPE-NVIDIA MLPerf Quantization
      1 7|2|This document is autogenerated from internal documentation. If|MLPerf Quantization
      1 7|2|/This release replaces the previous 0.35.0 release that was|Breaking Changes
      1 7|2|This sample is not supported on Ubuntu 14.04 and older.|Prerequisites
      1 7|2|this step frequently hangs when connected to a VPN (including|Update the Readthedocs.io API documentation
      1 7|2|This tutorial was updated on 10/29/2021. Changes include: 1) A|1. Configurations and tuning strategy
      1 7|2|To help you get started with the API, you can use the|Backends
      1 7|2|=t_w= and =t_h= from the input remain unchanged.|Parameters
      1 7|2|Videos/* parallax supported.|[[https://free.nkdev.info/jarallax/][Demo]]
      1 7|2|When using your own data, it is vital to correctly|Segmentation
      1 7|2|You still need to set =Graph= inputs and outputs yourself!|Running the example
      1 7|3||Build the markdown documentation
      1 7|3|container registry or when using a remote container registry|Local registry URI format
      1 7|3|Currently DeepSpeed Transformer Kernels do not support Sparse|*Integrate Transformer Kernel*
      1 7|3|DeepSpeed MoE requires Pytorch 1.8 or above. {: .notice--info}|Expert groups initialization
      1 7|3|Depending on the saved model exporter, some EfficientNet V1|Inference in Python
      1 7|3|Describe how the =--env.CK_ENABLE_BATCH= flag enables the choice|=ck_custom_preprocess=, =ck_custom_preprocess_batch=
      1 7|3|Describe how the =--env.CK_ENABLE_TENSORRT= flag enables the|=load_graph_tensorrt_custom=:
      1 7|3|/Downloading and pre-processing instructions are coming soon./|Running the Bing BERT model
      1 7|3|enter closed/Azure*. From now on, all of the commands detailed|Launching the environment on datacenter/desktop systems
      1 7|3|enter closed/ConnectTechInc*. From now on, all of the commands|Launching the environment
      1 7|3|enter closed/Dell*. From now on, all of the commands detailed in|Launching the environment on datacenter/desktop systems
      1 7|3|enter closed/HPE*. From now on, all of the commands detailed in|Launching the environment
      1 7|3|enter closed/IEI*. From now on, all of the commands detailed in|Launching the environment
      1 7|3|enter closed/Inspur*. From now on, all of the commands detailed|Launching the environment
      1 7|3|enter closed/Nutanix*. From now on, all of the commands detailed|Launching the environment
      1 7|3|For details about loading checkpoint, argument parsing,|2.1 Running BingBertSQuAD with DeepSpeed and 1-bit Adam
      1 7|3|For large model training, see|AlexNet
      1 7|3|For some reason only debug version of the library can be used|Weights package
      1 7|3|If you wanted to train your own model and then perform inference|TensorRT API layers and ops
      1 7|3|Information* What version of Triton are you using?|Reproduce* Steps to reproduce the behavior.
      1 7|3|In order to proceed, you need to re-export the saved model. If|Create ONNX Graph
      1 7|3|Inside the Docker container, [[file:closed/Alibaba]] will be|Prerequisites
      1 7|3|Inside the Docker container, [[file:closed/Fujitsu]] will be|Before you run commands
      1 7|3|Inside the Docker container, [[file:closed/Fujitsu]] will be|Prerequisites
      1 7|3|In the case where we use gradient accumulation, backward on the|Configuration
      1 7|3|It's important to preprocess the data and convert it to the|TensorRT API layers and ops
      1 7|3|no residual connections exist in MobileNet-v1.|MobileNet-v2
      1 7|3|/On Linux, the command-line toolkit is usually installed to|Building From Source
      1 7|3|Perform custom quantization on a ResNet-like model. More|*Default Quantization*
      1 7|3|residual connections exist in MobileNet-v2.|Notes
      1 7|3|Since the datasets and checkpoints are stored in the directory|(Optional) Trying a different configuration
      1 7|3|TFLOPs/GPU* on 128 NDm A100 v4-series A100 systems (i.e., 1024|Scaling the 1T and 2T models
      1 7|3|The cost for this service depends on the size of the files that|Creating ECS execution role
      1 7|3|The cost of this service depends on the instance you choose and|Creating EC2 instance
      1 7|3|the =lambda= in the middle of =layers= above is not a|Inputs and Outputs
      1 7|3|The time measurements do not include the time required to copy|Results
      1 7|3|This code is almost the same to =CodeTypeSSD::CENTER_SIZE= using|=inputOrder=
      1 7|3|This service is free.|Creating ECS cluster
      1 7|3|This service is free.|Creating S3 bucket
      1 7|3|This service is free.|Creating user
      1 7|3|We cannot provide support for the Windows Insider program or for|Building your own GaNDLF Docker Image
      1 7|3|When building Triton on Jetson, you will require a recent|Runtime Dependencies for Triton
      1 7|4|2022* - Migrated code from parsing a =caffe= model to an =onnx=|2021* - Change names and topic from "reformat-free" to "I/O
      1 7|4|/By default, dependencies will be installed using the current|Installing Manually
      1 7|4|*Compression* has seven different components, including layer|Layer Reduction
      1 7|4|If you receive any error messages about non sufficient workspace|INT8 Precision
      1 7|4|RELEASE: You are currently on the main branch which tracks|to Triton Inference Server?* Make use of
      1 7|4|TFOD EfficientDet models will have a slightly reduced throughput|3. TFHub Models
      1 7|4|You do not need to create this file manually. Our workflow will|5. Users and bots in allowlist
      1 7|5|Driver Version*:|Version*:
      1 7|5|old onnx-weekly packages on PyPI* * Once ONNX has been released|opset version for ai.onnx* * Bump opset version for ai.onnx domain
      1 7|5|the server*|up a local ZenML Server*
      1 7|6|conda-forge package with the new ONNX version* * Conda builds of|into main branch* * After everything above is done, merge the
      1 7|6|License.* You hereby grant, and agree to grant, to SAP a|Rights.* To the fullest extent permitted under applicable law,
      1 7|6|The pipeline engine expects data loaders to return a =tuple= of|out!* The pipeline engine /pulls/ data from an iterator instead
      1 7|6|this mode cannot be combined with the =fp16= mode described|[dictionary]
      1 7|6|This tutorial is updated on 03/04/2021 to reflect the 1-bit Adam|out!* 1) The NCCL-based implementation requires PyTorch >= 1.8
      1 8|1|The model is currently cloned into =${INSTALL_DIR}=, which|Run YAD2K demo
      1 8|2|can be defined as a set of operators. A few operators in this|Supported Types
      1 8|2||Customize the Training
      1 8|2|is a package that is worked on by the MLCommons community in|Issues
      1 8|2|is a semantic segmentation method that automatically adapts to|What can nnU-Net do for you?
      1 8|2|permission issue when container tries to write to local|get =useradd: user 'root' already exists= when running
      1 8|2|Sometimes a device is not detected when hot-plugged. The|Standard debug protocol for device detection issues
      1 8|2|Started:* Jump to [[#getting-started][our introductory colab|Evaluation Metric
      1 8|3|10*.|Software packages
      1 8|3|Additional preprocessing needs to be applied to the data before|TensorRT API layers and ops
      1 8|3|[[https://schema.org/Text][sc:Text]]|BoundingBox
      1 8|3|[[https://schema.org/Text][sc:Text]]|column
      1 8|3|[[https://schema.org/Text][sc:Text]]|jsonPath
      1 8|3|[[https://schema.org/Text][sc:Text]]|Reference
      1 8|3|job -> S3*|Enabling CUDA for GPU-backed hardware
      1 8|3||Running multiple experiments (optional)
      1 8|4|5*|Version
      1 8|4|(please complete the following information):* - OS:|did you install GaNDLF* Please provide all steps followed during
      1 8|4|Update, and Delete Methods*|Active User and Active Stack
      1 8|5|to a pre-existing server*|your deployed server details*
      1 8|7|docker*|virtualenv*
      1 8|7|* In release branch update the version number in file|Distribution* * Make sure all the git submodules are updated *
      1 8|7|Validation*|distribution verification* * Test the source distribution by
      1 8|7|* Windows/Linux_x86_64/Linux_aarch64/Mac * Create a new API|Distribution* * Follow the same process in TestPyPI to produce
      1 9|1|2019* - This =README.md= file was recreated, updated and|Known issues
      1 9|1|Optimizations:* When applicable, MII automatically applies|MII-Public and MII-Azure
      1 9|1||Rules
      1 9|2|attribute(s) targeted in the images:*|Contact the organizers
      1 9|2|[[#datasource][DataSource]]|Open issues/questions
      1 9|2|F1 score*:|Performance
      1 9|2|in model size* as well as *increase in number of GPUs*. As|Experimental Setup
      1 9|2|module. Data iterators are flexible, easy to reason about and|Other details for better NMT models
      1 9|2|Only attempt this if you are very comfortable with the|Energy Mode Hardware
      1 9|2|Packages* * Containerized build *|Downloading TensorRT Build
      1 9|2|store* and then loaded from there when the next step needs|Registering a stack
      1 9|2|There is no synchronization between when Triton polls the|Modifying the Model Repository
      1 9|2|to download Dataset and Checkpoint:*|Training data packing
      1 9|2|to the Broad Community:*|What is a benchmark in the MedPerf perspective?
      1 9|2|Ubuntu 20.04 on x86-64 with cuda-11.6.2 (default)*|Step 1: PyTorch model to ONNX model
      1 9|2|- Updated NodeJS from v12 to v16|[[https://github.com/cla-assistant/github-action/tree/v2.0.1-alpha][v2.0.1-alpha]]
      1 9|3|artifacts* can be used to pass values to steps that are|Using a custom step invocation ID
      1 9|3|attribute(s) targeted in the images:* None/Not Applicable|Example of stereotyping
      1 9|3|[[#datasource][DataSource]]|format
      1 9|3|[[#field][Field]]|data
      1 9|3|[[#field][Field]]|dataType
      1 9|3|[[#field][Field]]|parentField
      1 9|3|[[#field][Field]]|references
      1 9|3|[[#fileobject][FileObject]], [[#fileset][FileSet]]|includes
      1 9|3|[[#fileset][FileSet]]|excludes
      1 9|3|[[#fileset][FileSet]]|source
      1 9|3|from nnU-Net v1 can be converted to V2 by running|Experiment planning and preprocessing
      1 9|3|[[#recordset][RecordSet]]|field
      1 9|3|[[#recordset][RecordSet]], [[#field][Field]]|key
      1 9|3|[[#recordset][RecordSet]]|subField
      1 9|3|[[#recordset][RecordSet]]|transform
      1 9|3|repository instead, you'll have to|How to find the registry URI
      1 9|3|store*.|Orchestrator
      1 9|3|the bug* A clear and concise description of what the bug is.|Reproduce* Steps to reproduce the behavior:
      1 9|4|To estimate the variance of the results, this tuning will be|Self-tuning ruleset
      1 9|5|or scripts*:|you tried [[https://developer.nvidia.com/tensorrt][the latest
      1 9|6||on model validation*
      1 9|7||
      1 9|7|GA build* *|Packages* * [[https://developer.nvidia.com/cuda-toolkit][CUDA]]
      1 9|7|This simply means that any pipeline you run will be using the|stack* as its environment. {% endhint %} {% endtab %} {% endtabs
      1 9|7|Version*:|GPU*:
      1 9|7|Version*: *ONNX-TensorRT Version / Branch*: *GPU Type*:|Driver Version*: *CUDA Version*: *CUDNN Version*: *Operating
      1 9|8|-|-
      1 9|8|the Bug*|Steps/Code to Reproduce the Bug*
      2 10|2|allow you to send messages to chat services (like Slack,|Alerter Flavors
      2 10|2|and a /module.py* with the automation actions implemented as|Extending meta descriptions of artifacts
      2 10|2|information as self contained* as possible in this section.]|Proposed API Change (s)
      2 10|2|[[../LICENSE.md][Apache 2.0]]|Collective Mind automation language (CM)
      2 10|2|Preprocess the dataset with =Channel= component at beginning|Input Variables coming from Dependencies
      2 10|2|the JSON format is not yet implemented!/*|File Layout
      2 10|5||
      2 10|7|details overview:* In this example we will use *Random|Classifier:* =tf.contrib.kernel_methods.KernelLinearClassifier=
      2 10|9|Mixed Precision (AMP)*|rate decay*
      2 11|2|models are programs, and need to be treated as such from a|Running untrusted models
      2 11|2||Recap
      2 11|3|notes:/*|Adding a New or Custom System
      2 11|4|When custom config files are generated they override the|Information
      2 11|5||
      2 11|5|If you wish to place the models in your assets manually,|Build
      2 11|6||
      2 11|8||
      2 11|9||
      2 12|10||
      2 12|10|2021-2023 [[https://mlcommons.org][MLCommons]]|[[../LICENSE.md][Apache 2.0]]
      2 12|10|The high-level idea is to use a non-linear map to transform|details overview:* In this example we will use *Random
      2 12|10|training*|Mixed Precision (AMP)*
      2 12|1|by [[https://cKnowledge.org/gfursin][Grigori Fursin]] on|MLPerf™ Inference v0.5 - Image Classification - OpenVino 2019 R3
      2 12|1|by [[https://cKnowledge.org/gfursin][Grigori Fursin]] on|MLPerf™ Inference v1.0 - Image Classification - TFLite 2.4.1
      2 12|1|by [[https://cKnowledge.org/gfursin][Grigori Fursin]] on|MLPerf™ Inference v1.0 - Image Classification - TFLite 2.4.1 (x86)
      2 12|1|by [[https://cKnowledge.org/gfursin][Grigori Fursin]] on|MLPerf™ Inference v1.0 - Image Classification - TFLite 2.5.0 RUY (x86)
      2 12|1|by [[https://cKnowledge.org/gfursin][Grigori Fursin]] on|MLPerf™ Inference v1.0 - Object Detection - TFLite
      2 12|1|by [[https://cKnowledge.org/gfursin][Grigori Fursin]] on|MLPerf™ Inference v1.0 - Object Detection - TFLite 2.4.1 with RUY
      2 12|1|by [[https://cKnowledge.org/gfursin][Grigori Fursin]] on|MLPerf™ Inference v1.0 - Object Detection - TFLite (with Coral EdgeTPU
      2 12|1|by [[https://cKnowledge.org/gfursin][Grigori Fursin]] on|System packages
      2 12|2||Data:
      2 12|4|actions/* are implemented using|modules/* are always stored in */module / < CK module name >/*
      2 12|9|TODO(rzhao): add URL for English-Vietnamese trained model.|Speed*: (0.37s step-time, 15.3K wps) on /K40m/ & (0.17s
      2 12|9|TODO(rzhao): add URL for German-English trained model.|Speed*: (2.1s step-time, 3.4K wps) on /Nvidia K40m/ & (0.7s
      2 13|12||
      2 13|3|array to match with the *lower-rank* array.|Formal definition
      2 13|5||
      2 14|7||
      2 15|2|-|License
      2 15|6||
      2 15|7||logger*
      2 15|9||
      2 16|13|target_space_id, is_character_level,|
      2 17|2|module. Data iterators are flexible, easy to reason|Other details for better NMT models
      2 17|7||
      2 17|8||
      2 18|12||
      2 18|7||
      2 19|15||
      2 19|17||
      2 19|3|Flag to run the check for power submissions|Summary
      2 19|3|using the request index which is the immediate|Additional design notes
      2 19|5||
      2 19|9|object, keyed by request index (uint32_t), stores it|method basically does 2 things:
      2 2|1|Add new CK packages|/PACKAGE_DIR/ - the path to the CK package entry. This is useful if
      2 2|1|Android (Linux host)|CK installation
      2 2|1|Assets|Exporter Usage
      2 2|1|Automated workflows|Coordinator
      2 2|1|Backwards-Incompatible Changes|Release 0.5.0
      2 2|1|Breaking Changes to the API|=tf.mul=, =tf.sub= and =tf.neg= are deprecated in favor of
      2 2|1|Bug Fixes and Other Changes|Release 0.7.0
      2 2|1|Bug Fixes and Other Changes|Release 1.0.0
      2 2|1|Bug Fixes and Other Changes|Release 1.10.0
      2 2|1|Bug Fixes and Other Changes|Release 1.12.2
      2 2|1|Bug Fixes and Other Changes|Release 1.13.0
      2 2|1|Bug Fixes and Other Changes|Release 1.2.0
      2 2|1|Bug Fixes and Other Changes|Release 1.4.0
      2 2|1|Code of Conduct|Publications
      2 2|1|Compiler infrastructure|Getting started with MLIR
      2 2|1|Customizable dashboards|Table of contents
      2 2|1|DeepSpeed-Compression|DeepSpeed Software Suite
      2 2|1|DeepSpeed on Azure|DeepSpeed Adoption
      2 2|1|Detect CUDA on Windows|System dependencies
      2 2|1|Disclaimer|2. Directions
      2 2|1|Docker|Further info
      2 2|1|Download and install MLPerf™ inference results via CK|Available results
      2 2|1|Evaluation frequency|5. Steps to run the model
      2 2|1|Evaluation thoroughness|5. Steps to run the model
      2 2|1|Evaluation thoroughness|MLCommons Inference
      2 2|1|Evaluation thoroughness|Reference runs
      2 2|1|Examples|Reference
      2 2|1|Example: TFExampleDecoder|Data Provision
      2 2|1|Export all MLPerf inference results|Coordinator
      2 2|1|Further analysis of results|Contact us
      2 2|1|Generate MLPerf submission|Use =--submitter=<Your name>= if your organization is an official
      2 2|1|Get the data|Run cube on a local machine with Docker runner
      2 2|1|How to use|=[--docker_os, --docker_os_version, --cm_repo and --script_tags]= are
      2 2|1|How to use|=[DOCKER_OS_VERSION]= is one of =18.04=, =20.04=, =22.04= for =ubuntu=
      2 2|1|Initial checkpoint|4. Quality
      2 2|1|Install virtual environment|CM automation for the MLPerf benchmark
      2 2|1|Linear index formulas for tiling given a shape and a tile|Tiling as pad-reshape-transpose
      2 2|1|MLPerf tasks|TBD
      2 2|1|New state|CLI
      2 2|1|💔No Cover Experimental Operators|Model Test Coverage
      2 2|1|On Google Cloud TPU|6. Software versions
      2 2|1|Optimizer|4. Quality
      2 2|1|🗺 Overview|🖥 Run the example
      2 2|1|Prepare submission|The next steps
      2 2|1|Publication/Attribution|5. Quality
      2 2|1|Putting it all together|Hyperparameters
      2 2|1|Raspberry Pi|Other notes
      2 2|1|[[../README.md][TOC]] ]*|Adaptive CK container for MLPerf™ Inference v1.0 - Image
      2 2|1|[[../README.md][TOC]] ]*|Adaptive CK container for MLPerf™ Inference v1.0 - Object Detection -
      2 2|1|[[../README.md][TOC]] ]*|Analyse MLPerf™ inference results
      2 2|1|[[../README.md][TOC]] ]*|Automated design space exploration of ML/SW/HW stacks
      2 2|1|[[../README.md][TOC]] ]*|CK components for ML Systems (automation recipes)
      2 2|1|[[../README.md][TOC]] ]*|CK workflows for image classification with ONNX
      2 2|1|[[../README.md][TOC]] ]*|CK workflows for image classification with OpenVino
      2 2|1|[[../README.md][TOC]] ]*|CK workflows for image classification with PyTorch
      2 2|1|[[../README.md][TOC]] ]*|CK workflows for image classification with TensorFlow
      2 2|1|[[../README.md][TOC]] ]*|CK workflows for image classification with TFLite
      2 2|1|[[../README.md][TOC]] ]*|CK workflows for image classification with TVM
      2 2|1|[[../README.md][TOC]] ]*|CK workflows for language (NLP) with ONNX
      2 2|1|[[../README.md][TOC]] ]*|CK workflows for medical imaging (3d-unet and BraTS) with ONNX
      2 2|1|[[../README.md][TOC]] ]*|CK workflows for medical imaging (3d-unet and BraTS) with PyTorch
      2 2|1|[[../README.md][TOC]] ]*|CK workflows for object detection with ONNX
      2 2|1|[[../README.md][TOC]] ]*|CK workflows for object detection with TensorRT
      2 2|1|[[../README.md][TOC]] ]*|CK workflows for object detection with TFLite
      2 2|1|[[../README.md][TOC]] ]*|CK workflows for object detection with TVM
      2 2|1|[[../README.md][TOC]] ]*|CK workflows for speech recognition with PyTorch
      2 2|1|[[../README.md][TOC]] ]*|CK workflows for the MLPerf inference benchmark
      2 2|1|[[../README.md][TOC]] ]*|Common setup for the MLPerf inference benchmark
      2 2|1|[[../README.md][TOC]] ]*|Common workflow for MLPerf inference
      2 2|1|[[../README.md][TOC]] ]*|Continuous integration for CK workflows
      2 2|1|[[../README.md][TOC]] ]*|Customize MLPerf™ inference benchmark
      2 2|1|[[../README.md][TOC]] ]*|Example of CK dashboards for ML Systems DSE
      2 2|1|[[../README.md][TOC]] ]*|Ideas to improve automation
      2 2|1|[[../README.md][TOC]] ]*|Ideas to improve CK
      2 2|1|[[../README.md][TOC]] ]*|Logging infrastructure
      2 2|1|[[../README.md][TOC]] ]*|Misc MLPerf™ inference notes
      2 2|1|[[../README.md][TOC]] ]*|MLCube™ project
      2 2|1|[[../README.md][TOC]] ]*|MLPerf™
      2 2|1|[[../README.md][TOC]] ]*|MLPerf™ Inference v0.7 - Image Classification - Nvidia Jetson Xavier
      2 2|1|[[../README.md][TOC]] ]*|MLPerf™ Inference v1.0: medical imaging
      2 2|1|[[../README.md][TOC]] ]*|Nvidia-based generic platforms with Ubuntu
      2 2|1|[[../README.md][TOC]] ]*|Nvidia Jetson Nano board
      2 2|1|[[../README.md][TOC]] ]*|Preparing models
      2 2|1|[[../README.md][TOC]] ]*|Rapsberry Pi 4 with a standard port of Debian
      2 2|1|[[../README.md][TOC]] ]*|Rapsberry Pi 4 with Ubuntu Server 20.04.2 LTS 64-bit
      2 2|1|[[../README.md][TOC]] ]*|Rapsberry Pi 4 with Ubuntu Server 20.04.2 LTS 64-bit with Coral Edge
      2 2|1|[[../README.md][TOC]] ]*|Reproducibility reports: MLPerf™ inference benchmark v1.1
      2 2|1|[[../README.md][TOC]] ]*|Standardization
      2 2|1|[[../README.md][TOC]] ]*|Upgrade the CK framework
      2 2|1|[[../README.md][TOC]] ]*|x8664-based generic platforms with Ubuntu
      2 2|1|[[../README.md][TOC]] ]*|x8664-based generic platforms with Yocto
      2 2|1|Repository content|3. Quality
      2 2|1|[[../reproduce/README.md#][Back to MLPerf v1.1 reproducibility|MLPerf inference v1.1 reproducibility report for the OctoML
      2 2|1|Run a container with external ImageNet|
      2 2|1|Run and time|Bert PT
      2 2|1|Run and time PyTorch Resnet50|Bert TF
      2 2|1|Setup|3d-unet
      2 2|1|Setup|Bert
      2 2|1|Setup|GPT-J
      2 2|1|Setup|Resnet50
      2 2|1|Setup|Retinanet
      2 2|1|Setup|RNNT
      2 2|1|Supported and Tested OS|Examples
      2 2|1|System requirements to run MLPerf on Nvidia GPU|MLCommons CM automation meta-framework
      2 2|1|Table of Contents|Getting Started
      2 2|1|Test data order|3. Model
      2 2|1|Thanks to our Contributors|Release 0.10.0
      2 2|1|Thanks to our Contributors|Release 0.11.0
      2 2|1|Thanks to our Contributors|Release 0.12.0
      2 2|1|Thanks to our Contributors|Release 0.6.0
      2 2|1|Thanks to our Contributors|Release 0.7.1
      2 2|1|Thanks to our Contributors|Release 0.8.0
      2 2|1|Thanks to our Contributors|Release 0.9.0
      2 2|1|Thanks to our Contributors|Release 1.0.1
      2 2|1|Thanks to our Contributors|Release 1.1.0
      2 2|1|Thanks to our Contributors|Release 1.10.1
      2 2|1|Thanks to our Contributors|Release 1.11.0
      2 2|1|Thanks to our Contributors|Release 1.12.0
      2 2|1|Thanks to our Contributors|Release 1.12.3
      2 2|1|Thanks to our Contributors|Release 1.14.0
      2 2|1|Thanks to our Contributors|Release 1.2.1
      2 2|1|Thanks to our Contributors|Release 1.3.0
      2 2|1|Thanks to our Contributors|Release 1.4.1
      2 2|1|Thanks to our Contributors|Release 1.5.0
      2 2|1|Thanks to our Contributors|Release 1.6.0
      2 2|1|Thanks to our Contributors|Release 1.7.0
      2 2|1|Thanks to our Contributors|Release 1.8.0
      2 2|1|Thanks to our Contributors|Release 1.9.0
      2 2|1|Using CK adaptive containers (to be tested!)|Other reproducibility studies
      2 2|1|v1.0|
      2 2|1|V1.0.0|Prototyping phase
      2 2|1|Windows|CM CLI testing
      2 22|4|in the same repository (specified by /./)|Viewing CM meta description
      2 23|13|[boolean]|[boolean]
      2 23|2||Smart reply
      2 24|19|Flag to ignore errors in submissions|Flag to run the check for power submissions
      2 24|2||Known problems
      2 25|3|Flag to avoid checking if mandatory|Summary
      2 25|7||
      2 30|12||
      2 3|1|20210723|PyTorch 1.5.0 * TorchVision 0.6.0 (works with PyTorch 1.5.0) * ONNX
      2 3|1|Challenge|
      2 3|1|🧽 Clean up|📜 Learn more
      2 3|1|CLIP|Quality
      2 3|1|Current known limitations|CMake GUI build (all platforms)
      2 3|1|Description|System dependencies
      2 3|1|Evaluation thoroughness|6. Additional notes
      2 3|1|Evaluation thoroughness|6. Other
      2 3|1|Example evaluation frequency|Running the model
      2 3|1|How to run|5. Quality
      2 3|1|Model checkpoint|5. Quality
      2 3|1|Model Definition|[[file:model/attention_layer.py][attention_layer.py]]: Defines the
      2 3|1|Multi-node (with SLURM)|Benchmark details
      2 3|1|Platform information|List of all sorted CM scripts
      2 3|1|Preprocessed data download|4. Model
      2 3|1|Preprocess using Pillow (slightly worse accuracy but works most of|Plug in full ImageNet 2012 val dataset with 50000 images
      2 3|1|Reproduced papers|List of all sorted CM scripts
      2 3|1|Results|
      2 3|1|Run benchmark with SLURM for NVIDIA DGXA100 ( 8 nodes )|3. Model
      2 3|1|Run|Open discussions and developments
      2 3|1|Some stats of the generated tfrecords:|Stopping criteria
      2 3|1|Steps to download and verify data|3. Model
      2 3|1|Steps to launch training|3. Dataset/Environment
      2 3|1|Steps to run and time|3. Model
      2 3|1|Steps to run and time|4. Dataset/Environment
      2 3|1|Steps to run benchmark.|3. Dataset/Environment
      2 3|1|Use Python virtual environment with CM and MLPerf|The next steps
      2 3|1|Using Docker|Future work
      2 3|2|2022|Resources
      2 3|2|2023|Resources
      2 3|2|7) Create Stack|And you're already done!
      2 3|2|9. run|Docker
      2 3|2|Accuracy benchmark|Quantization and calibration
      2 3|2|Accuracy|Direct usage
      2 3|2|Actions|App Addons
      2 3|2|Add CK entries from a zip file to an existing CK repository|CLI to manage CK entries
      2 3|2|Additional Apollo3 Instructions|Building for the Eta Compute ECM3531 EVB using Make
      2 3|2|AOT (Ahead-of-time) compilation for CPU with =tfcompile=|Inspect compiled programs
      2 3|2|API Freeze Consequences|Coding Style
      2 3|2|Architecture|Customize model
      2 3|2|Arguments|closure_css_binary
      2 3|2|Arguments|closure_java_template_library
      2 3|2|Arguments|closure_js_binary
      2 3|2|Arguments|closure_js_deps
      2 3|2|Arguments|closure_js_proto_library
      2 3|2|Arguments|closure_js_template_library
      2 3|2|Arguments|closure_proto_library
      2 3|2|Arguments|closure_py_template_library
      2 3|2|Arguments|phantomjs_test
      2 3|2|Artifacts reusable (pilot project with MLCommons)|Distinguished artifact award
      2 3|2|AutoML mobile models|Object detection
      2 3|2|Backend (CPU and GPU acceleration) tags:|Examples:
      2 3|2|Base Abstraction 3: =Flavor=|Implementing a Custom Stack Component Flavor
      2 3|2|Batching|Building the Model
      2 3|2|Binary Size|Known Limitations
      2 3|2|Build the code|Deploy to Arduino
      2 3|2|Caching output of CM scripts|Assembling pipeline to compile and run image corner detection
      2 3|2|Cannot build for Android (hence removed patches)|Resolved issues
      2 3|2|Caveat Emptor|Setup
      2 3|2|Choices (flags)|Example: MLPerf inference - Python - RetinaNet FP32 - Open Images -
      2 3|2|Chrome Web Store .zip|Platform Notes
      2 3|2|Chunk Type 0x2/event_data: Event Data|Chunk Part Types
      2 3|2|CM script execution flow|If a script is already cached, then the =preprocess=, =run file= and
      2 3|2|CMSIS-NN optimized kernels (---under development---)|Goals
      2 3|2|COCO-2014|Downloading the checkpoints
      2 3|2|COCO 2014|The Model
      2 3|2|Community Supported Builds|Resources
      2 3|2|Compile MLPerf loadgen|CM automation for the MLPerf benchmark
      2 3|2|Computing the power result|Step by step examples
      2 3|2|Convert a tf.Keras model|Quantization
      2 3|2|Converting models prior to TensorFlow 1.9|Basic examples
      2 3|2|Convert to a C array|Model architecture and training
      2 3|2|Copy a given CK entry|CLI to manage CK actions
      2 3|2|Creating Your own App|Building the TensorFlow iOS libraries from source
      2 3|2|Current known limitations|Building with CMake
      2 3|2|Customizing sub-dependencies in a pipeline|Using Python virtual environments
      2 3|2|Debug Mode|Preprocessing the minival dataset
      2 3|2|Defining the kernel in the TensorFlow Lite runtime|Best Practices
      2 3|2|Description|Download the needed files
      2 3|2|Description|Managing the configuration files
      2 3|2|Description|Set up
      2 3|2|Description|Setup for Google Cloud Instances
      2 3|2|Detect llvm with non-standard name|Force new detection even if llvm is already found and cached
      2 3|2|Detect python with non-standard name|Force new detection even if python is already found and cached
      2 3|2|Differentiating ML artifacts|Environment variables
      2 3|2|Distributed Reinforcement Learning|Common Gotchas!
      2 3|2|Eight-bit Calculations|Transform Reference
      2 3|2|Enable the Feature in WTF|Usage
      2 3|2|End-to-end MobileNet conversion|Summary of changes in Python API between 1.X and 2.0
      2 3|2|Energy Efficiency|Supported Ops
      2 3|2|Evaluation Loop|Authors
      2 3|2|Evaluation results|Detailed instructions
      2 3|2|Everyone else:|Abbreviated building instructions:
      2 3|2|Example applications and guides|How it works
      2 3|2|Example|Reusable automation actions
      2 3|2|Expected time to do benchmark runs|Validity of the submission
      2 3|2|Exporting a quantized GraphDef|Additional instructions
      2 3|2|Exporting a tf.keras File|Complex examples
      2 3|2|Exporting TF.learn models|Importing (C++ code)
      2 3|2|Exporting the concrete function|Example program
      2 3|2|Extra system requirements for Nvidia GPU|MLCommons CM automation language
      2 3|2|Files in ./results directory:|Generate the TFRecords for Wiki dataset
      2 3|2|Fine-Tuning a Model on a different task|Evaluating Models.
      2 3|2|Flash the board|Apply Perf Optmization (AGX and NX applicable)
      2 3|2|Formal definition|Broadcasting similar-rank arrays with degenerate dimensions
      2 3|2|From Source|Steps to download data
      2 3|2|From zip file|Adding reusable automations for related artifacts
      2 3|2|Generate and upload MLPerf submission|Questions? Suggestions?
      2 3|2|Generate project files|Build the library
      2 3|2|Generate wrapper functions for ops|Support
      2 3|2|Generic Signatures (custom or advanced usage)|Custom Initialization
      2 3|2|Getting Dimensions From An Operation|Constructors
      2 3|2|Git project|Adding CM script to prepare and run your experiment
      2 3|2|Helm v2|Model Repository
      2 3|2|[[https://github.com/arm-software/armnn-mlperf#preprocess-on-an-x86-machine-and-detect-on-an-arm-dev-board][Detect|Run once (classical CK interface)
      2 3|2|HUD|node.js apps
      2 3|2|IGFS|Limitations
      2 3|2|ImageNet dataset|Build
      2 3|2|Informal Description and Examples|GetDimensionSize
      2 3|2|Install any available version|Install CK packages with ONNX (GPU)
      2 3|2|Install any available version|Install CK packages with TensorFlow (GPU)
      2 3|2|Install any available version|Tested configurations
      2 3|2|Installation - C++|Quick start: Loadgen Over the Network
      2 3|2|Install COCO 2017 val dataset (5000 images) and|Setup for EdgeTPU (Host: RPi 4)
      2 3|2|Install COCO 2017 val dataset (5000 images) and|Setup for RPi4 CPU
      2 3|2|Install dependencies|Installation
      2 3|2|Installing on Linux|Usage
      2 3|2|iOS (ObjC++)|Advanced Usage
      2 3|2|iOS|Supported Models and Ops
      2 3|2|JSON|Chunk Types
      2 3|2|Latency and accuracy results|Choice of tool
      2 3|2|Linux|Load and run a model in C++
      2 3|2|Linux|Run a container and record experiments locally
      2 3|2|Location|Starter model
      2 3|2|Machine learning at the edge|Get started
      2 3|2|Makefile|AssetManagerFileSystem
      2 3|2|MD5sums of provided files:|Extract
      2 3|2|Micro Vision example|Run inference
      2 3|2|model,image-classification,mlperf,onnx,resnet50,v1.5-opset-11|Scenario: Performance; Single Stream
      2 3|2|Model Optimization Toolkit|Next steps
      2 3|2|Models|Installation
      2 3|2|Model Tests|Contributor License Agreement
      2 3|2|Multi-GPU training|Abstractions
      2 3|2|NOT EDIT THE DOCKERFILES/ DIRECTORY MANUALLY!* The files within are|Building
      2 3|2|Note:|Finetuning from Detectron weights on custom datasets
      2 3|2|Note to active contributors|TensorFlow Code of Conduct
      2 3|2|Obtain the output|Next steps
      2 3|2|On desktop:|Reducing variance between runs on Android.
      2 3|2|One liner to do an end-to-end submission using the reference|Please modify the =--adr.gptj-model.checkpoint= value to the path
      2 3|2|ONNX, CPU|Run command
      2 3|2|=<Operation> convolve(const Kernel& kernel, const Dimensions& dims)=|Geometrical Operations
      2 3|2|=<Operation> cumprod(const Index& axis)=|Convolutions
      2 3|2|=<Operation>     eval()=|Representation of scalar values
      2 3|2|=<Operation>  extract_image_patches(const Index patch_rows, const Index patch_cols, const Index row_stride, const Index col_stride, const PaddingType padding_type)=|ColMajor 1st dimension: channels (of size d) 2nd dimension: rows (of
      2 3|2|=<Operation> Logical operators=|Selection (select(const ThenDerived& thenTensor, const ElseDerived&
      2 3|2|=<Operation> random()=|Unary Element Wise Operations
      2 3|2|=<Operation> reduce(const Dimensions& new_dims, const Reducer& reducer)=|Trace
      2 3|2|Operations|4. Optimize your model
      2 3|2|=<Operation> trace()=|Scan Operations
      2 3|2|=<Operation>  unaryExpr(const CustomUnaryOp& func)=|Binary Element Wise Operations
      2 3|2|Ops compatibility|3. Run inference with the model
      2 3|2|Other Input Options:|Use modular Docker container with the CM API
      2 3|2|Other use cases|CK portal
      2 3|2|Output|Customize model
      2 3|2|Outputs|=preprocess_submission.py=
      2 3|2|Overriding Dependency Versions|Examples
      2 3|2|Portable reference code|Goals
      2 3|2|Power measurements|Prepare submission
      2 3|2|Prepare MLPerf submission|Trying deepsparse backend
      2 3|2|Preprocess using OpenCV (better accuracy but may fail on some|Optional: install reduced ImageNet 2012 val dataset with the first
      2 3|2|Pre-trained model|Compute Devices
      2 3|2|Proxy Constraints|Components
      2 3|2|Push data to a dashboard|Notes
      2 3|2|Python pip Package|Metrics
      2 3|2|Quantization|Tweak the number of threads
      2 3|2|Record benchmarking results to the CK repository|Analyze experimental results
      2 3|2|References|Appendix
      2 3|2|Reload Your Page|Capturing Call Traces
      2 3|2|Removing pruning ops from the trained graph|Example: Pruning and training deep CNNs on the cifar10 dataset
      2 3|2|rename|CM internal automations
      2 3|2|ResNet-50 (no-argmax)|Run MLPerf™ benchmark
      2 3|2|Reviewing other submissions|Changes from MLCommons Inference 3.0
      2 3|2|Run a dummy workload with power inside a docker container|Running MLPerf Image Classification with power
      2 3|2|Run benchmark|CK automation
      2 3|2|Run Command|Run ResNet50 TFLite via CM
      2 3|2|Run experiments|Second approach: adding CM interface to your research project
      2 3|2|Run inception model by building all from the source code|Building libraries
      2 3|2|Running|Loading and processing traces
      2 3|2|Running MLPerf Image Classification with power inside a docker|Further questions?
      2 3|2|Running the power server inside a docker container|Running a dummy workload with power (on host machine)
      2 3|2|Run on macOS|Deploy to Arduino
      2 3|2|Run TensorFlow CI Scripts Natively on your Machine|TensorFlow Continuous Integration
      2 3|2|▶️ Run the Code|CLI Commands for the Label Studio Integration
      2 3|2|Sample application|How it works
      2 3|2|=Scalar* data()= and =const Scalar* data() const=|Tensor Operations
      2 3|2|See all installed packages and detected components|Install CK workflow Python dependencies
      2 3|2|See all installed packages and detected components|Install PyTorch model (Resnet50; int8; quantized)
      2 3|2|See all installed packages and detected components|Install TFLite model (MobileNet v2; Large; Minimalistic; 224; 1.0;
      2 3|2|Show/clean CM cache with all installations|Windows
      2 3|2|Skip deploying to a repository|The overall flow
      2 3|2|Specifying subgraphs|Logging
      2 3|2|Stable versions|Current projects
      2 3|2|Start training|Performance
      2 3|2|STMicroelectronics NUCLEO-L4R5ZI|Download and run EEMBC Energy Runner
      2 3|2|strip_unused_nodes|Writing Your Own Transforms
      2 3|2|Summary|MLPerf inference - C++ - RetinaNet FP32 - Open Images - ONNX - GPU -
      2 3|2|Summary|MLPerf inference - Python - ResNet50 FP32 - ImageNet - TVM - CPU -
      2 3|2|Summary|MLPerf inference - Python - RetinaNet FP32 - Open Images - ONNX -
      2 3|2|Summary|MLPerf inference - Python - RetinaNet FP32 - Open Images - PyTorch -
      2 3|2|Support for AngularJS|closure_js_test
      2 3|2|System packages|Install Collective Knowledge (CK) and Virtual Environment
      2 3|2|TBD|Notes
      2 3|2|TensorFlow Makefile|Before you start (all platforms)
      2 3|2|TensorMap|Contents Initialization
      2 3|2|TensorRT|Test CK automation (platform detection)
      2 3|2|=<Tensor-Type> setRandom()=|Data Access
      2 3|2|Test Bench|Speech Model Architectures
      2 3|2|the role of the output dimension:* Intuitively, the larger the|Explicit kernel mappings: summary and practical tips
      2 3|2|Third level files|Examples
      2 3|2|Train a custom model|2. Convert the model
      2 3|2|Training Data Packing|Training Data for ResNet50
      2 3|2|Troubleshooting|Setup the Jetson Orin NX System
      2 3|2|TTT (Time to Train) Calculation for BERT|Training ResNet50
      2 3|2|TTT (Time to Train) Calculation for ResNet50|Supported Configurations
      2 3|2|Tutorials|Releases
      2 3|2|Update program sources|Update software dependencies
      2 3|2|Use "dummy-quantization" to try out quantized inference on a float|Specifying input and output arrays
      2 3|2|Use Final Output (with names on)|Install the tracing-framework Tools
      2 3|2|use [[https://github.com/tensorflow/tensorflow/issues][GitHub|Continuous build status
      2 3|2|Use ONNX|Scenario: Accuracy; Single Stream
      2 3|2|Use ONNX|Scenario: Performance; Single Stream
      2 3|2|Use reduced ImageNet to test the MLPerf workflow|Install PyTorch model (Resnet50; int8; quantized)
      2 3|2|Uses and limitations|Choose a different model
      2 3|2|Using CK modules|Generate reproducible and interactive articles
      2 3|2|Variadic Reduce|ReducePrecision
      2 3|2|View CK dashboard in your browser|Demo of a Docker with MLPerf™ dashboards for ML Systems DSE (Linux
      2 3|2|What is the difference between Repeatability, Reproducibility and|Discussions
      2 3|2|Windows|Run a container and record experiments locally
      2 3|2|Windows Support|Try it out
      2 3|2|Working Example: Specifying the VGG16 Layers|Training Models
      2 3|2|Working Example: Training the VGG16 Model|Fine-Tuning Existing Models
      2 3|2|wtf.addon|Tracing
      2 3|2|wtf.hud.app.mode/wtf.hud.app.endpoint|Remote Control
      2 3|2|wtf.remote.target|App
      2 3|2|you proceed to try to build and run from this directory, it is|Running
      2 38|2||Image segmentation
      2 4|1|Run and Time|3. Dataset/Environment
      2 4|1|Start a Tensorflow C++ project with CMake|Step-by-step Windows build (command prompt)
      2 4|1|This page is outdated. Please follow|MLPerf Inference - Image Classification
      2 4|1|This page is outdated. Please follow|MLPerf Inference - Object Detection
      2 4|1|With one CM command that will install all dependencies|The next steps
      2 4|2|Accuracy|CPU
      2 4|2|Arguments|Result Summarizer - Computing power metric
      2 4|2|Benchmark|Analyze experimental results
      2 4|2|BLEU computation|Term definitions
      2 4|2|Building the CUDA-enabled Android demo with gradle/Android Studio:|iOS
      2 4|2|can skip this step if you want to share your artifacts without the|Making artifacts available to evaluators
      2 4|2|Class =TensorRef=|Accessing Tensor Elements
      2 4|2|Compliance|Notes
      2 4|2|Creating other types of artifacts|Reusing others' artifacts in the CM format
      2 4|2|=<data_type> tensor(index0, index1...)=|TensorLayout
      2 4|2|default, CM will pull Git repositories and cache installations and|Ubuntu, Debian
      2 4|2|Determinism when scanning|Writing to Cloud Bigtable
      2 4|2|Evaluating On GPU|API Reference
      2 4|2|for timing* - you will not be able to get function times from this!|Preparing Your Code
      2 4|2|fp32|Acknowledgments
      2 4|2|Generate actual submission tree|Tensorflow backend (Reference implementation)
      2 4|2|How do I inspect a =.tflite= file?|Models & Operations
      2 4|2|How do I test that a TensorFlow Lite model behaves the same as the|Optimization
      2 4|2|Implementation details|Gather
      2 4|2|iOS|Tips and Tricks
      2 4|2|JSON in PARTIAL Mode|Chunks
      2 4|2|=lite.OpHint=|Installing TensorFlow
      2 4|2|*NOTE*|ToDense
      2 4|2|=<Operation>=|Built-in Tensor Methods
      2 4|2|Optimization|Raspberry Pi
      2 4|2|Outputs|Load and run a model in Python
      2 4|2|Outputs|Parameters
      2 4|2|platform/* to help the community share CK components, create live|How CK supports collaborative and reproducible ML&systems research
      2 4|2|Qualcomm SDK Linux installation fails with "Malformed|Maintainers
      2 4|2|Requirements|Steps to download data
      2 4|2|Screenshot|What is image classification?
      2 4|2|Simple example diff for using original TF code VS. TensorFlow Lite|Why introduce another set of LSTM APIs?
      2 4|2|Step 5. Release mode.|Trying the GPU Delegate on your own model
      2 4|2|Supervised finetuning|Reader Training
      2 4|2|Test that the model produces sensible outcomes|Measure on-device latency
      2 4|2|TF SSD Mobilenet-v1 non-quantized|Convert COCO to 1200x1200
      2 4|2|/Try your first TensorFlow program/|Contribution guidelines
      2 4|2|wtf.trace.provider.xhr|HUD
      2 4|3|Access details|Model conversion from Paxml checkpoints
      2 4|3|add|Maintainers
      2 4|3|Advanced Configuration|How do you use it?
      2 4|3|Analogy with compile-time constants and code optimization|Compound symbols
      2 4|3|any|Maintainers
      2 4|3|Artifact Store Flavors|How to use it
      2 4|3|Assigning to a =TensorRef=.|Controlling How Expressions Are Evaluated
      2 4|3|BERT-99.9|Run the client program (=pf002=)
      2 4|3|BERT-99|Run the client program (=pf002=)
      2 4|3|Block Sparsity|Adding pruning ops to the training graph
      2 4|3|Building the library|Load and run the example
      2 4|3|Build the library|Load and run the example
      2 4|3|C++|Constants
      2 4|3|Checkpoint Zarr format|How to run
      2 4|3|Consistency of shape|Undefined and None values in TensorFlow
      2 4|3|Constructor =Tensor<data_type, rank>(size_array)=|Class =TensorFixedSize<data_type, Sizes<size0, size1, ...>>=
      2 4|3|Container Registry|7) Create Stack
      2 4|3|Converting TensorFlow models to convert graphs|TF Graph Attributes
      2 4|3|Convert|Semi-automated instructions
      2 4|3|CUDA|Backend (ML framework)
      2 4|3|Custom Objects|wtf.trace.session.bufferSize
      2 4|3|Default minor-to-major ordering|Padding
      2 4|3|doc|Maintainers
      2 4|3|Dynamic variations|ENV flow during CM script execution
      2 4|3|Encryption key for =security@tensorflow.org=|Known Vulnerabilities
      2 4|3|Example: Megatron-LM|Usage Outside the DeepSpeed Runtime
      2 4|3|Experiment Tracker Flavors|How to use it
      2 4|3|From Docker|Steps to run and time
      2 4|3|Function conversion rules|Nested functions
      2 4|3|In a ZenML Step|Secret Schemas
      2 4|3|Install ONNX runtime for CPU|Download Bert-large model (FP32, ONNX format)
      2 4|3|Install SSD-ResNet34 1200x1200 non-quantized fp32 for ONNX opset-8|Benchmark
      2 4|3|Linux/clang:|Threading
      2 4|3|list_files_recursively|Maintainers
      2 4|3|Lower Precision|Calibration
      2 4|3|MobileNet, int8|Performance mode
      2 4|3|Model Registry Flavors|How to use it
      2 4|3|Models from other sources|Re-train a model (transfer learning)
      2 4|3|Modifications are not detected in methods|Python collections in TensorFlow control flow
      2 4|3|Myriad2 (compile only - still a work in progress):|Customizing
      2 4|3|Neon backend|224
      2 4|3|Note: See [[file:profile_model_architecture.md#caveats][Caveats]]|Time and Memory
      2 4|3|Objective-C|Bazel developers
      2 4|3|Objective-C|Import the library
      2 4|3|On CPU|Profile by Python Code
      2 4|3|Option Semantics In Different View|Times
      2 4|3|Other operations|Tested software versions
      2 4|3|Performance|Run experiments (via cmdgen)
      2 4|3|PHP with c extension|Node.js
      2 4|3|prepare|Maintainers
      2 4|3|Prepare MLPerf submission|Trying deepsparse backend
      2 4|3|Press enter to show the default options|Examples
      2 4|3|Private challenges|Copyright
      2 4|3|Process results|Display results
      2 4|3|Python collections of fixed structure with dynamic index|Shape and dtype consistency in TensorFlow control flow
      2 4|3|Python values modified in TensorFlow control flow become Tensors|=if= statements
      2 4|3|Recognize image|Display results
      2 4|3|Record results to the CK repository|PyTorch-based models for CPU
      2 4|3|Recovering signatures|Generic Signatures (custom or advanced usage)
      2 4|3|Reduction along all dimensions|=<Operation> sum(const Dimensions& new_dims)=
      2 4|3|reindex|Maintainers
      2 4|3|replay|Maintainers
      2 4|3|Run harness|Notes on runtime and performance
      2 4|3|Run harness on engines|Notes on runtime and performance
      2 4|3|show|Maintainers
      2 4|3|Special env keys|Script Meta
      2 4|3|Special keys in script meta|How cache works?
      2 4|3|Step 3. Build and run|iOS (with XCode)
      2 4|3|Stripping Default valued attributes|Loader
      2 4|3|TVM ONNX (Python)|Datasets
      2 4|3|uid|Maintainers
      2 4|3|Unregister|CUDA Shared Memory
      2 4|3|Valid variation combinations checked by the community|Script workflow, dependencies and native scripts
      2 4|3|With one CM command that will install all dependencies|Use Python virtual environment with CM and MLPerf
      2 4|3|Zones|Part Type 0x30000/string_table: String Table
      2 45|2||Pose estimation
      2 46|19|located in|object, keyed by request index (uint32_t), stores it
      2 48|2|and simplifying MLPerf|Authors
      2 5|1||CK based object detection DSE
      2 5|1|SMT Control: Disable|Management Firmware Settings
      2 5|1|that the 1st generation of the CK framework was discontinued in|Collective Knowledge framework (CK)
      2 5|2|Exported TensorFlow Lite Model.|Caveat
      2 5|2|For official MLPerf Inference submissions on 50,000 images,|Variations
      2 5|2|gcc 5.4 is required on Ubuntu 16.04, see|Patch
      2 5|2|if* they have been placed on any publicly accessible archival|Submitting artifacts
      2 5|2|If you have previously installed the =coco= datasets, you should|Compiling
      2 5|2||Notes
      2 5|2|Ophinted Customized Graph|Simple Tutorial
      2 5|2|Refresh all CK repositories after any updates (e.g. bug fixes):|Build (Linux or Windows)
      2 5|2|See the quotes magic explained|Run the default command
      2 5|2|TensorFlow Lite LSTM op ("fused ops")|How to use
      2 5|2|that you must set up virtual env on Ubuntu 23+ before using any|Red Hat
      2 5|2|This package is deprecated by =1.15.0=, which includes additional|Unresolved issues
      2 5|2|Using Python 3 is recommended.|Dependencies
      2 5|2|We now have =ck-env:package:lib-python-cython= but it does not|Build warning
      2 5|3|1 sample per batch (for the SingleStream scenario)|Compile the Server/Offline model for the PCIe server cards
      2 5|3|CK will only run the executable once (=--repetitions=1=)|Running ArmCL-OpenCL examples (before ArmCL v18.0x)
      2 5|3|CK will use the latest ArmCL instance compiled with =ck compile=.|Benchmarking ArmCL-OpenCL examples (from ArmCL v18.0x)
      2 5|3|Equivalent to =all.500= for the "min" dataset.|MLPerf Inference option 1
      2 5|3|If you have installed the COCO or KITTI datasets a while ago, you|Run the program
      2 5|3|Install|Android Studio with Bazel
      2 5|3|NUMA nodes per socket: NPS4|CPU Common Options
      2 5|3|On 25/Apr/2019 we informed Google of a bug in their converter,|Manual instructions
      2 5|3|Please detect only one Python interpreter. We recommend Python|Python v3.8
      2 5|3|Proof of convergence|Poisson log loss
      2 5|3|Since the preprocessed ImageNet dataset takes up 7.1G, you may|Download the MLPerf TensorFlow model
      2 5|3|The input tensor's shape gets updated ("fixed") from =?x224x224x3=|Obtain a profile using
      2 5|3|These dependencies are /explicit/, i.e. CK will try to satisfy|Compile the Server/Offline model for the PCIe server cards
      2 5|3|These dependencies are /explicit/, i.e. CK will try to satisfy|Hint
      2 5|3|This option was used for MLPerf Inference v0.5 by Intel and|MLPerf Inference option 2
      2 5|3||Unit-tests
      2 5|3|Use =--target_os=android23-arm64= to build for Android API 23|TensorFlow models
      2 5|3|You may want to install the profiler from the following package:|Printing help messages
      2 5|4|1 sample per batch (for the Single Stream scenario)|Calibrate on your own
      2 5|4|BF16 training|Checkpoint Parameters
      2 5|4|Example: Bert|In Model Training Workflow
      2 5|4|Examples|Determinism when scanning
      2 5|4|=first.1= and =first.5= use a file list (with the first and the|Accuracy with
      2 5|4|For more information, please see the|Use precalibrated profiles
      2 5|4|If =CK_TMP_IMAGE_SIZE= is set and valid, this parameter is not|=CK_SUBTRACT_MEAN=
      2 5|4|=OID= (*TBD*)|=CK_TF_GPU_MEMORY_PERCENT=
      2 5|4|Redirect scrubber control: Auto|Memory Addressing
      2 5|4|Since the preprocessed =1200x1200= COCO dataset takes up 21G, you|Calibrate on your own
      2 5|4|SMT Control: Enable|Global C-state Control: Enabled
      2 6|1|As tensorflow.contrib is being|ConstrainedOptimization (TFCO)
      2 6|1|that on Windows you also need to install /ctuning@ck-win*|Docker
      2 6|1|that this CK-MLPerf documentation is discontinued after the|MLPerf™ inference benchmark automation
      2 6|1|The current data generation pipeline is run on CPU and is|3. Dataset/Environment
      2 6|2|2*|Training and test data separation
      2 6|2|Of Contents* -|Description
      2 6|2|Of Contents*|Description
      2 6|2|=tf_upgrade_v2= is installed automatically as a script by the pip|Report
      2 6|2|that allows you to fetch or reference them in your pipelines and|Centralized secrets store
      2 6|2|We provide Linux build instructions primarily for the purpose of|Current Status
      2 6|3|2*|Training and test data separation
      2 6|3|Decide benchmark name* | name | framework | acc. | AUC | dataset|Disclaimer
      2 6|3|It is extremely critical to set the value of =random_seed= to|TFRecord Features
      2 6|4|This part is only necessary if the accuracy check in Part II|each target accuracy metric, the delta between the two accuracy
      2 6|5||
      2 7|1|refer to /closed/NVIDIA for detailed instructions, including|Dell Submission Systems
      2 7|2|do not report security vulnerabilities through public GitHub|Preferred Languages
      2 7|2|If the repository is not in the PYTHONPATH, make sure to update|Build and Deploy HabanaLabs MLPerf Training 2.1 Container
      2 7|2|If you're using the|Debugging Image Capture
      2 7|2|It uses TensorFlow Lite|Prerequisites
      2 7|2|logger*|Quick Start Guide
      2 7|2|MLPerf inference v1.1; Image Classification; Resnet50; TVM; GCP|Install system dependencies
      2 7|2|MLPerf inference v1.1; Image Classification; Resnet50; TVM;|Install system dependencies
      2 7|2|note that all command instructions provided in this README are|Prerequisites
      2 7|2|note that all the commands in the following sections are|Downloading the dataset
      2 7|2|Please make sure that the =onnx-graphsurgeon= module installed|Model Conversion
      2 7|2|versions* of the official models targeting releases of|Running the models
      2 7|2|WikiExtractor.py replaces some of the tags present in XML such|Clean up and dataset seperation
      2 73|66||is an
      2 7|3|Classifier:* =tf.contrib.kernel_methods.KernelLinearClassifier=|the role of stddev:* The classification quality is very sensitive to
      2 7|3|Please check the|How to deal with numerical accuracy and instability?
      2 7|3|Stage/* ## Run Commands ### Quick submission run (short run)|Customizations
      2 7|3|Take into account that we only update the published wheels after|Testing your Installation
      2 7|3|Update with manual and semi-automatic instructions.|Regular NMS
      2 7|3|why we have that policy*: TensorFlow developers respond to|System information
      2 7|4|Need to introduce an environment variable for the model|Convert
      2 7|5||
      2 8|2|Preparation:* In order to use dataset one needs to preprocess|Run and time
      2 8|2|Start Docker while mounting CK repository with experiments:|Integration with web services and CI platforms
      2 8|7||
      2 9|1|by the|Reproducing MLPerf™ inference benchmarks (v0.7 and v1.0)
      2 9|1|your model*|Run select task
      2 9|2|March 6th, 2019*|Usability
      2 9|2|- With OpenMPI corrupt data will be received resulting in an|Implementation details
      2 9|3|the bug* A clear and concise description of what the bug is.|Reproduce* Steps to reproduce the behavior: 1. Go to '...' 2. Click
      2 9|6||
      2 9|8|to download Dataset and Checkpoint:*|Preparation:* In order to use dataset one needs to preprocess
      3 11|3|note/*: It's worth pointing out that we divide the loss by|Gradient computation & optimization
      3 12|3|The final tarball should not include hidden files. This can|Submit your submission
      3 14|3|Bahdanau-style attention often requires bidirectionality|Multi-GPU training
      3 14|6|institutions*|
      3 16|2|- [[https://arxiv.org/pdf/1812.05784][PointPillars]]|License
      3 2|1|Benchmarking|Example Command related to this Scenario and Workload
      3 2|1|Hands-on -- building an attention-based NMT model|Tips & Tricks
      3 2|1|Inference -- How to generate translations|Intermediate
      3 2|1|Running the sample|Additional resources
      3 2|1|Standard HParams|Other resources
      3 3|1|Multi-GPU training|Benchmarks
      3 3|1|Run benchmark with SLURM for NVIDIA DGXA100 ( 224 nodes )|3. Model
      3 3|2|Get the Results|Get started with DLRM
      3 3|2|Get the Results|Get Started with ResNet50
      3 3|2|Gradient computation & optimization|Hands-on -- Let's train an NMT model
      3 3|2|ImageNet dataset|Benchmark
      3 3|2|INT8 Quantization|Results
      3 3|2|Run experiments|Notes
      3 3|2|SciPy|Install via CK
      3 3|2|Setup with docker image|Run Benchmarks
      3 3|2|Step 3: TensorRT Deployment|Results
      3 3|2|Steps to download data|Steps to launch training
      3 3|2|USB-C Power Adapters|Running a Benchmark
      3 4|1|L2 Up/Down Prefetcher = Auto|Management Firmware Settings
      3 4|2|Benchmark Engine|Inference
      3 4|3|2. into container|3.Run SSD-Resnet34
      3 4|3|(2) into container|4.Run Resnet50
      3 4|3|Bonus: other MobileNets models|Compile the TFLite Image Classification client
      3 4|3|dataset:|2.load images
      3 4|3|dataset:|2.Start a container
      3 4|3|Image cropping|Visualizing experimental results
      3 4|3|Install Dependencies for Resnet50|Download Model
      3 4|3|Install the quantized finetuned model (courtesy of|Run the TensorFlow (Python) Object Detection client on 50 images
      3 4|3|MobileNet, NCHW|Benchmark the performance
      3 4|3|MobileNet quantized|Example: OpenCV preprocessing (default), MobileNet non-quantized
      3 4|3|MobileNet quantized|Example: universal OpenCV preprocessing (default), MobileNet
      3 4|3|Option 3: User-space installation via CK (under =$HOME= and|[Optional] Install Android SDK and NDK
      3 4|3|Option 3: user-space installation via CK (under =$HOME= and|Pull CK repositories
      3 5|2|Refresh all CK repositories after any updates (e.g. bug fixes):|Build
      3 5|3|ACPI SRAT L3 Cche As NUMA Domain = Disabled|CPU Common Options
      3 5|3|Bonus: other MobileNets models|Compile the TensorFlow (C++) Image Classification client
      3 5|3|Currently we have no TFLite 1.13.1 prebuilt packages. Please|Install the SSD-MobileNet models for TFLite
      3 5|3|Fixed SOC Pstate = P0|DF Common Options
      3 5|3|If you already have the ImageNet validation dataset downloaded in|Preprocess datasets
      3 5|3|If you have previously installed the COCO 2017 validation dataset|Preprocess the COCO 2017 validation dataset (first 50 images)
      3 5|3|ImageNet dataset descriptions are in|Install the full dataset (50,000 images)
      3 5|3|Install the ResNet model|Run the ONNX Image Classification client
      3 5|3|On Ubuntu 18.04, NDK r13b gets installed. On Ubuntu 16.04,|Pull CK repositories
      3 5|3|Please [[file:info@dividiti.com][let us know]] if you would like|Install the models for TFLite
      3 5|3|TensorFlow preprocessing (*NOT APPLICABLE!*)|Benchmark the performance
      3 5|3|The prediction from =tflite= differs from that from =tf-cpp=.|Benchmark the performance
      3 5|3|The ResNet model has a|Install models for TensorFlow (C++)
      3 5|3|The TFLite weights are in the =mobilenet_v1_1.0_224*.tflite= file.|Inspecting recorded experimental results
      3 5|3|This excludes "uber" packages which can be used to install all|Run the TensorFlow (Python) Image Classification client
      3 5|3|This TFLite model has been|Compile the TFLite Object Detection client
      3 5|3|Transitive dependencies include|Install a small dataset (500 images)
      3 5|3|Transitive dependencies include|Install the COCO 2017 validation dataset (5,000 images)
      3 5|3|We are working on resolving the difference in mAP between the TF|Benchmark the performance
      3 5|3|We are working on resolving the difference in mAP between the TF|Using Collective Knowledge
      3 5|4|MobileNet quantized|Bonus: other MobileNets models
      3 5|4|OpenCV preprocessing|MobileNet non-quantized
      3 5|4|OpenCV preprocessing|MobileNet quantized
      3 5|4|TensorFlow preprocessing|MobileNet non-quantized
      3 5|4|TensorFlow preprocessing (*NOT APPLICABLE!*)|MobileNet quantized
      3 6|2|Currently only TensorFlow packages provide env variable giving|Program parameters
      3 6|2|matters in the attention mechanism?/*|Attention Wrapper API
      3 6|2|out!* 1) The NCCL-based implementation requires PyTorch >= 1.8|1. Overview
      3 6|3||Model
      3 6|4|out!* This NCCL-based implementation requires PyTorch >= 1.8. It|1.2.2 MPI-based implementation
      3 6|4|This part is only necessary if the accuracy check in Part II|delta between the two accuracy metrics should be within 1% for the
      3 7|1|refer to /closed/Intel for detailed instructions for Intel CPU|Dell Technologies Submission Systems
      3 7|2|Three models in three graphs, with three Sessions sharing the|Data Input Pipeline
      3 7|3|Inside the Docker container, [[file:closed/NVIDIA]] will be|Prerequisites
      3 7|3|This document is autogenerated from internal documentation. If|NVIDIA Submissions
      3 7|6||
      3 8|7|Three models in a single graph and sharing a single Session*|Three models in three graphs, with three Sessions sharing the
      3 9|2|Speed*: (0.37s step-time, 15.3K wps) on /K40m/ & (0.17s|WMT German-English
      3 9|2|Speed*: (2.1s step-time, 3.4K wps) on /Nvidia K40m/ & (0.7s|WMT English-German --- Full Comparison
      4 10|3|are all standardized on =TFRecord= files with|Problems and Modalities
      4 10|3|define training-time hyperparameters for the dataset and|Models
      4 12|6||
      4 13|3|define the core tensor-to-tensor transformation,|Hyperparameter Sets
      4 15|3|sets* are defined and registered in code with|Trainer
      4 2|1|Compile MLPerf loadgen|CM automation for the MLPerf benchmark
      4 2|1|Debug benchmarks|
      4 2|1|Integration with web services and CI platforms|Questions and feedback
      4 2|1|Launch a Docker container on the client side (=pf002=)|Scenarios
      4 2|1|[[../README.md][TOC]] ]*|MLPerf™ Inference v1.0: image classification
      4 2|1|[[../README.md][TOC]] ]*|MLPerf™ Inference v1.0: NLP
      4 2|1|[[../README.md][TOC]] ]*|MLPerf™ Inference v1.0: object detection
      4 2|1|[[../README.md][TOC]] ]*|MLPerf™ Inference v1.0: recommendation
      4 2|1|[[../README.md][TOC]] ]*|MLPerf™ Inference v1.0: speech recognition
      4 2|1|Supported and Tested OS|CLI
      4 2|1|Test composable ML benchmark with other models, data sets, frameworks|The next steps
      4 3|1|Learning rate schedule|5. Quality
      4 3|1|one can control which GPUs are used with the NV_GPU variable|3. Dataset/Environment
      4 3|1|Publication/Attribution|5. Quality
      4 3|1|Recommended setup|2. Directions
      4 3|1|Sample =--help= options|Additional resources
      4 3|1|See all installed packages and detected components|Reproducibility report: design space exploration
      4 3|1|Summary|The next steps
      4 3|2|Add new data set|Participate in reproducibility and optimization challenges
      4 3|2|Assess Accuracy|LoadGen over the Network
      4 3|2|Compilation for datacenter category 16 NSP PCIe|Benchmarking
      4 3|2|Contents|Requirements
      4 3|2|Contents|Walkthrough
      4 3|2|Docker Setup|Run Commands
      4 3|2|Full run|3d-unet
      4 3|2|Full run|Resnet50
      4 3|2|Full run|Retinanet
      4 3|2|Full run|RNNT
      4 3|2|Get the Results|Get started with DLRM2
      4 3|2|Install any available version|Notes
      4 3|2|Install Collective Knowledge (CK)|Pull [[https://github.com/mlcommons/ck-mlops][CK MLOps repository]]
      4 3|2|Load and run the example|Deploy to SparkFun Edge
      4 3|2|Measure power|Debug benchmarks
      4 3|2|Other backends|Run benchmarks and submit results
      4 3|2|Outputs|=log_parser.py=
      4 3|2|Outputs|=submission_checker.py=
      4 3|2|Outputs|=truncate_accuracy_log.py=
      4 3|2|Preprocess using pillow (slightly worse accuracy but works most of|Install framework TFLite 2.4.1 with RUY
      4 3|2|Preprocess using pillow (slightly worse accuracy but works most of|Install reduced ImageNet 2012 val dataset with the first 500 images
      4 3|2|Run the Benchmark|Get Started with ResNet50
      4 3|2|Summary|=generate_final_report.py=
      4 3|2|Summary|=pack_submission.sh= (Deprecated)
      4 3|2|Summary|=repository_checks.sh=
      4 3|2|TensorRT API layers and ops|Preparing sample data
      4 3|2|Test CUDA installation|Install Python virtual environment
      4 3|2|Trainer|Adding your own components
      4 3|2|Try PyTorch backend|Test composable ML benchmark with other models, data sets, frameworks
      4 4|2|Generate actual submission tree|Pytorch backend
      4 4|2|Push the results to GitHub repo|Tensorflow backend
      4 4|2|This file uses the [Trace Event Format]|Q: Why is the code littered with so many lambdas? My eyes hurt.
      4 4|3|3.b Option 2: build docker|4. Run command for accuracy and performance
      4 4|3|Authentication Methods|How do you use it?
      4 4|3|Calibrate and dump int8 model|Run the Benchmark
      4 4|3|Client side|bert-99.9
      4 4|3|Organizers|Initial discussion and materials
      4 4|3|Push the results to GitHub repo|Try PyTorch backend
      4 4|3|Push the results to GitHub repo|Using ARMNN with NEON
      4 4|3|QDL Additional Methods|Example
      4 4|3|Reproduce results from ACM/IEEE/NeurIPS papers|Project coordinators
      4 4|3|Run performance|Results
      4 5|3|For more information, please see the|Compilation for 15w AEDKs (edge category)
      4 5|3|Refresh all CK repositories after any updates (e.g. bug fixes):|Set up environment variables
      4 5|3|Since the preprocessed =1200x1200= COCO dataset takes up 21G, you|Use precalibrated profiles
      4 6|5|Offline|SingleStream
      4 6|5|SingleStream|Compile and install the models to the 20W AEDK
      4 7|2|If you're using the|Deploy to STM32F746
      4 7|2|MLPerf inference v1.1; Image Classification; Resnet50;|Install system dependencies
      4 7|2|MLPerf inference v1.1; Image Classification; Resnet50; ONNX (out|Install system dependencies
      4 8|3|Path to .csv output file of the|Outputs
      4 8|3|Path to directory containing your submission *output*: Path to|Outputs
      5 10|7|output metric*|work*
      5 3|1|4. Run command for server and offline mode|Setup with Docker
      5 3|1|Note on Server scenario|Setup with Docker
      5 3|1|Running other datasets:|3. Dataset
      5 3|1|Sample --help options|Additional resources
      5 3|2|Bare metal installation|Container
      5 3|2|BERT|Parameters
      5 3|2|Choose your TestSettings carefully!|Responsibilities of a LoadGen User
      5 3|2|Install Python packages (in user-space)|Prevent running out of memory
      5 3|2|OPEN|Submission Rules
      5 3|2|Program-specific preprocessing|Images dataset
      5 3|2|Run Compliance Tests and Update Compliance Logs|Instructions for Auditors
      5 3|2||Steps to run RNNT
      5 3|2|USB-C Power Adapters|Download Data, Model and Preprocess the data
      5 3|2|Validate accuracy for ssd-mobilenet and ssd-resnet34 benchmarks|Datasets
      5 4|2|By default, the loadgen will output an /mlperf_log_summary.txt/|Q: The reference implementation for </some_model/> prints out results
      5 4|2|If you have a custom build setup, make sure you run the|Q: What is this /version_generator.py/ script?
      5 4|2|It currently targets and requires C++14. It should compile with|Q: What dependencies does the LoadGen code have?
      5 4|2|Lambdas are a convenient and efficient way to ship arbitrary data +|Q: What C++ version does the LoadGen target?
      5 4|2|No. To keep the playing field level, please upstream any local|Q: Where can I find the results of a test?
      5 4|2|On GitHub: https://github.com/mlcommons/inference/issues/new|Q: Can I make local modifications to the LoadGen for submission?
      5 4|2|SingleStream|Info
      5 4|2|The LoadGen records git stats (if available) and the SHA1 of all|Q: How do I view the /mlperf_log_trace.json/ file?
      5 4|2|The MLPerf spec is /always/ right. Please file a LoadGen bug so it|Q: How can I file a bug?
      5 4|2|They are not. The LoadGen results are the ground truth for|Q: I'm getting linker errors for LoadgenVersion definitions. Where is
      5 4|3|(2) into container|3.Run DLRM
      5 4|3|=CK_RECREATE_CACHE=|Input preprocessing parameters
      5 4|3|Gory details|Copy the results to a machine for analysis
      5 4|3|model:|2.Start and log into a container
      5 4|3|SingleStream|Compile and install the models to the 16 NSP AEDK
      5 5|2|aware of how to score the accuracy of a model's outputs. * *NOT*|Submission Considerations
      5 5|3|Please detect only one Python interpreter. Python 3.6, the default|Python v3.6 (default)
      5 5|4|This is equivalent to the default run command:|Gory details
      5 7|2|work*|Installation
      5 7|3|enter closed/NVIDIA*. From now on, all of the commands detailed|Launching the environment on datacenter/desktop systems
      6 11|3|notes:/*|Launching the environment on Jetson Orin systems
      6 13|5|for DLRMv2*, the dataset is quite large. We recommend|that once the scratch space is setup and all the data, models, and
      6 14|2|the datasets for inferences* described in|Running a Benchmark
      6 15|3||Fixing INVALID results
      6 15|3||Tuning parameters for better performance
      6 20|19|[dictionary]|[dictionary]
      6 2|1|BERT-99.9%: MobileBERT Offline - DeepSparse|ResNet50
      6 2|1|Clean up|Running the model
      6 2|1|[[../README.md][Back to TOC]] ]*|Shortcuts
      6 2|1|Steps to download and verify data|3. Model
      6 2|1|Usage|Contact us
      6 3|1|Run Benchmark|Setup with Docker
      6 3|1|Steps to run and time|3. Dataset/Environment
      6 3|2|option 1: build docker|convert dataset and model
      6 3|2|Performance: Offline (500 samples)|note from the community:* A valid Offline performance run
      6 3|2|Performance: Single Stream (500 samples)|note from the community:* A valid SingleStream performance run
      6 3|2|Power Measurement Setup|Reproducing the Nvidia Jetson AGX Orin Submission
      6 3|2|Run benchmark with SLURM - single-node/multi-node training.|Steps to download and verify data
      6 3|2|Troubleshooting|Apply Perf Optmization (AGX and NX applicable)
      6 3|2|USB-C Power Adapters|Apply Perf/W Optmization (only AGX applicable)
      6 4|2|Generate actual submission tree|Tensorflow backend
      6 4|3|Infrastructure Deployment|How to use it
      6 4|3|MobileNet quantized|Benchmark the accuracy
      6 5|1|Below we give an /essential/ sequence of steps that should result|Table of Contents
      6 5|3|Care must be taken not to mix Python 3 and Python 2 packages. If|Install required Python 3 packages
      6 5|3|Convert NumPy dataset to raw format.|Specify the preprocessed data paths in the training script.
      6 5|3|For the =imagenet-2012-val-min= dataset, change|Using Collective Knowledge
      6 5|3|If you would like to get a feel of CK workflows, you can skip|Install common tools and libraries
      6 5|3|that once the scratch space is setup and all the data, models, and|Download the datasets
      6 5|3|that you do not need to download the datasets or models for|Downloading the model files
      6 5|3|When using the batch count of *N*, the program classifies *N*|Benchmark the accuracy
      6 5|3|When using the batch count of *N*, the program runs object|Benchmark the accuracy
      6 5|4|CK can normally detect available Python interpreters|Install implicit dependencies via pip
      6 5|4|For the =imagenet-2012-val-min= dataset, change|ResNet
      6 5|4|When using the batch count of *N*, the program classifies *N*|ResNet
      6 6|5|precision fp16|=r282_z93_q5= [optional]
      6 6|5|precision fp16|=r282_z93_q8= [optional]
      6 7|2|MLPerf inference v1.1; Image Classification; DSE (Pareto|Install system dependencies
      6 7|2|MLPerf inference v1.1; Image Classification; Resnet50; TVM; AWS|Install system dependencies
      6 7|3|the above steps (/Download the datasets, Downloading the model|Setting up the DLRMv2 Scratch Space
      6 9|3|Only original source code from you and other people that have|Contributing code
      7 3|2|Calibration set|Running the benchmark
      7 3|2|Disclaimer|Prerequisites and Installation
      7 3|2|Get the Results|Get started with BERT
      7 3|2|Get the Results|Get Started with Retinanet
      7 3|2|Get the results|Get Started with RNNT
      7 4|3|Accuracy|Get the Results
      7 4|3|Convert Dataset and Model|Run the Benchmark
      7 4|3|GPU|Examples for testing
      7 4|3|Install Python and IPex|Download Model
      7 4|3|Login to Docker Container|Preprocess model and dataset
      7 4|3|Login to Docker Container|Run the Benchmark
      7 4|3|Preprocess Data|Run the Benchmark
      7 5|3|=g++=|=r282_z93_q5=: use QAIC settings (ECC on)
      7 5|4|CK can normally detect compilers automatically, but we are playing|Install implicit dependencies via pip
      7 5|4|=g292_z43_q16= [optional]|Power
      7 5|4|Run the below commands with =sudo= or as superuser.|Generic
      7 6|3|this is a one-way operation. Once you =eject=, you can't go|=npm run build= fails to minify
      8 10|3|study mode currently requires the C++ Minigo engine.*|Vs mode
      8 11|3|notes:/*|Launching the environment on a MIG (Multi-Instance GPU) instance
      8 2|1|Automated Tests|Basics
      8 2|1|Playing Against Minigo|Training Minigo
      8 3|1|Accuracy|=llvm -mcpu=cortex-a72 -mfloat-abi=hard=
      8 3|1|This is NOT an official version of AlphaGo|Goals of the Project
      8 3|1|Training data order|4. Model
      8 3|1|Validating on a different set of data|Retraining a model
      8 3|2|Kiosk mode|Technical discussion
      8 3|2|Observations|Third run, Minigo, model 250-..., Jan 20th-Feb 1st (ish)
      8 3|2|Performance|SingleStream scenario
      8 3|2|See all installed packages and detected components|Resources
      8 3|2|Setup|GCS for simple task signaling
      8 4|2|cc:gtp|Running the unit tests
      8 4|3|test|Maintainers
      8 4|3|Update messages|Synchronizing stdout and stderr
      8 4|3|v5 changelog:|v7a, first week of May
      8 5|4|=g292_z43_q16=|Performance
      8 5|4|=r282_z93_q5= [optional]|Power
      8 5|4|=r282_z93_q5=|Performance
      9 16|2|We initialized pretrained MobileBERT with 15|Benchmarks
      9 17|1|and *99*: We dialed JPQD pruning intensity to target|*BERT-base-99*: JPQD was configured to optimize BERT-base to meet 99%
      9 3|2|Troubleshooting|Apply Perf Optmization
     10 3|2|One liner to do an end-to-end submission using the reference|Please adjust the =target_qps= value as per your system performance
     10 6|4|precision fp16|Performance
     10 6|4|precision fp16|Power
     10 6|5|precision fp16|=g292_z43_q16= [optional]
     11 10|3|Currently, the non-=-devel= images on Pytorch Dockerhub do|Running your Apex container
     11 12|2|Use [[https://pytorch.org/docs/stable/amp.html][PyTorch|2. Distributed Training
     11 12|3|Use|Checkpointing
     11 12|4|training* below for more detail)|=--opt-level O2= ("Almost FP16" mixed precision. More dangerous
     11 13|10||Currently, the non-=-devel= images on Pytorch Dockerhub do
     11 2|1|[[https://github.com/mcarilli/mixed_precision_references/tree/master/GTC_2019][GTC|Contents
     11 30|5|and *run.sh* show an example using Amp|is intended purely as an instructional example, not a performance
     11 3|1|Checkpointing|Installation
     11 3|2|[Experimental] Windows|Custom C++/CUDA Extensions and Install Options
     11 3|2|Lower Precision|Instructions for Audits
     11 3|2|Preprocessing data|Model
     11 3|2|Running your Apex container|Option 2: Install Apex in a running container
     11 3|2|Transform unstructured sparsity to structured sparsity (as in Figure|References
     11 40|3|is deprecated. Use|Synchronized Batch Normalization
     11 4|2|=--opt-level O2= ("Almost FP16" mixed precision. More dangerous|Distributed training
     11 4|2|Test your own range!|Performance Comparisons
     11 4|3|CPU path|Important arguments
     11 4|3|[[https://github.com/NVIDIA/apex/tree/master/examples/FP16_Optimizer_simple/distributed_apex][Simple|Synchronized Batch Normalization
     11 7|2|To verify accuracy of your workload, run your command with|Code Diagram
     11 7|3|=--opt-level= =O1= and =O2= both use dynamic loss scaling by|Summary
     12 3|2|Replace ReLU6 with ReLU|Instructions for Audits
     12 3|2|Usage|License
     12 6|5|precision fp16|=r282_z93_q5=
     12 6|5|precision fp16|=r282_z93_q8=
     13 15|1|-|License
     13 4|3|3.b Option 3: pull docker|4. Run command for accuracy and performance
     13 5|4|These dependencies are /implicit/, i.e. CK will not try to satisfy|Install explicit dependencies via CK (also via =pip=, but register
     13 6|2|Of Contents* - [[#description][Description]] -|Description
     14 11|3|notes:/*|Launching the environment on Jetson Orin AGX/NX
     14 18|2|contains RetinaNetNMS PVA kernel|NMSPVAPlugin : Sample application to demonstrate how to use this
     14 4|3|power measurement* with VF configuration, you need to allow|Launching the environment on a MIG (Multi-Instance GPU) instance
     14 4|3|Steps to build the standalone plugin|MACROS in the NMSPVAPlugin
     14 5|4|Aarch64-Linux or Tegra-Linux Specific Paths|Steps to build the standalone plugin
     14 6|4|CLI flags can be used in the Python CM API as follows:*|Script flags mapped to environment
     15 5|3|that the same scale factor is used for the Q, K, and V output|Quantization in the Open Division Submissions
     16 3|1|Optimizer|4. Quality
     16 3|1|See all installed packages and detected components|Reproducibility report: benchmarking
     16 3|2|Development (live) components for MLPerf|Activate virtual environment
     16 3|2|Run Benchmarks with Int8|Docker Instructions
     16 5|4|SMT Control: Disable|Global C-state Control: Enabled
     17 4|3|Versions|Script workflow, dependencies and native scripts
     18 3|2|10. run|Docker
     18 4|1|Global C-state Control: Enabled|Management Firmware Settings
     18 4|3|Alternative cross compile option|Troubleshooting
     19 3|2|2. SW requirements|Steps to run DLRM
     20 4|3|Rule Polymorphism|Arguments
     20 6|5|precision fp16|=g292_z43_q16=
     20 7|3|enter closed/NVIDIA*. From now on, all of the commands detailed|Launching the environment
     21 4|3|Accuracy|Setup with docker image
     22 12|3|this webpage has not yet been finalized, so the|Instructions for Auditors
     22 9|3|measurements, and systems directories in your submission* for|later, you wish to remove a system*, simply edit this file and
     23 3|2|Model Source|Optimizations
     23 3|2|option 2: pull docker|convert dataset and model
     25 14|3||Tuning parameters for better performance
     25 14|6||stopping*: under the new rules, /min_query_count/ is no longer a
     25 15|6||stopping*: under the new rules, /min_query_count/ is no longer a
     25 3|2|Build|Setup Instructions - Docker
     25 3|2|Quantize Torchscript Model and Check Accuracy|Run Benchmark (Common for Docker & Baremetal)
     25 5|3|that once the scratch space is setup and all the data, models, and|Download the Datasets
     25 5|3|that you do not need to download the datasets or models for|Downloading the Model files
     25 6|3|stopping*: under the new rules, /min_query_count/ is no longer a|Fixing INVALID results
     25 7|3|the MLPerf Inference container*, launch a Python console and run|NVIDIA DGX Stations, this method will fail*, since the Mellanox NICs
     25 9|6||stopping*: under the new rules, /min_query_count/ is no longer a
     26 2|1|Prepare your submission|Visualize MLPerf results
     26 3|2|Alternative launch with nvidia-docker|Steps to launch training on multiple nodes
     27 3|1|Hyperparameter settings|3. Quality
     27 3|2|End-to-end Faster and Mask R-CNN baselines|Comparison with Detectron and mmdetection
     27 3|2|PASCAL VOC Annotations in COCO Format|Creating Symlinks for Cityscapes:
     28 3|2|Structure|Parameters
     28 7|2|This document is autogenerated from internal documentation. If|Heterogeneous MIG Workloads for Multi-MIG Systems
     29 16|3|for short).|How the HeteroMIG harness is designed
     29 2|1|Publication/Attribution|2. Directions
     29 7|3|This document is autogenerated from internal documentation. If|Before you continue
     30 10|7|Make sure that no files and directories exist in the project|The
     30 12|3|The way audit tests function is by placing an =audit.conf=|Truncating the Accuracy Logs
     30 3|2|your system is not listed above, you must add your system to our|Running your first benchmark
     30 4|3|submission that does not use one of these commit hashes will not be|Minimal Query Count
     30 5|4|an eye out for this announcement, as it will also include a|submission that does not use one of these commit hashes will not be
     30 6|2|Of Contents* - [[#description][Description]] *|Description
     30 7|3|The|Packaging your project for submission
     30 7|3|This document is autogenerated from internal documentation. If|MLPerf Inference Policies and Terminology
     31 10|3||you run into issues, invalid results, or would like to improve your
     31 12|3|engines is only required if*|Building and running engines for the "High Accuracy Target"
     31 12|3|Make sure your performance tuning changes (i.e. any change|Difference system configurations that use the same GPU configuration
     31 13|3|in order to use start_from_device or end_on_device in a|Using NUMA configurations
     31 2|1|Run command|Setup with docker
     31 5|3|that this flag is only supported / allowed on the following|you wish to use both start_from_device and end_on_device, you must
     31 7|3|proceeding, double check that you have downloaded both the|Preprocessing the datasets for inference
     31 7|3|This command does not need to be run every time you enter the|Running the actual benchmark
     31 7|3|This document is autogenerated from internal documentation. If|Make Targets
     31 7|3|This document is autogenerated from internal documentation. If|What if I have permission issues when I attempt to write to the
     32 2|1|Packed data|Running the model
     32 3|2||Steps to run RNN-T with three options
     32 4|1|Alternative launch with nvidia-docker|3. Dataset/Environment
     32 4|3|2. End-to-end run inference|Option 2: Build docker container
     33 4|1|Global C-state Control: Disabled|Management Firmware Settings
     33 4|2|NVIDIA DGX A100 (single node)|Alternative launch with nvidia-docker
     33 5|4|SMT Control: Enable|Global C-state Control: Disabled
     34 3|1|Training and test data separation|4. Model
     37 3|1|Example 2|Description of how the =results_text.tar.gz= file was prepared
     37 3|1|Loss function|5. Quality
     37 3|2|Sequence Splitting|Instructions for Audits
     37 4|3|Example|META_is_config_valid()
     37 4|3|Example|META_search_callback()
     37 7|2|This document is autogenerated from internal documentation. If|NVIDIA MLPerf Quantization
     38 2|1|Hyperparameter settings|Dataset/Environment
     38 2|1|Publication/Attribution|Quality
     38 2|1|The MLPerf Subset|Model
     38 3|1|Optimizer|5. Quality
     38 4|3|Removal of Softmax|Calibration
     39 2|1|Steps to download data|=dev-clean-wav/= * =dev-other-wav/= * =test-clean-wav/= *
     39 3|1|Hyperparameter settings|3. Dataset/Environment
     40 3|2|Calibration|Instructions for Audits
     42 3|2|Performance|Prepare your submission
     44 4|3|Default variations|Script workflow, dependencies and native scripts
     45 6|4|[[https://github.com/krai/ck-mlperf/tree/master/program/generate-target-latency][Estimate|Accuracy
     46 3|1|Test data order|4. Model
     49 3|2|Generating model binaries and running INT8 calibration|Instructions for Audits
     50 15|14||
     51 5|3|ACPI SRAT L3 Cche As NUMA Domain: Disabled|CPU Common Options
     51 5|4|Redirect scrubber control: Disabled|Memory Addressing
     54 5|3|DF Cstates: Auto|DF Common Options
     57 3|2|Embedding Table Sorting and Splitting|Instructions for Auditors
     57 3|2|Preprocessing the dataset for use|Model
     66 4|3|New environment keys (filter)|Maintainers
     70 3|2|Downloading / obtaining the model|Dataset
     70 3|2|Preprocessing the dataset for usage|Optimizations
     72 4|3|Versions|Script output
     83 3|2|TransposedConvolution -> Convolution + PixelShuffle Conversion|Instructions for Auditors
     87 3|2|Soft Dropping|Instructions for Auditors
     93 10|9||
     96 5|2|OpenCL|Benchmark via the "neoclassical" CK interface
    102 3|2|Generating model binaries and running INT8 calibration|Instructions for Auditors
    112 2|1|=[D1]= Extract and preprocess ImageNet on the device|E. Set up QAIC SDKs
    112 2|1|=[D1]= Run|D. Set up ImageNet and other datasets
    112 2|1|=[D1S]= Set user password|C. Initial device setup under the =krai= user
    112 2|1|=[D]= Verify with a quick run|Appendix: Arguments
    112 2|1|=[HR]= Compile the workloads on the host and copy to the device|F. Expected Results from Set up QAIC SDKs
    112 4|1|=DOCKER_DEVICE_TYPE=|Appendix: Info
    112 5|1|The full installation can take more than 50G. If the space on the|B. Initial device setup under the =root= user
    128 2|1|Requirements|2. Directions
    136 4|3|New environment keys auto-detected from customize|Maintainers
    140 3|2|Preprocessing the dataset|Model
    151 3|2|Goal of this Benchmark|Dataset
    152 4|1|Deprecated workloads|Info
    152 4|3|Set up Collective Knowledge environment|Target OS dependent, SDK dependent
    152 5|3|Edge - Q1 Pro|Further info
    152 5|3|Log out and log back in for the necessary group permissions to|Host OS independent
    152 5|4|Make sure to have copied the required datasets (e.g. ImageNet) and|Test Docker images
    162 6|4|CLI flags can be used in the Python CM API as follows:*|Default environment
    177 3|2|Preprocessing the dataset for usage|Model
    190 3|2|[[https://github.com/arm-software/armnn-mlperf#preprocess-on-an-x86-machine-and-detect-on-an-arm-dev-board][Detect|Benchmark performance via the "classical" CK interface
    190 4|2|Performance|Benchmark via the "neoclassical" CK interface
    192 4|3|=model-tflite-mlperf-efficientnet-lite4=|[[https://github.com/arm-software/armnn-mlperf#preprocess-on-an-x86-machine-and-detect-on-an-arm-dev-board][Detect
    197 3|2|Downloading / obtaining the model|Optimizations
    271 4|3|Default environment|Script workflow, dependencies and native scripts
    406 4|3|CM modular Docker container|Customization
    406 5|4|CM script automation help|CM CLI
   1425 5|3|This mode is supported only with CK ≤ v1.17.0 or ≥ v2.6.0:|ResNet50
   1743 5|4|[[https://github.com/krai/ck-mlperf/tree/master/program/generate-target-latency][Estimate|Accuracy
   2532 5|4|OpenCL|"All-in-one"
   2532 5|4|This mode is supported only with CK ≤ v1.17.0 or ≥ v2.6.0:|"All-in-one"
   2532 6|5|[[https://github.com/krai/ck-mlperf/tree/master/program/generate-target-latency][Estimate|Note that unlike [[#mobilenet_v1][MobileNet-v1]],
   2547 6|5|Use a uniform target latency|[[https://github.com/krai/ck-mlperf/tree/master/program/generate-target-latency][Estimate
   2548 5|4|OpenCL|Compliance
   2850 5|4|[[https://github.com/krai/ck-mlperf/tree/master/program/generate-target-latency][Estimate|"All-in-one"
   3957 5|4|Note that unlike [[#mobilenet_v1][MobileNet-v1]],|"All-in-one"
   5064 6|4|[[https://github.com/krai/ck-mlperf/tree/master/program/generate-target-latency][Estimate|"All-in-one"
   7641 6|5|[[https://github.com/krai/ck-mlperf/tree/master/program/generate-target-latency][Estimate|OpenCL
  12676 5|4|OpenCL|Accuracy
  12735 5|4|OpenCL|Performance
#+end_example

** Pairs
Here are some pairs:
  47908 #+BEGIN_EXAMPLE
  47908 #+END_EXAMPLE

  There are pairs
  88078      :END:
  88078      :PROPERTIES:

 And these :
 102501       :END:
 102501       :PROPERTIES:

*** create windows around lines, create ngrams of lines.

*** try other tokenizer:
chunk and tokenize in org mode [[microfts]]

construct json from pandoc
see [[process_json.py]]

tiktoken
https://github.com/openai/tiktoken


* Bibliography

@Article{2020,
  author       = {Zaman, Faisal},
  title	       = {Demo 1: Getting Started with TFLite},
  journal      = {TensorFlow Lite for Mobile Development},
  year	       = 2020,
  doi	       = {10.1007/978-1-4842-6666-3_5},
  url	       = {http://dx.doi.org/10.1007/978-1-4842-6666-3_5},
  ISBN	       = 9781484266663,
  publisher    = {Apress}
}

@article{1, (B) second approach-ResNet50, (C) third approach-ResNet50, (D) third approach-ResNet101, (green–wall, blue–no-wall)., url={http://dx.doi.org/10.7717/peerj-cs.1565/fig-6}, DOI={10.7717/peerj-cs.1565/fig-6}, publisher={PeerJ} }


tflite https://www.tensorflow.org/litex



https://github.com/ARM-software/armnn
